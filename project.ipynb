{
      "cells": [
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "!pip install wordcloud\n",
                        "!pip install Unidecode\n",
                        "!pip install python-louvain\n",
                        "!pip install --upgrade six\n",
                        "!pip install --upgrade pip \n",
                        "\n",
                        "!pip install --upgrade Pillow\n",
                        "!pip install fuzzywuzzy"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Part 0: Imports"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import requests\n",
                        "import os, re\n",
                        "from datetime import datetime\n",
                        "from bs4 import BeautifulSoup\n",
                        "import json\n",
                        "import random\n",
                        "import string\n",
                        "from collections import Counter\n",
                        "\n",
                        "from itertools import combinations\n",
                        "from concurrent.futures import ThreadPoolExecutor\n",
                        "from tqdm import tqdm\n",
                        "\n",
                        "from fuzzywuzzy import fuzz\n",
                        "\n",
                        "import networkx as nx\n",
                        "import matplotlib.pyplot as plt\n",
                        "import numpy as np\n",
                        "import pandas as pd\n",
                        "from pprint import pprint\n",
                        "from tqdm import tqdm\n",
                        "from unidecode import unidecode\n",
                        "from country_named_entity_recognition import find_countries\n",
                        "import country_converter as coco\n",
                        "\n",
                        "from typing import List, Dict\n",
                        "from sentence_transformers import SentenceTransformer\n",
                        "from sklearn.metrics.pairwise import cosine_similarity\n",
                        "\n",
                        "import networkx as nx\n",
                        "import matplotlib.pyplot as plt\n",
                        "from mpl_toolkits.mplot3d import Axes3D\n",
                        "\n",
                        "import powerlaw\n",
                        "from community import community_louvain\n",
                        "\n",
                        "import nltk\n",
                        "from nltk.stem import WordNetLemmatizer\n",
                        "from wordcloud import WordCloud, STOPWORDS\n",
                        "from nltk.corpus import stopwords\n",
                        "from nltk.corpus.reader import PlaintextCorpusReader\n",
                        "from nltk.text import Text\n",
                        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
                        "import string\n",
                        "\n",
                        "import nltk\n",
                        "from nltk.probability import FreqDist\n",
                        "from nltk.corpus import stopwords\n",
                        "\n",
                        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                        "from matplotlib import cm\n",
                        "\n",
                        "nltk.download('stopwords')\n",
                        "nltk.download('punkt')\n",
                        "nltk.download('omw-1.4')\n",
                        "nltk.download('wordnet')"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Part 1: Scraping the data\n",
                        "\n",
                        "Our first mission was to scrap the data from nature.com and then preprocess it to get beautiful list of nodes (universities) and edges (collaborations between universities).\n",
                        "\n",
                        "To do it, we decided to create a Scraper object with some functions to then be able to manipulate the data easily."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Scraper class for extracting universities and collaborations from nature.com articles\n",
                        "class Scraper:\n",
                        "    def __init__(self):\n",
                        "        # Initializing class variables\n",
                        "        self.base_url = \"https://www.nature.com\"\n",
                        "        self.year = 2023 # The year of the articles to be extracted\n",
                        "        self.unprocessed_universities = {} # Dictionnary to store the non processed universities\n",
                        "        self.universities = {}         # Dictionary to store universities and their occurrences\n",
                        "        self.collabs = {}              # Dictionary to store collaborations between universities\n",
                        "        self.raw_to_abstract_mapping = {}        # Mapping old university names to abstract names\n",
                        "        self.links_not_used_for_network = []         # List to save article links used for text preprocessing\n",
                        "        self.links_used_for_network = []         # List to save article links used in the network\n",
                        "        self.links_authors_countries_used_for_network = {}       # List to save countries of authors of the links saved for the netowrk\n",
                        "\n",
                        "    # ---- METHOD TO LOAD JSON FROM FILES ----\n",
                        "\n",
                        "    # Method to load a certain year\n",
                        "    def load_year_data(year):\n",
                        "        self.year = year\n",
                        "        self.load_universities(f\"{year}/universities.json\")\n",
                        "        self.load_collabs(f\"{year}/collabs.json\")\n",
                        "        self.load_links_used_for_network(f\"{year}/links_used_for_network.json\")\n",
                        "        self.load_links_not_used_for_network(f\"{year}/links_not_used_for_network.json\")\n",
                        "        self.load_links_authors_countries_used_for_network(f\"{year}/links_authors_countries_used_for_network.json\")\n",
                        "\n",
                        "    # Method to load collaboration data from a JSON file\n",
                        "    def load_collabs(self, filename):\n",
                        "        with open(filename, 'r') as file:\n",
                        "            self.collabs = json.load(file)\n",
                        "\n",
                        "    # Method to load unprocessed universities data from a JSON file\n",
                        "    def load_unpreprocessed_universities(self, filename):\n",
                        "        with open(filename, 'r') as file:\n",
                        "            self.unprocessed_universities = json.load(file)\n",
                        "\n",
                        "    # Method to load universities data from a JSON file\n",
                        "    def load_universities(self, filename):\n",
                        "        with open(filename, 'r') as file:\n",
                        "            self.universities = json.load(file)\n",
                        "\n",
                        "    # Method to load saved links not used for network from a JSON file\n",
                        "    def load_links_not_used_for_network(self, filename):\n",
                        "        with open(filename, 'r') as file:\n",
                        "            self.links_not_used_for_network = json.load(file)\n",
                        "\n",
                        "    # Method to load links used for the network from a JSON file\n",
                        "    def load_links_used_for_network(self, filename):\n",
                        "        with open(filename, 'r') as file:\n",
                        "            self.links_used_for_network = json.load(file)\n",
                        "\n",
                        "    # Method to load countries of the others of the articles we used for the network\n",
                        "    def load_links_authors_countries_used_for_network(self, filename):\n",
                        "        with open(filename, 'r') as file:\n",
                        "            self.links_authors_countries_used_for_network = json.load(file)\n",
                        "\n",
                        "    # ---- METHOD TO SAVE JSON IN FILES ----\n",
                        "\n",
                        "    # Method to save unprocessed universities data to a JSON file\n",
                        "    def save_unproccessed_universities_in_file(self):\n",
                        "        with open(f'{self.year}/unprocessed_universities.json', 'w') as fp:\n",
                        "            json.dump(self.unprocessed_universities, fp)\n",
                        "\n",
                        "    # Method to save universities data to a JSON file\n",
                        "    def save_universities_in_file(self):\n",
                        "        with open(f'{self.year}/universities.json', 'w') as fp:\n",
                        "            json.dump(self.universities, fp)\n",
                        "\n",
                        "    # Method to save collaborations data to a JSON file\n",
                        "    def save_collabs_in_file(self):\n",
                        "        with open(f'{self.year}/collabs.json', 'w') as fp:\n",
                        "            json.dump(self.collabs, fp)\n",
                        "\n",
                        "    # Method to save links not used for network to a JSON file\n",
                        "    def save_links_not_used_for_network_in_file(self):\n",
                        "        with open(f'{self.year}/links_not_used_for_network.json', 'w') as fp:\n",
                        "            json.dump(self.links_not_used_for_network, fp)\n",
                        "\n",
                        "    # Method to save saved links used for network to a JSON file\n",
                        "    def save_links_used_for_network_in_file(self):\n",
                        "        with open(f'{self.year}/links_used_for_network_in_file.json', 'w') as fp:\n",
                        "            json.dump(self.links_used_for_network, fp)\n",
                        "\n",
                        "    # Method to save links authors countries used for network\n",
                        "    def save_links_authors_countries_used_for_network_in_file(self):\n",
                        "        with open(f'{self.year}/links_authors_countries_used_for_network.json', 'w') as fp:\n",
                        "            json.dump(self.links_authors_countries_used_for_network, fp)\n",
                        "\n",
                        "    # ---- METHOD FOR DATA SCRAPPING ----\n",
                        "\n",
                        "    # Main method to scrap all the articles for subjects in a certain year and get the data in the ./year/ folder\n",
                        "    def scrap_website(self, year, subjects):\n",
                        "        self.year = year\n",
                        "        os.makedirs(f\"{year}/\", exist_ok=True)\n",
                        "\n",
                        "        print(f\"Starting the extraction of universities for {year} ...\")\n",
                        "        for s in subjects:\n",
                        "            self.extract_universities(s, year)\n",
                        "            self.save_unproccessed_universities_in_file()\n",
                        "        \n",
                        "        print()\n",
                        "        print(\"The universities are now extracted, let's start preprocess ...\")\n",
                        "        self.preprocess_universities()\n",
                        "        print(\"Preprocess finished, we will now proceed to the subject extractions and the text extractions\")\n",
                        "        print()\n",
                        "\n",
                        "        self.save_universities_in_file()\n",
                        "        self.create_mapping_from_raw_to_abstract_university_names()\n",
                        "\n",
                        "        for s in subjects:\n",
                        "            self.extract_collabs(s, year)\n",
                        "            self.save_links_used_for_network_in_file()\n",
                        "            self.save_links_not_used_for_network_in_file()\n",
                        "            self.save_links_authors_countries_used_for_network_in_file()\n",
                        "            self.save_collabs_in_file()\n",
                        "\n",
                        "        self.print_stats()\n",
                        "\n",
                        "    # Method to retrieve HTML content of a given URL and parse it using BeautifulSoup\n",
                        "    def retrieve_url(self, url: str) -> BeautifulSoup:\n",
                        "        page = requests.get(url)\n",
                        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
                        "        return soup\n",
                        "\n",
                        "    # Method to extract universities data from nature.com articles\n",
                        "    def extract_universities(self, subject, year):\n",
                        "        init_univs_len = len(self.unprocessed_universities)\n",
                        "        articles_counter = 1\n",
                        "        page = 1\n",
                        "\n",
                        "        while True:\n",
                        "            search_url = f'{self.base_url}/search?journal=srep&article_type=research&subject={subject}&date_range={year}-{year}&order=relevance&page={page}'\n",
                        "            soup = self.retrieve_url(search_url)\n",
                        "\n",
                        "            all_atags = soup.find_all('a', {'data-track-action': 'view article'})\n",
                        "\n",
                        "            # No articles\n",
                        "            if len(all_atags) == 0:\n",
                        "                break\n",
                        "            \n",
                        "            # For each article\n",
                        "            for atag in all_atags:\n",
                        "                random.seed(articles_counter)\n",
                        "\n",
                        "                # We keep it for network\n",
                        "                if(random.random() < 0.5):\n",
                        "                    soup = self.retrieve_url(self.base_url + atag['href'])\n",
                        "\n",
                        "                    try:\n",
                        "                        # Find universities and update dictionaries\n",
                        "                        authors_all = soup.findAll('p', class_='c-article-author-affiliation__address')\n",
                        "\n",
                        "                        for s in authors_all:        \n",
                        "                            university = s.text        \n",
                        "                            if university not in self.unprocessed_universities:\n",
                        "                                self.unprocessed_universities[university] = 1\n",
                        "                            else:\n",
                        "                                self.unprocessed_universities[university] += 1\n",
                        "                    except:\n",
                        "                        print(\"Error during the authors find all\")\n",
                        "                articles_counter += 1            \n",
                        "            page += 1\n",
                        "        print(f\"Extracted {len(self.unprocessed_universities) - init_univs_len} universities for {subject} ({articles_counter} articles).\\n\")\n",
                        "\n",
                        "    # Method to extract collaborations data from nature.com articles\n",
                        "    def extract_collabs(self, subject, year):\n",
                        "        articles_counter = 1\n",
                        "        page = 1\n",
                        "\n",
                        "        while True:\n",
                        "            search_url = f'{self.base_url}/search?journal=srep&article_type=research&subject={subject}&date_range={year}-{year}&order=relevance&page={page}'\n",
                        "            soup = self.retrieve_url(search_url)\n",
                        "\n",
                        "            all_atags = soup.find_all('a', {'data-track-action': 'view article'})\n",
                        "\n",
                        "            if len(all_atags) == 0:\n",
                        "                break\n",
                        "\n",
                        "            for atag in all_atags:\n",
                        "                random.seed(articles_counter)\n",
                        "\n",
                        "                if(random.random() < 0.5):\n",
                        "                    self.save_article_text(f\"./{year}/links_txts_network/\", soup, subject, atag['href'].split('/')[-1])\n",
                        "                    self.links_used_for_network.append((atag['href'], subject))\n",
                        "                \n",
                        "                    soup = self.retrieve_url(self.base_url + atag['href'])\n",
                        "\n",
                        "                    try:\n",
                        "                        authors_all = soup.findAll('p', class_='c-article-author-affiliation__address')\n",
                        "\n",
                        "                        for s in authors_all:\n",
                        "                            try:\n",
                        "                                university = self.raw_to_abstract_mapping[s.text]\n",
                        "\n",
                        "                                a_country = self.universities[university]['country']\n",
                        "                                article_id = atag['href'].split('/')[2]\n",
                        "                                if article_id not in self.links_authors_countries_used_for_network:\n",
                        "                                    self.links_authors_countries_used_for_network[article_id] = []\n",
                        "                                if a_country not in self.links_authors_countries_used_for_network[article_id]:\n",
                        "                                    self.links_authors_countries_used_for_network[article_id].append(a_country)\n",
                        "\n",
                        "                                if university not in self.collabs:\n",
                        "                                    self.collabs[university] = {}\n",
                        "\n",
                        "                                alreadyAddedUniv = []\n",
                        "                                for s2 in authors_all:\n",
                        "                                    try:\n",
                        "                                        other_university = self.raw_to_abstract_mapping[s2.text]\n",
                        "                                        if other_university not in alreadyAddedUniv:\n",
                        "                                            if university != other_university:\n",
                        "                                                    if other_university not in self.collabs[university]:\n",
                        "                                                        self.collabs[university][other_university] = {}\n",
                        "\n",
                        "                                                    if subject not in self.collabs[university][other_university]:\n",
                        "                                                        self.collabs[university][other_university][subject] = 0\n",
                        "\n",
                        "                                                    self.collabs[university][other_university][subject] += 1\n",
                        "                                            alreadyAddedUniv.append(other_university)\n",
                        "                                    except:\n",
                        "                                        continue\n",
                        "                            except:\n",
                        "                                continue\n",
                        "\n",
                        "                    except Exception as error:\n",
                        "                        print(\"[ERROR] Parse page: \", error) \n",
                        "                else:\n",
                        "                    self.links_not_used_for_network.append((atag['href'], subject))\n",
                        "                    self.save_article_text(f\"./{year}/links_txts_not_network/\", soup, subject, atag['href'].split('/')[-1])\n",
                        "\n",
                        "                articles_counter += 1  \n",
                        "            page += 1\n",
                        "        print(f\"Extracted collabs in {articles_counter} articles for {subject}\\n\")\n",
                        "\n",
                        "    # ---- METHOD FOR PREPROCESSING ----\n",
                        "\n",
                        "    # Method to create a mapping between raw and abscract university names\n",
                        "    def create_mapping_from_raw_to_abstract_university_names(self):\n",
                        "        self.raw_to_abstract_mapping = {}\n",
                        "        for new_name in self.universities.keys():        \n",
                        "            for old_name in self.universities[new_name]['init_names']:\n",
                        "                if old_name not in self.raw_to_abstract_mapping:\n",
                        "                    self.raw_to_abstract_mapping[old_name] = new_name\n",
                        "\n",
                        "    # Method to preprocess university data\n",
                        "    def preprocess_universities(self):\n",
                        "        universities = {}\n",
                        "\n",
                        "        # We first only keep scholar institute\n",
                        "        for entry, count in self.unprocessed_universities.items():\n",
                        "\n",
                        "            # Look for patterns that match university names\n",
                        "            university = None\n",
                        "            keywords = [\"Polytechnic\", \"Politecnico\", \"Escuela\", \"École\", \"Universitat\", \"Università\", \"Universität\", \"Universidad\", \"University\", \"Ecole\", \"Universiti\", \"Université\", \"College\", \"School\"]\n",
                        "\n",
                        "            for keyword in keywords:\n",
                        "                if university != None:\n",
                        "                    break\n",
                        "\n",
                        "                for i, s in enumerate(entry.split(',')):\n",
                        "                    if keyword in s:\n",
                        "                        university = entry.split(',')[i:].strip()\n",
                        "                        break\n",
                        "                    \n",
                        "            # If a university name is found, store the entry in the dictionary\n",
                        "            if university:\n",
                        "                university = unidecode(university)\n",
                        "\n",
                        "                if university in universities:\n",
                        "                    # universities[university]['init_names'].append(entry)\n",
                        "                    universities[university]['count'] += count\n",
                        "                else:\n",
                        "                    universities[university] = {\"init_names\": [entry], \"count\": count, 'country': entry.split(',')[-1].strip()}\n",
                        "            # else:\n",
                        "            #     print(f\"Not accepted this uni: {entry}\")\n",
                        "        \n",
                        "        university_names = list(universities.keys())\n",
                        "        \n",
                        "        # Use a trained model to encode universities name and then do the cosine similarity\n",
                        "        model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
                        "        embeddings = model.encode(university_names, convert_to_tensor=True)\n",
                        "        similarity_matrix = cosine_similarity(embeddings, embeddings)\n",
                        "\n",
                        "        # Create similar groups based on a threshold on the cosine similarity\n",
                        "        threshold = 0.85\n",
                        "        similar_groups = []\n",
                        "\n",
                        "        # Utilize a progress bar with tqdm\n",
                        "        with tqdm(total=len(similar_groups), desc=\"Clustering to groups\") as pbar:\n",
                        "            for i in range(len(university_names)):\n",
                        "                found = False\n",
                        "                for group in similar_groups:\n",
                        "                    if found:\n",
                        "                        break\n",
                        "                    \n",
                        "                    for j in group:\n",
                        "                        print(f\"{university_names[i].lower()} | {university_names[j].lower()}: {similarity_matrix[i, j]} -- {fuzz.token_set_ratio(university_names[i].lower(), university_names[j].lower())}\")\n",
                        "                        \n",
                        "                        if similarity_matrix[i, j] > threshold or fuzz.token_set_ratio(university_names[i].lower(), university_names[j].lower()) > 85:\n",
                        "                            group.add(i)\n",
                        "                            found = True\n",
                        "                            break\n",
                        "                if not found:\n",
                        "                    similar_groups.append({i})\n",
                        "        # Convert sets to lists in similar_groups\n",
                        "        similar_groups = [list(group) for group in similar_groups]\n",
                        "\n",
                        "        # Create abstract names containing all the same groups universities\n",
                        "        for gr in similar_groups:\n",
                        "            first_uni = gr[0]\n",
                        "\n",
                        "            if first_uni in universities:\n",
                        "                if 'init_names' not in universities[first_uni]:\n",
                        "                    universities[first_uni]['init_names'] = first_uni['init_names']\n",
                        "                if 'count' not in universities[first_uni]:\n",
                        "                    universities[first_uni]['count'] = first_uni['count']\n",
                        "                for uni in gr[1:]:\n",
                        "                    if uni in universities:\n",
                        "                        universities[first_uni]['init_names'].extend(universities[uni].get('init_names', []))\n",
                        "                        universities[first_uni]['count'] = universities.get(first_uni, {}).get('count', 0) + universities.get(uni, {}).get('count', 0)\n",
                        "                        if 'country' not in universities[first_uni] and 'country' in uni:\n",
                        "                            universities[first_uni]['country'] = uni['country']\n",
                        "                        \n",
                        "                        universities.pop(uni)\n",
                        "\n",
                        "        # Manually fix some mistakes\n",
                        "        mistakes_in_countries = {\n",
                        "            'Brasil':'Brazil',\n",
                        "            'Espana':'Spain',\n",
                        "            'UAE': 'United arab emirates',\n",
                        "            'ROC': 'Taiwan',\n",
                        "            'Northern Ireland': 'Ireland'\n",
                        "        }\n",
                        "\n",
                        "        # Use country converter to have same format for every countries\n",
                        "        cc = coco.CountryConverter()\n",
                        "\n",
                        "        for uni, values in universities.items():\n",
                        "            #Clean unicode characters like egne\n",
                        "            country = unidecode(values['country'])\n",
                        "            #Clean spelling mistakes, or not recognized names\n",
                        "            country = mistakes_in_countries.get(country,country)\n",
                        "            convert_result = cc.convert(country, to = 'name_short')\n",
                        "            #If last part of string was not a good country, we take the string before\n",
                        "            if convert_result == 'not found':\n",
                        "                convert_result = cc.convert(values['init_names'][0].split(',')[-2], to = 'name_short')\n",
                        "                \n",
                        "            universities[uni]['country'] = convert_result\n",
                        "\n",
                        "        # Remove leading and ending space\n",
                        "        universities = {key.strip(): value for key, value in universities.items()}\n",
                        "\n",
                        "        self.universities = universities\n",
                        "\n",
                        "    # ---- UTILS METHODS ----\n",
                        "    \n",
                        "    # Method to print statistics about extracted data\n",
                        "    def print_stats(self):\n",
                        "        print(f'Number of nodes: {len(self.universities)}')\n",
                        "        links = 0\n",
                        "        links_with_fields = 0\n",
                        "\n",
                        "        for u1 in self.collabs:\n",
                        "            for u2 in self.collabs[u1]:\n",
                        "                for u3 in self.collabs[u1][u2]:\n",
                        "                    links_with_fields += 1\n",
                        "                links += 1\n",
                        "\n",
                        "        print(f'Number of links (without fields): {links}')\n",
                        "        print(f'Number of links (with fields): {links_with_fields}')\n",
                        "\n",
                        "\n",
                        "    # Extract the text of all the articles in the self.links_saved and save them by subject in a folder\n",
                        "    def save_article_text(self, directory, soup, subject, id):\n",
                        "        os.makedirs(directory, exist_ok=True)\n",
                        "        try:\n",
                        "            main_content = soup.find_all(\"div\", {\"class\": \"main-content\"})\n",
                        "            if len(main_content) > 0:\n",
                        "                text = main_content[0].get_text(separator='\\n')\n",
                        "                formatted_text = ' '.join(text.split())\n",
                        "\n",
                        "                os.makedirs(directory + subject + \"/\", exist_ok=True)\n",
                        "\n",
                        "                with open(directory + subject + \"/\" + id + '.txt', \"w\") as file:\n",
                        "                    file.write(formatted_text)\n",
                        "        except:\n",
                        "            print(\"Cant save article text\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Before constructing the networks, we want to delve into our data cleaning and preprocessing choices, with a significant portion handled by the \"preprocess_universities\" function.\n",
                        "\n",
                        "Our approach to data extraction involved meticulously reviewing each article and extracting the authors' affiliation addresses. After scraping all the articles, we obtained a list of addresses, such as \"Zhejiang University of Science and Technology, Hangzhou, China.\"\n",
                        "\n",
                        "An initial challenge arose from the fact that nature.com features articles not only from university students but also, for example, from institutes and hospitals. As our focus was on universities, we needed to filter and extract only papers associated with academic institutions. To achieve this, we parsed each address, splitting it using commas and searching for keywords indicative of academic affiliations, such as \"University,\" \"Politecnico,\" \"Escuela,\" \"École,\" \"Universitat,\" \"Università,\" and so forth.\n",
                        "\n",
                        "Following this step, we obtained text representations of universities, which were then mapped to their actual addresses. For instance, \"Zhejiang University of Science and Technology\" might be mapped to ['Zhejiang University of Science and Technology, Hangzhou, China']. Subsequently, our goal was to generate more abstract representations of these university names. To achieve this, we employed a trained model to encode university names and calculated the cosine similarity to create a similarity matrix. We then grouped the names into clusters, considering names with a cosine similarity above a predefined threshold.\n",
                        "\n",
                        "As an illustration, considering the previous example, we aimed to merge similar nodes such as 'Zhejiang University of Science and Technology, Hangzhou, China' and 'Zhejiang University, Hangzhou, China' into a single node, represented as 'Zhejiang University.'\n",
                        "\n",
                        "Additionally, in the preprocessing phase, we incorporated a library to associate each specific country with a generic representation (allowing subsequent analysis to be conducted using abstract country names)."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Part 2: Creation of the network\n",
                        "\n",
                        "Now that we've implemented a scraper to acquire and preprocess data, we can construct our network. In this network, each node corresponds to a university along with its respective country. An edge in the network signifies a collaboration on a specific subject between two universities. The network is undirected.\n",
                        "\n",
                        "Our approach involves creating individual edges for each subject-specific collaboration between universities. Additionally, we include an overarching edge between two universities, denoted by the subject attribute \"All.\" The weight of this overall edge is set as the sum of the weights of all subject-specific collaborations between the two universities."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# The year we want to load/study (cause we organized our folder by years)\n",
                        "year_folder = 2023"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Create scraper object and load universities / collabs\n",
                        "data = Scraper()\n",
                        "data.load_universities(str(year_folder)+\"/universities.json\")\n",
                        "data.load_collabs(str(year_folder)+\"/collabs.json\")\n",
                        "\n",
                        "# Create the graph\n",
                        "G = nx.MultiGraph()\n",
                        "\n",
                        "# Add nodes in the network\n",
                        "for uni in data.universities:\n",
                        "    G.add_node(uni, country= data.universities[uni]['country'])\n",
                        "\n",
                        "# Add edges in the network\n",
                        "for source_uni, target_unis in data.collabs.items():\n",
                        "    for target_uni, collaborations in target_unis.items():\n",
                        "        if(target_uni in G.nodes() and source_uni in G.nodes()):\n",
                        "            overall_weight = 0\n",
                        "            for subject, weight in collaborations.items():\n",
                        "                G.add_edge(source_uni, target_uni, subject=subject, weight=weight)\n",
                        "                overall_weight += weight\n",
                        "\n",
                        "            # Add the overall edge with field 'All' and the sum of subject weights as weight\n",
                        "            G.add_edge(source_uni, target_uni, subject='All', weight=overall_weight)\n"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "With our network now established, the next step involves eliminating isolated nodes from the graph. Isolated nodes refer to universities that do not engage in any collaborations within the given network context. Removing these nodes streamlines the network representation by focusing on interconnected universities, enhancing the clarity and relevance of the collaborative relationships within the system."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "isolated_nodes = list(nx.isolates(G))\n",
                        "print(f\"Number of isolated nodes is {len(isolated_nodes)}\")\n",
                        "G.remove_nodes_from(isolated_nodes)\n",
                        "print(f\"Number of nodes after removal is {len(G.nodes())}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We will also remove the selfloops cause we want to study the collaborations between different universities."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "G.remove_edges_from(list(nx.selfloop_edges(G)))"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Part 3: Creation of the subgraphs based on subject\n",
                        "\n",
                        "The next step is to create different networks (one for each subject).\n",
                        "\n",
                        "To do this, we will start by defining a color coding for every subject, and use such color coding for visual representations.\n",
                        "\n",
                        "We picked colors from the nature.com color coding, so\n",
                        "* for the field of physical sciences, we have shades of blue\n",
                        "* for the field of Earth and Environmental sciences, we have shades of green\n",
                        "* for the field of Biological sciences, we have shades of purple\n",
                        "* for the field of Health sciences, we have shades of red\n",
                        "* for the field \"All\", we will take the black"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "subj_color_mapping = {\n",
                        "    'health-care': '#FFBF00', #Orange, yellowish\n",
                        "    'mathematics-and-computing': '#00008B', #Blue\n",
                        "    'genetics': '#702963', #Purple\n",
                        "    'ecology': '#50C878', #Green\n",
                        "    'diseases': '#FF5F1F', #Orange danger\n",
                        "    'microbiology': '#CCCCFF',  #Purple\n",
                        "    'physics': '#7393B3', #Blue\n",
                        "    'All':'#000000' #Black\n",
                        "}"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Distribution of edges across different subjects\n",
                        "\n",
                        "Before creating the subnetworks, we can proceed to generate a histogram to visualize the distribution of edges across different subjects. This will provide insights into the quantity of data available for each subject, allowing us to identify areas with more extensive collaboration data."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def subject_distribution(subjects, first_n):\n",
                        "    distrib = {}\n",
                        "    for subject, weight in subjects:\n",
                        "        if subject != 'All':\n",
                        "            distrib[subject] = distrib.get(subject,0) + weight\n",
                        "    width = 0.8 # width of the bars\n",
                        "\n",
                        "    fig, ax = plt.subplots()\n",
                        "\n",
                        "    ordered_items = sorted(distrib.items(),key=lambda x: x[1], reverse = True)[:first_n]\n",
                        "    xvalues = [subject for subject, value in ordered_items]\n",
                        "    yvalues = [value for subject, value in ordered_items]\n",
                        "    \n",
                        "    colors = [subj_color_mapping[subject] for subject in xvalues]\n",
                        "\n",
                        "    rects1 = ax.bar(xvalues, yvalues, width, color=colors)\n",
                        "    ax.set_title(\"Subject histogram\")\n",
                        "    ax.set_xlabel(\"Subject\")\n",
                        "    ax.set_xticks(range(len(xvalues)),xvalues, rotation = 90) # set the position of the x ticks\n",
                        "    ax.set_ylabel(\"# of link with this topic\")\n",
                        "    plt.show()\n",
                        "\n",
                        "\n",
                        "#Analyzing subjects\n",
                        "subjects = [(attr['subject'], attr['weight']) for node1, node2, attr in G.edges(data=True)]\n",
                        "first_n = 10\n",
                        "subject_distribution(subjects, first_n)\n",
                        "subject_set = set([subj for subj, weight in subjects])"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Observing the histogram, it becomes evident that health-care and diseases and microbiology are the three prominent fields, followed by the clusters of ecology, genetics. In contrast, mathematics and physics exhibit a lower collaboration level. \n",
                        "\n",
                        "This leads us into thinking that the universities in field of physics and mathematics are collaborating less during 2023 than other fields.\n",
                        "\n",
                        "__FUTURE WORK__: study the evolution over time of this plot, see if it has always been the case or not."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Creation of the subnetworks\n",
                        "\n",
                        "Having gained a clearer understanding of the data distribution across subjects, our next step involves building subnetworks. Each subject will be represented as an individual network, and concurrently, we will create a comprehensive subnetwork encompassing all connections between universities, regardless of the subject. This approach allows us to explore both subject-specific collaborations and the overarching collaborative landscape across all subjects."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Create a subgraph for a certain subject\n",
                        "def create_subgraph(G, subj):\n",
                        "    edges = set([(node1, node2, key) for node1, node2 in G.edges() for key, value in G.get_edge_data(node1, node2).items() if value['subject'] == subj])\n",
                        "    graph = G.edge_subgraph(edges)\n",
                        "\n",
                        "    # Remove isolated nodes\n",
                        "    isolated_nodes = list(nx.isolates(graph))    \n",
                        "    if isolated_nodes:\n",
                        "        graph.remove_nodes_from(isolated_nodes)\n",
                        "\n",
                        "    return graph\n",
                        "    "
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can now use the utility function to save all the graphs in a dictionary."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "graphs = {}\n",
                        "for subj in subj_color_mapping.keys():\n",
                        "    graphs[subj] = create_subgraph(G, subj)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Creation of the degree distributions\n",
                        "\n",
                        "Subsequently, we can access the desired graphs through the 'graphs' dictionary. Our next objective is to generate the corresponding degree distributions for each graph within this dictionary. \n",
                        "\n",
                        "Since we are in a graph with weighted edges, the degree of a node is the sum of the weights of all the edges incident to that node."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Get the degree of a node\n",
                        "def get_node_degree(graph, node):\n",
                        "    sum = 0\n",
                        "    for fr, to, d in graph.edges(node, data=True):\n",
                        "        sum = sum + d['weight']\n",
                        "    return sum"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "degrees = {}\n",
                        "sorted_degrees = {}\n",
                        "for subj in graphs:\n",
                        "    degrees[subj] = [(n, graphs[subj].nodes[n]['country'], get_node_degree(graphs[subj], n)) for n in graphs[subj].nodes]\n",
                        "    sorted_degrees[subj] = sorted(degrees[subj], key=lambda x: x[2], reverse=True)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Part 4: Analysis of the subnetworks"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now that we have access to both graphs and degree distributions, we can start the analysis of networks to uncover any noteworthy patterns.\n",
                        "\n",
                        "### Utilities functions for analysis\n",
                        "\n",
                        "To facilitate this analysis, we'll begin by creating two utility functions – one for populate bar charts and another for rendering network visualizations and bar charts for each subject. These functions will serve as valuable tools in our exploration of potential patterns within the collaborative networks."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def populate_plot(data, subj, fig, grid, labels):\n",
                        "    dmax = max(data[:])\n",
                        "    dmin = min(data[:])\n",
                        "    \n",
                        "    #Bin plot\n",
                        "    bins = np.arange(dmin, dmax+2)\n",
                        "    hist, bins = np.histogram(data[:], bins)\n",
                        "    \n",
                        "    ax = fig.add_subplot(grid) \n",
                        "    ax.bar(bins[:-1], hist, width=1, align='center', edgecolor='k', color = subj_color_mapping[subj])\n",
                        "    ax.set_title(f'{subj}')\n",
                        "    ax.set_xlabel(labels['xlabel'])\n",
                        "    ax.set_ylabel('How many')\n",
                        "    return ax"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def drawLogLogPoints(data, subj, fig, grid, labels):\n",
                        "    dmax = max(data[:])\n",
                        "    dmin = min(data[:])\n",
                        "    \n",
                        "    #Bin plot\n",
                        "    bins = np.arange(dmin, dmax+2)\n",
                        "    hist, bins = np.histogram(data[:], bins)\n",
                        "    \n",
                        "    ax = fig.add_subplot(grid) \n",
                        "    ax.loglog(bins[:-1], hist,'o', color = subj_color_mapping[subj])\n",
                        "    ax.set_title(f'{subj}')\n",
                        "    ax.set_xlabel(labels['xlabel'])\n",
                        "    ax.set_ylabel('How many')\n",
                        "    return ax"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "#Only requires subject associated with list of data to barplot\n",
                        "def create_visual_graph(subj_data_dict, labels, category='bar'):\n",
                        "    fig = plt.figure(\"Distribution\", figsize=(15, 30))\n",
                        "    \n",
                        "    # Create a gridspec for adding subplots of different sizes\n",
                        "    axgrid = fig.add_gridspec(4, 2)\n",
                        "    \n",
                        "    axes = []\n",
                        "    for i, grid in enumerate(axgrid):\n",
                        "        if category == 'bar':\n",
                        "            axes.append(populate_plot(list(subj_data_dict.values())[i], list(subj_data_dict.keys())[i], fig, grid, labels))\n",
                        "        if category == 'network2d':\n",
                        "            axes.append(drawNetwork2D(list(subj_data_dict.values())[i], list(subj_data_dict.keys())[i], fig, grid, labels))\n",
                        "        if category == 'network3d':\n",
                        "            graph = list(subj_data_dict.values())[i].subgraph(sorted(nx.connected_components(list(subj_data_dict.values())[i]), key=len, reverse=True)[0])\n",
                        "            positions = nx.spring_layout(graph, k=1.2, scale=8)\n",
                        "            axes.append(drawNetwork3D(graph, positions=positions, fig=fig, grid=grid, name=labels[i]))\n",
                        "        elif category == 'loglog':\n",
                        "            axes.append(drawLogLogPoints(list(subj_data_dict.values())[i], list(subj_data_dict.keys())[i], fig, grid, labels))\n",
                        "    \n",
                        "    fig.tight_layout()\n",
                        "    plt.show()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Count of Nodes and Edges and top 3 universities\n",
                        "\n",
                        "To start our analysis, we'll start by examining the number of nodes and edges within each subnetwork. This initial exploration will provide us with fundamental insights into the structure and scale of each collaborative network"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "for subj in graphs:\n",
                        "    graph = graphs[subj]\n",
                        "    print(f\"Graph on \\033[1m{subj}\\033[0m has {len(graph.nodes())} nodes and {len(graph.edges())} edges.\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can now get the top 3 universities for each subject."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "for subj in graphs:\n",
                        "    print(f\"Top 3 universities \\033[1m({subj})\\033[0m:\")\n",
                        "    for i, (name, country, degree) in enumerate([(name, country, degree) for name, country, degree in sorted_degrees[subj]][:3], 1):\n",
                        "        print(f\"{i}. University: {name} ({country}), Number of collaborations: {degree}\")\n",
                        "    print(\"\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "The first thing we directly remark is, of course, that the University of Copenhagen is the second in ecology and the University of Southern Denarmk is the third in ecology (wouhou!).\n",
                        "\n",
                        "Interestingly, MIT is conspicuously absent from the top 3 in any section, in contrast to the noteworthy presence of Saudi Arabian universities in these leading positions.\n",
                        "\n",
                        "It's crucial to acknowledge that collaboration rankings are subject to fluctuations based on the dataset and the specific timeframe. In this case, our analysis is confined to nature.com articles from the year 2023.\n",
                        "\n",
                        "Regarding MIT, potential reasons for its absence might extend beyond a preference for solo study. Factors such as specific research focuses, international collaboration strategies, or the nature of collaborations pursued by MIT could be contributing to the observed pattern."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Degree distributions\n",
                        "\n",
                        "Let's now draw all the degree distributions:"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "subj_degrees_dict = {}\n",
                        "labels = {'xlabel':'Degree'}\n",
                        "for subject, graph in graphs.items():\n",
                        "    subj_degrees_dict[subject] = list(map(lambda x: x[2], sorted_degrees[subject]))\n",
                        "    \n",
                        "create_visual_graph(subj_degrees_dict, labels, 'bar')"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "What is interesting to see is that all distributions exhibit an alternating behaviour, where nodes with even degrees are very frequent while nodes with odd degrees aren't as much. This provides insights about universities cooperation: considering many researchers of the same paper can be from the same university, it is probably harder to work in cluster of 4,6,8 universities and so on. \n",
                        "\n",
                        "Disregarding this alternating behaviour, the distribution overall seems to have exponential shape, with very few nodes having a high degree(hubs). We will check properties of these networks in the following section."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Checking scale-freeness"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Log-Log plot"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "create_visual_graph(subj_degrees_dict, labels, 'loglog')"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Checking nodes assortativity, with and without country differentiation"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "#Degree assortativity\n",
                        "for subj, graph in graphs.items():\n",
                        "    print(f'Subj \\033[1m{subj}\\033[0m:')\n",
                        "    degree_assortativity = nx.degree_assortativity_coefficient(graph)\n",
                        "    print(f\"\\tDegree assortativity: {degree_assortativity}\")\n",
                        "\n",
                        "    #Country assortativity\n",
                        "    country_assortativity = nx.attribute_assortativity_coefficient(graph, 'country')\n",
                        "    print(f\"\\tCountry assortativity: {country_assortativity}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "A positive assortativity coefficient indicates assortative mixing, while a negative coefficient suggests disassortative mixing, where nodes tend to connect to nodes with different characteristics.\n",
                        "\n",
                        "We can see from the above results that in general, nodes in our networks have a quite strong tendency to connect with nodes with similar degrees. These means that hubs connect to each other, possibly leading to small world properties. Also, because of this, we can say that our network is most likely not a scale-free network. This is an expected result, given what's written in __Quote PAPER Found__.\n",
                        "\n",
                        "Additionally, nodes tend to link with other nodes from the same country."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "#### Verifying small world property: Average shortest path"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Here, we compute the average shortest path to verify if the network exhibits small world properties"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "for subj, graph in graphs.items():\n",
                        "    Gcc = sorted(nx.connected_components(graph), key=len, reverse=True)[0]\n",
                        "    connected_graph = graph.subgraph(Gcc).copy()\n",
                        "    asp = nx.average_shortest_path_length(connected_graph, weight='weight')\n",
                        "    print(f\"Subject \\033[1m{subj}\\033[0m has average shortest path: {asp}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "All the subnetwork exhibit low average short distances"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Clustering coefficent"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "for subj, graph in graphs.items():\n",
                        "    undirected = nx.Graph(graph)\n",
                        "    ac = nx.average_clustering(undirected, weight='weight')\n",
                        "    print(f\"Subject \\033[1m{subj}\\033[0m has average clustering coefficent of: {ac}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Friendship paradox"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "#Friendship part\n",
                        "\n",
                        "iterations = 1000\n",
                        "for subj, graph in graphs.items():\n",
                        "    count_f_paradox = 0\n",
                        "    for i in range(iterations):\n",
                        "        random_node = random.choice(list(graph.nodes()))\n",
                        "        random_node_degree = graph.degree[random_node]\n",
                        "        random_node_neighbors = list(graph.neighbors(random_node))\n",
                        "        #print(random_node_degree, random_node_neighbors)\n",
                        "        avg_neighbor_degree =  []\n",
                        "\n",
                        "        for neighbor in random_node_neighbors:\n",
                        "            avg_neighbor_degree.append(graph.degree[neighbor])\n",
                        "        avg_neighbor_degree = sum(avg_neighbor_degree)/len(avg_neighbor_degree)\n",
                        "\n",
                        "        if avg_neighbor_degree > len(random_node_neighbors):\n",
                        "            count_f_paradox+=1\n",
                        "    print(f\"Subject \\033[1m{subj}\\033[0m: out of {iterations} tests, the paradox is true {count_f_paradox} times\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Exponent using the powerlaw package\n",
                        "\n",
                        "Let's now try to find the exponent of the degree distributions for each subject."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "for subj in graphs:\n",
                        "    # Fit the power-law distribution\n",
                        "    fit = powerlaw.Fit(list(map(lambda x: x[2], degrees[subj])), verbose=False)\n",
                        "    exponent = fit.alpha\n",
                        "    print(f\"Powerlaw of \\033[1m{subj}\\033[0m is {fit.alpha}\");"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "The power-law results offer valuable insights into the distribution of collaborations among universities in diverse academic disciplines. The consistently high exponents observed in fields such as Health Care, Microbiology, and Physics indicate a significant concentration of collaborative activities among a few key institutions. This suggests a pattern where certain universities play pivotal roles in fostering collaborations within these disciplines. On the other hand, the lower exponents in Ecology and the overarching \"All\" category suggest a more evenly spread distribution of collaborations, implying a broader engagement across a wider array of institutions in these areas.\n",
                        "\n",
                        "__QUOTE 4.7 barabasi book__"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Connected components\n",
                        "\n",
                        "Let's now focus on the connected component, we can try to print the connected component size distribution for each subject."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "##How many connected components? And how big are they?\n",
                        "def connected_component_size_distribution(G, subj):\n",
                        "    Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
                        "    print(f\"Subject \\033[1m{subj}\\033[0m has {len(Gcc)} connected components\")\n",
                        "    Gcc_len_distrib = [len(cc) for cc in Gcc]\n",
                        "    return Gcc_len_distrib"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "subj_cc_size_dict = {}\n",
                        "labels = {'xlabel':'Amount of nodes per CC'}\n",
                        "for subject, graph in graphs.items():\n",
                        "    subj_cc_size_dict[subject] = connected_component_size_distribution(graph, subject)\n",
                        "\n",
                        "create_visual_graph(subj_cc_size_dict, labels, 'bar')"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can see that in most cases, there is a component that far exceeds the others in terms of size "
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Universities centrality\n",
                        "\n",
                        "We will now start answering our question using __network centrality__. So, we are now going to investigate which universities are most central in our network, and therefore which country has the highest mean centrality with respect to a particular field."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Degree centrality"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def compute_degree_centrality(G, subj, n_first):\n",
                        "    centrality = nx.degree_centrality(G)\n",
                        "    centrality_list = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
                        "    \n",
                        "    if n_first != 0:\n",
                        "        most_central_universities = centrality_list[:n_first]\n",
                        "        print(f\"Most central uni's of \\033[1m{subj}\\033[0m\")\n",
                        "        pprint(most_central_universities)\n",
                        "        print(\"\\n\")\n",
                        "    \n",
                        "    return centrality"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "degree_centralities = {}\n",
                        "\n",
                        "n_first = 3\n",
                        "for subj, graph in graphs.items():\n",
                        "    degree_centralities[subj] = compute_degree_centrality(graph, subj, n_first)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Betweennes centrality"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def compute_betweenness_centrality(G, subj, n_first):\n",
                        "    centrality = nx.betweenness_centrality(G, weight='weight', seed=12345)\n",
                        "    centrality_list = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
                        "    \n",
                        "    if n_first != 0:\n",
                        "        most_central_universities = centrality_list[:n_first]\n",
                        "        print(f\"Most central uni's of \\033[1m{subj}\\033[0m\")\n",
                        "        pprint(most_central_universities)\n",
                        "        print(\"\\n\")\n",
                        "        \n",
                        "    return centrality"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "betweenness_centralities = {}\n",
                        "\n",
                        "n_first = 3\n",
                        "for subj, graph in graphs.items():\n",
                        "    betweenness_centralities[subj] = compute_betweenness_centrality(graph, subj, n_first)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Create a scatter plot to compare centrality measures\n",
                        "def compare_centrality_scatter(ax, centrality1, centrality2, labels, subj):\n",
                        "    x = [centrality1[node] for node in centrality1]\n",
                        "    y = [centrality2[node] for node in centrality2]\n",
                        "\n",
                        "    ax.scatter(x, y, alpha=0.5)\n",
                        "    ax.set_title(f'Comparison of Centrality Measures - {subj}')\n",
                        "    ax.set_xlabel('Betweenness Centrality')\n",
                        "    ax.set_ylabel('Degree Centrality')"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Assuming `graphs` is a dictionary with subjects as keys and corresponding graphs as values\n",
                        "fig, axs = plt.subplots(4, 2, figsize=(15, 15))\n",
                        "\n",
                        "for i, (subj, graph) in enumerate(graphs.items()):\n",
                        "    betweenness_centrality_data = betweenness_centralities[subj]\n",
                        "    degree_centrality_data = degree_centralities[subj]\n",
                        "\n",
                        "    # Calculate subplot position\n",
                        "    row = i // 2\n",
                        "    col = i % 2\n",
                        "\n",
                        "    # Create subplots\n",
                        "    ax = axs[row, col]\n",
                        "    compare_centrality_scatter(ax, betweenness_centrality_data, degree_centrality_data, labels=list(graph.nodes), subj=subj)\n",
                        "    ax.set_title(subj)\n",
                        "\n",
                        "# Adjust layout to prevent overlap\n",
                        "plt.tight_layout()\n",
                        "plt.show()\n"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Countries Centrality"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Scatter plot comparing degree and betweenness centrality"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Countries Centrality"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Degree centrality according to country (average)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "country_degree_centralities = {}\n",
                        "\n",
                        "n_first = 3\n",
                        "for subj, graph in country_graphs.items():\n",
                        "    country_degree_centralities[subj] = compute_degree_centrality(graph, subj, n_first)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Betweenness centrality according to country (average)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "country_betweenness_centralities = {}\n",
                        "\n",
                        "n_first = 3\n",
                        "for subj, graph in country_graphs.items():\n",
                        "    country_betweenness_centralities[subj] = compute_betweenness_centrality(graph, subj, n_first)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "##### Scatter plot according to country centrality"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Assuming `graphs` is a dictionary with subjects as keys and corresponding graphs as values\n",
                        "fig, axs = plt.subplots(4, 2, figsize=(15, 15))\n",
                        "\n",
                        "for i, (subj, graph) in enumerate(country_graphs.items()):\n",
                        "    country_betweenness_centrality_data = country_betweenness_centralities[subj]\n",
                        "    country_degree_centrality_data = country_degree_centralities[subj]\n",
                        "\n",
                        "    # Calculate subplot position\n",
                        "    row = i // 2\n",
                        "    col = i % 2\n",
                        "\n",
                        "    # Create subplots\n",
                        "    ax = axs[row, col]\n",
                        "    compare_centrality_scatter(ax, country_betweenness_centrality_data, country_degree_centrality_data, labels=list(country_graph.nodes), subj=subj)\n",
                        "    ax.set_title(subj)\n",
                        "\n",
                        "# Adjust layout to prevent overlap\n",
                        "plt.tight_layout()\n",
                        "plt.show()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "TODO: Comments about centrality"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Part 5: Networks plotting\n",
                        "\n",
                        "Now, we'll attempt to visualize our network in both 2D and 3D. \n",
                        "To facilitate this, we'll begin by defining some utility functions."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Utilities functions"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Generate colors for x countries\n",
                        "def get_color_map(top_countries):\n",
                        "    dict_color = {}\n",
                        "    colors = plt.cm.get_cmap('hsv', len(top_countries) + 2)\n",
                        "    \n",
                        "    for i, d in enumerate(top_countries):\n",
                        "        dict_color[d] = colors(i)\n",
                        "    \n",
                        "    dict_color['other'] = colors(len(top_countries))\n",
                        "\n",
                        "    return dict_color"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Get the 10 biggest countries\n",
                        "def get_10_biggest_countries(subj):\n",
                        "    countries_count = {}\n",
                        "    while len(countries_count) < 10:\n",
                        "        for n, c, w in sorted_degrees[subj]:\n",
                        "            if c not in countries_count:\n",
                        "                countries_count[c] = 0\n",
                        "            countries_count[c] += 1\n",
                        "            \n",
                        "            if len(countries_count) >= 10:\n",
                        "                break\n",
                        "    \n",
                        "    return list(map(lambda x: x[0], sorted(countries_count.items(), key=lambda x:x[1], reverse=True)[:8]))"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### 2D Networks plotting"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def drawNetwork2D(G, subj, fig, grid, labels, scaling_factor=1):   \n",
                        "    #We draw only the biggest component/we should use the biggest component in general\n",
                        "    Gcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\n",
                        "        \n",
                        "    #color_list = [subj_color_mapping[attr['subject']] for node1, node2, attr in G.edges(data=True)]\n",
                        "    color = subj_color_mapping[subj]\n",
                        "    size_measure = dict(Gcc.degree)\n",
                        "    #pos = nx.kamada_kawai_layout(Gcc)\n",
                        "    pos = nx.spring_layout(Gcc)\n",
                        "    \n",
                        "    nodes_conf = {'node_size':[v*scaling_factor for v in size_measure.values()], 'linewidths':1, 'edgecolors':'black'}\n",
                        "    \n",
                        "    edges_conf = {'width':0.7, 'alpha':0.8, 'edge_color':color}\n",
                        "\n",
                        "    ax = fig.add_subplot(grid)\n",
                        "    nx.draw_networkx_nodes(Gcc, pos=pos, ax = ax, **nodes_conf)\n",
                        "    nx.draw_networkx_edges(Gcc, pos=pos, ax=ax, **edges_conf)\n",
                        "    ax.set_title(f\"{labels['title']} of {subj} network\")\n",
                        "    ax.set_axis_off()\n",
                        "    return ax"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "labels = {'title':'Biggest component'}\n",
                        "create_visual_graph(graphs, labels, 'network2d')"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "In this networks, what is clear is that there is no supernode, connected to almost every node in the network and working as a hub. There are many groups of nodes clustering together, probably to form communities.\n",
                        "\n",
                        "In this setting, it looks clear that these networks don't follow a powerlaw distribution."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### 3D Networks plotting"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def drawNetwork3D(graph, positions, fig, grid, name):\n",
                        "\n",
                        "    node_sizes = [4 * get_node_degree(graph, n) for n in graph.nodes]\n",
                        "\n",
                        "    top_countries = get_10_biggest_countries(name)\n",
                        "    dict_color = get_color_map(top_countries)\n",
                        "\n",
                        "    node_colors = [dict_color[node['country']] if node['country'] in top_countries else dict_color['other'] for v, node in graph.nodes(data=True)]\n",
                        "    \n",
                        "    ax = fig.add_subplot(grid, projection='3d')\n",
                        "\n",
                        "    xs = [pos[0] for pos in positions.values()]\n",
                        "    ys = [pos[1] for pos in positions.values()]\n",
                        "    zs = [node_size for node_size in node_sizes]\n",
                        "\n",
                        "    alreadyDisplayed = []  \n",
                        "    i = 0\n",
                        "    for edge in graph.edges():\n",
                        "        node1 = positions[edge[0]]\n",
                        "        node2 = positions[edge[1]]\n",
                        "        alr = 0\n",
                        "        for (n1, n2) in alreadyDisplayed:\n",
                        "            if n1[0] == node1[0] and n1[1] == node1[1] and n2[0] == node2[0] and n2[1] == node2[1]:\n",
                        "                alr = 1\n",
                        "        if not alr:\n",
                        "            x_values = [node1[0], node2[0]]\n",
                        "            y_values = [node1[1], node2[1]]\n",
                        "            z_values = [node_sizes[list(graph.nodes).index(edge[0])], node_sizes[list(graph.nodes).index(edge[1])]]\n",
                        "\n",
                        "            ax.plot(x_values, y_values, z_values, alpha=0.3)\n",
                        "            alreadyDisplayed.append((node1, node2))\n",
                        "            alreadyDisplayed.append((node2, node1))\n",
                        "        i = i + 1\n",
                        "\n",
                        "    ax.scatter(xs, ys, zs, s=node_sizes, c=node_colors, depthshade=True)\n",
                        "\n",
                        "    custom_legend = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=label) for label, color in dict_color.items()]\n",
                        "    first_legend = ax.legend(handles=custom_legend, title=\"Country\", loc='upper right')\n",
                        "\n",
                        "    ax.set_title(f\"{name} biggest component (3D)\")\n",
                        "\n",
                        "    ax.set_xticks([])\n",
                        "    ax.set_yticks([])\n",
                        "    ax.set_zticks([])\n",
                        "    return ax"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "create_visual_graph(graphs, list(subj_color_mapping.keys()), 'network3d')"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "__VERY IMPORTANT TO COMMENT HERE__"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Part 6: Community detection"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def compute_modularity(graph, partition_nodes):\n",
                        "        L = len(graph.edges())\n",
                        "        #compute k_c\n",
                        "        degrees = [y for x,y in graph.degree(partition_nodes)]\n",
                        "        k_c = sum(degrees)\n",
                        "        \n",
                        "        #get L_c\n",
                        "        comm =graph.subgraph(partition_nodes)\n",
                        "        L_c = len(comm.edges())\n",
                        "        #get M_c\n",
                        "        M_c = L_c/L - (k_c/(2*L))**2\n",
                        "        \n",
                        "        return M_c"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Communities using louvain algorithm"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "partitions = {}\n",
                        "for subj in graphs:\n",
                        "    partitions[subj] = community_louvain.best_partition(graphs[subj])\n",
                        "\n",
                        "    partitions_number = max(partitions[subj].values())+1\n",
                        "    print(f\"The number of partitions for \\033[1m{subj}\\033[0m is {partitions_number}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's now print the distribution of community sizes."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "subj_cc_size_dict = {}\n",
                        "labels = {'xlabel':'Community id'}\n",
                        "for subject, graph in graphs.items():\n",
                        "    subj_cc_size_dict[subject] = [v for k,v in partitions[subject].items()]\n",
                        "\n",
                        "create_visual_graph(subj_cc_size_dict, labels, 'bar')"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can see that there are a lot of communities with a high number of nodes (from 5 to 8 approximately)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "communities_graphs = {}\n",
                        "\n",
                        "for subj in graphs:\n",
                        "    partitions[subj] = community_louvain.best_partition(graphs[subj])\n",
                        "\n",
                        "    partitions_number = max(partitions[subj].values())+1\n",
                        "\n",
                        "    M = 0\n",
                        "    for i in range(max(partitions[subj].values())):\n",
                        "        c_graph = graphs[subj].copy()\n",
                        "\n",
                        "        random.seed(i)\n",
                        "        blue_color_percentage = random.random()\n",
                        "        red_color_percentage = random.random()\n",
                        "        green_color_percentage = random.random()\n",
                        "        community_color = (red_color_percentage, green_color_percentage, blue_color_percentage)\n",
                        "        \n",
                        "        nodes_colors= [(nodes, community_color) for nodes in partitions[subj].keys() if partitions[subj][nodes] == i]\n",
                        "        nodes, colors = zip(*nodes_colors)\n",
                        "        \n",
                        "        nodes_colors = dict(nodes_colors)\n",
                        "        nx.set_node_attributes(c_graph, nodes_colors, 'color')\n",
                        "        \n",
                        "    \n",
                        "        M+=compute_modularity(c_graph, nodes)\n",
                        "        communities_graphs[subj] = c_graph\n",
                        "    \n",
                        "    print(f\"The modularity of the partition for \\033[1m{subj}\\033[0m is {M}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "It is pretty clear from the 2D plot of the networks above that every network is well suited for communities. In most of them, some areas are dense with nodes interconnected to each other, with few nodes outside that have some (but not a lot of) connections. The network where this phenomena is lesss visible is indeed physics, hence the low modularity. Still, even this community is very well defined. So there is something that brings universities to choose to work together instead of starting a new collaboration with university outside the cluster. This could either be:\n",
                        "* country\n",
                        "* top universities always collaborate with each other, which we know happens thanks to assortativity measurement"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Community split according to countries"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "#Computing dict of {country:number of unis}\n",
                        "countries = [attr['country'] for node, attr in graphs['All'].nodes(data=True)]\n",
                        "countries_dict = Counter(countries)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "print('PARTITION ACCORDING TO COUNTRY:')\n",
                        "for subject, graph in graphs.items():\n",
                        "    node_countries = [(node, attr['country']) for node, attr in graph.nodes(data=True)]\n",
                        "    M = 0\n",
                        "    for ref_country in countries_dict.keys():\n",
                        "        nodes = [node for node, country in node_countries if country == ref_country]\n",
                        "        mod = compute_modularity(graph, nodes)\n",
                        "        M+=mod\n",
                        "    print(f'Modularity for subject \\033[1m{subject}\\033[0m is: {M}')"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "In general the partition could be better, but is still good, especially in some fields. This implies that universities from the same country actually tend to work together for some topics like life sciences. This is most likely to happen because of similar ethics and health legislations, or different education in the field with respect to other countries.  "
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Significance test to see the difference\n",
                        "We shuffle the country labels and randomly reassign them, to see if it's random or not the country partition"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "iterations_precision = 3\n",
                        "print('PARTITION ACCORDING TO COUNTRY, RANDOMIZED:')\n",
                        "print(f\"\\t showing mean over {iterations_precision} trials\")\n",
                        "for subj, graph in graphs.items():\n",
                        "    #Original values to use back when simulation is over\n",
                        "    G = graph.copy()\n",
                        "    original_values = dict(G.nodes(data=True))\n",
                        "    mean_M= []\n",
                        "    nodes = G.nodes()\n",
                        "\n",
                        "    #Get graph countries with repetitions\n",
                        "    countries_labels = [attr['country'] for node, attr in G.nodes(data=True)]\n",
                        "    \n",
                        "    for i in range(iterations_precision):\n",
                        "        M = 0\n",
                        "        #Shuffle them\n",
                        "        new_labels = random.shuffle(countries_labels)\n",
                        "        new_attr = dict(map(lambda i,j : (i,j) , nodes , countries_labels)) \n",
                        "\n",
                        "        #Reassign the new dict value\n",
                        "        nx.set_node_attributes(G, new_attr , 'country')\n",
                        "        node_countries = [(node, attr['country']) for node, attr in G.nodes(data=True)]\n",
                        "        #Compute modularity\n",
                        "        for ref_country in countries_dict.keys():\n",
                        "            nodes = [node for node, country in node_countries if country == ref_country]\n",
                        "            mod = compute_modularity(G, nodes)\n",
                        "            M+=mod\n",
                        "            \n",
                        "        mean_M.append(M)\n",
                        "    print(f'\\t \\t Modularity for subject \\033[1m{subj}\\033[0m is: {sum(mean_M)/len(mean_M)}')\n",
                        "    nx.set_node_attributes(G, original_values)\n",
                        "      \n",
                        "        "
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can see that, displaying the node country attribute at random, the modularity is very low compared to the previous one. This shows that indeed, university from the same country tend to cooperate more."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "__FUTURE WORK__: It could be interesting to study if this modularity of countries has increased by the years, and correlate it to the level of globalization, and the pace globalization is going at."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Part 7: Analyzing wordclouds to understand trends in research"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "During the data scraping process, we made a deliberate choice to exclude certain articles from the creation of the network. Instead, we reserved them specifically for text processing purposes. Utilizing the capabilities of the scraper, we stored the texts of these articles in the 'links_txts' folder, organized by subject, with each subject having its own dedicated folder (after 6 hours of parallel scraping!)."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's try now to do beautiful word clouds! To do this we will use some preprocessing to tokenize, remove_punctuation, lowercase and lemmatize tokens.\n",
                        "\n",
                        "After this, we will be able to get our term frequency dictionaries."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Data cleaning"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def dict_to_sorted_list(dictionary, reverse=True):\n",
                        "    ret_list = list(dictionary.items())\n",
                        "    ret_list = sorted(ret_list, key= lambda x:x[1], reverse=reverse)\n",
                        "    return ret_list\n",
                        "\n",
                        "\n",
                        "class custom_tokenizer():\n",
                        "    def tokenize(text):\n",
                        "        return word_tokenize(text)\n",
                        "    \n",
                        "def clean_text(tokens_list):\n",
                        "    cleaned_text = [lemmatizer.lemmatize(word.lower()) for word in tokens_list if word not in string.punctuation]\n",
                        "    cleaned_text = [word for word in cleaned_text if word not in stopwords.words('english')]\n",
                        "    #Delete lemmatization errors like 'wa' and numbers that occur for sections and similars\n",
                        "    cleaned_text = [word for word in cleaned_text if not word.isnumeric() and word != '-' and len(word)>2]\n",
                        "    #Delete web references\n",
                        "    cleaned_text = [word for word in cleaned_text if not word.startswith('/') and not word.startswith('//')]\n",
                        "    #Delete latex commands\n",
                        "    cleaned_text = [word for word in cleaned_text if not word.startswith('\\\\') and not 'rangle' in word and not word.startswith('|')]                                                                                                                      \n",
                        "    return cleaned_text\n",
                        "\n",
                        "\n",
                        "def create_corpus(path, regex):\n",
                        "    corpus = PlaintextCorpusReader(path, regex, word_tokenizer=custom_tokenizer)\n",
                        "    corpus = Text(corpus.words())\n",
                        "    return corpus\n",
                        "\n",
                        "def get_vocabulary_counts(corpus):\n",
                        "    vocab = {}\n",
                        "        \n",
                        "    #Count of words for this corpus\n",
                        "    for word in corpus:\n",
                        "        vocab[word] = vocab.get(word,0) + 1\n",
                        "                \n",
                        "    return vocab\n",
                        "\n",
                        "def merge_vocabularies(vocabs):\n",
                        "    total_vocab = set()\n",
                        "    for subj, vocab in vocabs.items():\n",
                        "        for word, count in vocab.items():\n",
                        "            total_vocab.add(word)\n",
                        "    \n",
                        "    for subj, vocab in vocabs.items():\n",
                        "        for word in total_vocab:\n",
                        "            if word not in vocab.keys():\n",
                        "                vocab[word] = 0\n",
                        "        \n",
                        "    "
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "lemmatizer = WordNetLemmatizer()\n",
                        "subj_len = {}\n",
                        "subj_counts = {}\n",
                        "subj_counts_list = {}\n",
                        "subj_cleaned_corpus = {}\n",
                        "\n",
                        "clean=False\n",
                        "\n",
                        "if clean:\n",
                        "    #Reader is a list of tokens\n",
                        "    for subj in tqdm(subj_color_mapping.keys()):\n",
                        "        if subj != 'All':\n",
                        "            path = './'+str(year_folder)+'/links_txts_not_network/'+subj+'/'\n",
                        "            corpus = create_corpus(path, '.*\\.txt')\n",
                        "            subj_len[subj] = len(corpus)\n",
                        "            corpus = clean_text(corpus)\n",
                        "            subj_cleaned_corpus[subj] = ' '.join(corpus)\n",
                        "            \n",
                        "            with open('./'+str(year_folder)+'/corpora/'+subj+'_corpus.json','w') as fi:\n",
                        "                json.dump(subj_cleaned_corpus[subj],fi) \n",
                        "else:\n",
                        "    for subj in subj_color_mapping.keys():\n",
                        "        if subj != 'All':\n",
                        "            f=open('./'+str(year_folder)+'/corpora/'+subj+'_corpus.json','r')\n",
                        "            data=f.read()\n",
                        "            subj_cleaned_corpus[subj]=json.loads(data)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "for subj in subj_color_mapping.keys():\n",
                        "    if subj != 'All':\n",
                        "        f=open('./'+str(year_folder)+'/corpora/'+subj+'_corpus.json','r')\n",
                        "        data=f.read()\n",
                        "        subj_cleaned_corpus[subj]=json.loads(data)\n",
                        "        \n",
                        "        to_save = subj_cleaned_corpus[subj].split()\n",
                        "        to_save = [word for word in to_save if word != 'non' and word != 'based']\n",
                        "        with open('./'+str(year_folder)+'/corpora/'+subj+'_corpus.json','w') as fi:\n",
                        "            json.dump(' '.join(to_save),fi) "
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### TR-TF Score"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "It's now time to calculate vocabulary counts, term frequencies, weighted frequencies, and term-weighted frequencies for each subject (and word) in the corpus.\n",
                        "\n",
                        "For calculate the TRTF we adapt the formula we saw in lectures to this :\n",
                        " \n",
                        "For some token $t$ with term frequency $\\textrm{TF}^{(u)}_t$, for subject $u$, we define the weight as:\n",
                        "\n",
                        "$$\n",
                        "w_{t}^{(u)} = \\frac{\\textrm{TF}^{(u)}_t}{\\textrm{Mean}_{\\substack{v \\\\ v \\neq u}} \\textrm{TF}^{(v)}_t + c}\n",
                        "$$\n",
                        "\n",
                        "where:\n",
                        "- $w_{t}^{(u)}$ is the weight of token $t$ in subject $u$.\n",
                        "- $\\textrm{TF}^{(u)}_t$ is the term frequency of token $t$ in subject $u$.\n",
                        "- $\\textrm{Mean}_{\\substack{v \\\\ v \\neq u}} \\textrm{TF}^{(v)}_t$ is the mean term frequency of token $t$ across all subjects except $u$.\n",
                        "- $c$ is a constant.\n",
                        "\n"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "tf = {}\n",
                        "tokens = {}\n",
                        "\n",
                        "for subj in subj_color_mapping.keys():\n",
                        "    if subj != 'All':\n",
                        "        tf[subj] = Counter(subj_cleaned_corpus[subj].split())\n",
                        "        tokens[subj] = subj_cleaned_corpus[subj].split()\n",
                        "    \n",
                        "# Constant 'c' to avoid division by zero\n",
                        "c = 0.5\n",
                        "\n",
                        "# Initialize dictionaries to store TF-TR scores for each coast\n",
                        "tf_tr = {}\n",
                        "\n",
                        "\n",
                        "for subj in tokens:\n",
                        "    tf_tr[subj] = {}\n",
                        "\n",
                        "    for word, freq in tf[subj].items():\n",
                        "        other_subjs_freq_sum = 0\n",
                        "        for other_subj in tokens:\n",
                        "            if other_subj != subj:\n",
                        "                other_subjs_freq_sum += tf[other_subj][word]\n",
                        "        tf_tr[subj][word] = freq / (other_subjs_freq_sum / (len(tokens) - 1) + c)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's now have a look to the top words used for each subject."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Rank tokens based on TF-TR scores (top_n words)\n",
                        "top_n = 3\n",
                        "\n",
                        "for subj in tokens:\n",
                        "    print(f\"Top words for {subj}:\")\n",
                        "    for word in sorted(tf_tr[subj], key=tf_tr[subj].get, reverse=True)[:top_n]:\n",
                        "        print(f\"{word}, TF-TR Score = {tf_tr[subj][word]}\")\n",
                        "    print(\"\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "This actually makes sense. For example, we can envision utilizing the top words as distinctive features for various research domains within the collaboration network.\n",
                        "\n",
                        "In the context of health care, significant emphasis appears to be placed on terms such as \"walking,\" \"perceived,\" and \"community,\" suggesting a focus on aspects related to physical activity, perception, and community engagement.\n",
                        "\n",
                        "In mathematics and computing, the prevalence of terms like \"office,\" \"remote,\" and \"district\" indicates potential themes related to work environments, remote work arrangements, and geographical districts within the collaborative research landscape.\n",
                        "\n",
                        "Genetics, as depicted by terms like \"pws,\" \"hqct,\" and \"food,\" may revolve around topics such as specific genetic conditions, advanced imaging techniques, and nutritional aspects.\n",
                        "\n",
                        "Ecology seems to be centered around terms like \"cave,\" \"surface,\" and \"zone,\" suggesting a focus on subterranean ecosystems, surface environments, and distinct ecological zones.\n",
                        "\n",
                        "Within the diseases category, terms like \"hernia,\" \"lumbar,\" and \"patient\" hint at a concentration on medical conditions related to hernias, lumbar regions, and patient-centric research.\n",
                        "\n",
                        "Microbiology, as reflected in terms like \"symbiodiniaceae,\" \"culture,\" and \"bacteria,\" likely involves studies on specific microbial species, laboratory cultures, and broader bacterial research.\n",
                        "\n",
                        "Finally, in physics, the prominence of terms such as \"position,\" \"layer,\" and \"node\" implies a focus on mechanical physics involving positions, layered structures, and nodal points within collaborative investigations."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Wordclouds TR-TF"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Let's now combined the text for each subject to have a single string by subject."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Create the combined text\n",
                        "combined_text = {}\n",
                        "for subj in tokens:\n",
                        "    combined_text[subj] = \"\"\n",
                        "    for word, tf_tr_score in tf_tr[subj].items():\n",
                        "        repetitions = int(tf_tr_score)\n",
                        "        combined_text[subj] += (word + \" \") * repetitions"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We can now create the word clouds."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "wordclouds = {}\n",
                        "for subj in tokens:\n",
                        "    wordclouds[subj] = WordCloud(width=800, height=400, collocations=False).generate(combined_text[subj])"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "And print them."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "sequential_colormap = ['YlOrBr','bone','BuPu','Greens','OrRd','Purples','Blues','Greys']"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Calculate the number of rows and columns\n",
                        "num_rows = (len(tokens) + 1) // 2  # Ceiling division to ensure there are enough rows for all subplots\n",
                        "num_cols = 2\n",
                        "\n",
                        "# Create subplots with the calculated number of rows and columns\n",
                        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(30, 30))\n",
                        "\n",
                        "# Flatten the NumPy array of subplots\n",
                        "axes = axes.flatten()\n",
                        "\n",
                        "# Iterate over each corpus and create word clouds\n",
                        "for i, ax in enumerate(axes):\n",
                        "    ax.axis('off')\n",
                        "    if i < len(tokens):\n",
                        "        ax.set_title(f'{list(tokens.keys())[i]} word cloud', fontsize=50)\n",
                        "        \n",
                        "        cloud = wordclouds[list(tokens.keys())[i]]\n",
                        "        color = sequential_colormap[i]\n",
                        "        sequential_cmap = cm.get_cmap(color)\n",
                        "        cloud.recolor(colormap=sequential_cmap, random_state = 42)\n",
                        "        ax.imshow(cloud)\n",
                        "\n",
                        "\n",
                        "# Adjust layout to prevent clipping of titles\n",
                        "plt.tight_layout()\n",
                        "\n",
                        "# Show the plots\n",
                        "plt.show()\n"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "The vocabulary used in each word cloud appears highly distinct for every subject, reflecting specific terminology relevant to each research domain. After experimenting with various values for the parameter 'c,' we have found that setting 'c' to 2 provides the most representative and distinctive word clouds for our analysis."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### TF-IDF Score\n"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "We need to adhere to the library input format, thus we create a list of documents"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "tfidf_corpus = []\n",
                        "for subj, corpus in subj_cleaned_corpus.items():\n",
                        "    tfidf_corpus.append(corpus)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Now we apply a state of the art vectorizer that uses tfidf to generate the same wordclouds.\n",
                        "We apply a cutoff to the words that does not consider words with a high frequency or with a too low of a frequency."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "vectorizer = TfidfVectorizer(max_df = 0.99, min_df = 0.01)\n",
                        "X = vectorizer.fit_transform(tfidf_corpus)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "After vectorization, we have 138746 features (words), and to each one we have assigned a value for the field"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "feature_names = vectorizer.get_feature_names_out()\n",
                        "\n",
                        "dense = X.todense()\n",
                        "lst1 = dense.tolist()\n",
                        "df = pd.DataFrame(lst1, index=list(subj_color_mapping.keys())[:-1], columns=feature_names)\n",
                        "print(df.iloc[0].shape)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "Creating the colormap list to color the wordclouds"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Wordcloud 2"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Create subplots with the calculated number of rows and columns\n",
                        "fig, axes = plt.subplots(4, 2 , figsize=(30, 30))\n",
                        "\n",
                        "# Flatten the NumPy array of subplots\n",
                        "axes = axes.flatten()\n",
                        "\n",
                        "# Iterate over each corpus and create word clouds\n",
                        "for i, ax in enumerate(axes):\n",
                        "    ax.axis('off')\n",
                        "    if i < len(tfidf_corpus):\n",
                        "        ax.set_title(f'{list(subj_color_mapping.keys())[i]} word cloud', fontsize=50)\n",
                        "        cloud = WordCloud(background_color=\"black\", collocations=False).generate_from_frequencies(df.iloc[i])\n",
                        "        color = sequential_colormap[i]\n",
                        "        sequential_cmap = cm.get_cmap(color)\n",
                        "        cloud.recolor(colormap=sequential_cmap, random_state = 42)\n",
                        "        ax.imshow(cloud)\n",
                        "\n",
                        "# Adjust layout to prevent clipping of titles\n",
                        "plt.tight_layout()\n",
                        "\n",
                        "# Show the plots\n",
                        "plt.show()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "The wordclouds can help us identyifing the current focus of the research in each field, the one which is yielding more results. For example, we can see that in the mathematics and computing wordcloud there are words such as sentiment and lstm, suggesting that language models are indeed prominent in the field at the moment."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# Part 8: Dictionary based topic analysis\n",
                        "In the previous section we have trained and built a tfidf vectorizer, that embeds words assigning them a value based on relative importance to the field. This vectorizer was trained using papers not included in our network, so that now, we can apply the learned tfidf values to the words in papers related to the network articles.\n",
                        "\n",
                        "We have to handle the case where a word is found with no associated weight, we  will:\n",
                        "* Not consider such word for the summation\n",
                        "\n",
                        "Then, for each and every country we will:\n",
                        "* sort out their papers\n",
                        "* for every term we can find in the text:\n",
                        "    * Check if it's in the tf-idf representation for some field,\n",
                        "    * Add the tf-idf value to the one of the corresponding field for that country, \n",
                        "    * At the end, divide every field value for the number of total words (summation of found words per field) that were found in the tf-idf representation."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Getting the country dictionary"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "#Computing dict of {country:number of unis}\n",
                        "countries = [attr['country'] for node, attr in graphs['All'].nodes(data=True)]\n",
                        "countries_dict = Counter(countries)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Getting json file mapping txt file and countries who published that paper "
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "country_txt_association = {}\n",
                        "filepath = './'+str(year_folder)+'/links_authors_countries_used_for_network.json'\n",
                        "with open(filepath,'r') as fi:\n",
                        "    data=fi.read()\n",
                        "    country_txt_association=json.loads(data)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Get previously computed dictionary of field scores"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "reference_dict = df.to_dict()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Analyze topics"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def analyze_topics(token_list, reference_dict, subjects, default_dict):\n",
                        "    freqdist = FreqDist(token_list)\n",
                        "    paper_relevant_words = 0\n",
                        "    topic_score = default_dict\n",
                        "    for token in freqdist:\n",
                        "        if token in reference_dict.keys():\n",
                        "            #Get number of times word appears in paper\n",
                        "            freq = freqdist[token]\n",
                        "            \n",
                        "            #Get total number of relevant words in this paper\n",
                        "            paper_relevant_words += freq\n",
                        "            tmp_dict = reference_dict[token].copy()\n",
                        "            tmp_dict.update((x, y*freq) for x, y in tmp_dict.items())\n",
                        "            topic_score = Counter(tmp_dict)+Counter(topic_score)\n",
                        "            \n",
                        "    \n",
                        "    #Merge topic_score dict according to fields, disregarding tokens\n",
                        "    return topic_score, paper_relevant_words\n"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Core func"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "clean = False\n",
                        "txts_dir = './'+str(year_folder)+'/links_txts_network/'\n",
                        "cleaned_corpora_dir = './'+str(year_folder)+'/network_corpora/'\n",
                        "\n",
                        "countries_scores = {}\n",
                        "countries_words = {}\n",
                        "default_dict = Counter({'health-care':0, 'mathematics-and-computing':0, 'genetics':0, 'ecology':0, 'diseases':0, 'microbiology':0, 'physics':0})\n",
                        "       \n",
                        "            \n",
                        "#Loop over paper txts\n",
                        "for filename in tqdm(os.listdir(txts_dir)):\n",
                        "    f = os.path.join(txts_dir, filename)\n",
                        "    #checking if it is a file\n",
                        "    if os.path.isfile(f):\n",
                        "        if clean:\n",
                        "            #Open raw file\n",
                        "            with open(txts_dir+filename, \"r\") as inp:\n",
                        "                paper_txt = inp.read()\n",
                        "            #Clean it\n",
                        "            corpus = word_tokenize(paper_txt)\n",
                        "            corpus = clean_text(corpus)\n",
                        "            #Save it so we don't have to clean again\n",
                        "            with open(cleaned_corpora_dir+filename, \"w\") as output:\n",
                        "                output.write(' '.join(corpus))\n",
                        "        else:\n",
                        "            #Open already cleaned file\n",
                        "            with open(cleaned_corpora_dir+filename, 'r')as inp:\n",
                        "                corpus = inp.read()\n",
                        "                corpus = corpus.split()\n",
                        "         \n",
                        "        \n",
                        "        filename_wo_extension = filename[:-4]\n",
                        "        #Count the score of the paper for each field\n",
                        "        #Returns a dict with fields as keys\n",
                        "        counter = 0\n",
                        "        paper_topics, n_words = analyze_topics(corpus, reference_dict, list(subj_color_mapping.keys()), default_dict)\n",
                        "        if filename_wo_extension in country_txt_association.keys():\n",
                        "            for country in country_txt_association[filename_wo_extension]:\n",
                        "                countries_words[country] = countries_words.get(country,0) + n_words\n",
                        "                score_so_far = countries_scores.get(country, default_dict)\n",
                        "                countries_scores[country] = Counter(score_so_far) + Counter(paper_topics)\n",
                        "        else:\n",
                        "            counter+=1\n",
                        "print(counter)\n",
                        "        \n",
                        "\n",
                        "#Average the results\n",
                        "for country in countries_scores.keys():\n",
                        "    dividend = countries_words.get(country,1)\n",
                        "    #scores = countries_scores.get(country,default_dict)\n",
                        "    for item, count in countries_scores[country].items():\n",
                        "        countries_scores[country][item] /= dividend"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "#Maybe multiply by 100, normalize all values? idk\n",
                        "print(countries_scores)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# Part 9: DTU involvement\n",
                        "Dtu appears under the name: DTU Technical University of Denmark. Let's investigate:\n",
                        "* It's degree\n",
                        "* How central it is\n",
                        "* Which are the nodes in the same community"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "name = 'Technical University of Denmark'\n",
                        "for subj, graph in graphs.items():\n",
                        "    degree = dict(graph.degree()).get(name, 0)\n",
                        "    #degree = list(graph.degree['Nicolaus Copernicus University'])\n",
                        "    print(f\"In subj \\033[1m{subj}\\033[0m , {name} has {degree} degree\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "for subj, graph in graphs.items():\n",
                        "    if subj != 'All':\n",
                        "        centralities = nx.betweenness_centrality(graph, weight='weight', seed=42) \n",
                        "        centrality = centralities.get(name, -1)\n",
                        "        print(f\"In subj \\033[1m{subj}\\033[0m , {name} has {centrality} centrality\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "country =  graphs['All'].nodes[name]['country']\n",
                        "print(country)\n",
                        "for subj, graph in graphs.items():\n",
                        "    same_country_count = 0\n",
                        "    community_size = 0\n",
                        "    partition = community_louvain.best_partition(graph)\n",
                        "    community = partition.get(name, -1)\n",
                        "    print(f\"In subj \\033[1m{subj}\\033[0m , {name} belongs to community {community}\")\n",
                        "    print('Other universities in same community are:')\n",
                        "    for uni, comm in partition.items():\n",
                        "        if comm == community:\n",
                        "            print(f\"\\t \\t {uni}\")\n",
                        "            community_size+=1\n",
                        "            other_country =  graphs['All'].nodes[uni]['country']\n",
                        "            if other_country == country:\n",
                        "                same_country_count+=1\n",
                        "    print(f\"\\t\\t \\033[1m{same_country_count}/{community_size} from the same country\\033[0m\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": []
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "Python 3 (ipykernel)",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.10.13"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}