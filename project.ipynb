{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfkit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpdfkit\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pdfkit'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os, re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapper file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper class for extracting nodes and links from nature.com articles\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        # Initializing class variables\n",
    "        self.base_url = \"https://www.nature.com\"\n",
    "        self.universities = {}         # Dictionary to store universities and their occurrences\n",
    "        self.collabs = {}              # Dictionary to store collaborations between universities\n",
    "        self.mapping_univs = {}        # Mapping old university names to new names\n",
    "        self.links_saved = []          # List to save article links used after\n",
    "\n",
    "    # Method to load collaboration data from a JSON file\n",
    "    def load_collabs(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            self.collabs = json.load(file)\n",
    "\n",
    "    # Method to load university data from a JSON file\n",
    "    def load_universities(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            self.universities = json.load(file)\n",
    "\n",
    "    # Method to load saved links from a JSON file\n",
    "    def load_links(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            self.links = json.load(file)\n",
    "\n",
    "    # Method to save universities data to a JSON file\n",
    "    def save_universities_in_file(self):\n",
    "        with open('universities.json', 'w') as fp:\n",
    "            json.dump(self.universities, fp)\n",
    "\n",
    "    # Method to save collaborations data to a JSON file\n",
    "    def save_collabs_in_file(self):\n",
    "        with open('collabs.json', 'w') as fp:\n",
    "            json.dump(self.collabs, fp)\n",
    "\n",
    "    # Method to save saved links to a JSON file\n",
    "    def save_links_in_file(self):\n",
    "        with open('links.json', 'w') as fp:\n",
    "            json.dump(self.links_saved, fp)\n",
    "\n",
    "    # Method to retrieve HTML content of a given URL and parse it using BeautifulSoup\n",
    "    def retrieve_url(self, url: str) -> BeautifulSoup:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    # Method to create a mapping between old and new university names\n",
    "    def create_map(self):\n",
    "        for new_name in self.universities.keys():        \n",
    "            for old_name in self.universities[new_name]['init_names']:\n",
    "                if old_name not in self.mapping_univs:\n",
    "                    self.mapping_univs[old_name] = new_name\n",
    "\n",
    "    # Method to extract universities and collaboration data from nature.com articles\n",
    "    def extract_universities(self):\n",
    "        articles_counter = 1\n",
    "        page = 1\n",
    "\n",
    "        while True:\n",
    "            search_url = f'{self.base_url}/search?journal=srep&article_type=research&subject=mathematics-and-computing&date_range=last_year&order=relevance&page={page}'\n",
    "            soup = self.retrieve_url(search_url)\n",
    "\n",
    "            all_atags = soup.find_all('a', {'data-track-action': 'view article'})\n",
    "\n",
    "            if len(all_atags) == 0:\n",
    "                break\n",
    "\n",
    "            for atag in all_atags:\n",
    "                random.seed(articles_counter)\n",
    "\n",
    "                if(random.random() < 0.5):\n",
    "                    self.parse_page(self.base_url + atag['href'], univs=True, collabs=False)\n",
    "                    print(f\"[EXTRACTING UNIVS] {articles_counter} articles and {len(self.universities)} universities found\")\n",
    "                else:\n",
    "                    self.links_saved.append(atag['href'])\n",
    "                    print(f\"[EXTRACTING UNIVS] {articles_counter} articles found and {len(self.links_saved)} links saved\")\n",
    "\n",
    "                articles_counter += 1            \n",
    "            page += 1\n",
    "\n",
    "    # Method to extract collaboration data from nature.com articles\n",
    "    def extract_collabs(self):\n",
    "        articles_counter = 1\n",
    "        page = 1\n",
    "\n",
    "        while True:\n",
    "            search_url = f'{self.base_url}/search?journal=srep&article_type=research&subject=mathematics-and-computing&date_range=last_year&order=relevance&page={page}'\n",
    "            soup = self.retrieve_url(search_url)\n",
    "\n",
    "            all_atags = soup.find_all('a', {'data-track-action': 'view article'})\n",
    "\n",
    "            if len(all_atags) == 0:\n",
    "                break\n",
    "\n",
    "            for atag in all_atags:\n",
    "                random.seed(articles_counter)\n",
    "\n",
    "                if(random.random() < 0.5):\n",
    "                    self.parse_page(self.base_url + atag['href'], univs=False, collabs=True)\n",
    "                    print(f\"[EXTRACTING COLLABS] {articles_counter} articles done\")\n",
    "                articles_counter += 1  \n",
    "            page += 1\n",
    "\n",
    "    # Method to print statistics about extracted data\n",
    "    def print_stats(self):\n",
    "        print(f'Number of nodes: {len(self.universities)}')\n",
    "        links = 0\n",
    "        links_with_fields = 0\n",
    "\n",
    "        for u1 in self.collabs:\n",
    "            for u2 in self.collabs[u1]:\n",
    "                for u3 in self.collabs[u1][u2]:\n",
    "                    links_with_fields += 1\n",
    "                links += 1\n",
    "\n",
    "        print(f'Number of links (without fields): {links}')\n",
    "        print(f'Number of links (with fields): {links_with_fields}')\n",
    "\n",
    "    # Method to parse the content of an article page and update dictionaries\n",
    "    def parse_page(self, url: str, univs=True, collabs=True):\n",
    "        soup = self.retrieve_url(url)\n",
    "\n",
    "        try:\n",
    "            # Find universities and update dictionaries\n",
    "            authors_all = soup.findAll('p', class_='c-article-author-affiliation__address')\n",
    "            subjects_all = soup.findAll('meta', {'name': 'dc.subject'})\n",
    "        \n",
    "            if univs:\n",
    "                for s in authors_all:        \n",
    "                    university = s.text        \n",
    "                    if university not in self.universities:\n",
    "                        self.universities[university] = 1\n",
    "                    else:\n",
    "                        self.universities[university] += 1\n",
    "\n",
    "            if collabs:\n",
    "                for s in authors_all:\n",
    "                    try:\n",
    "                        university = self.mapping_univs[s.text]\n",
    "\n",
    "                        if university not in self.collabs:\n",
    "                            self.collabs[university] = {}\n",
    "\n",
    "                        alreadyAddedUniv = []\n",
    "                        for s2 in authors_all:\n",
    "                            try:\n",
    "                                other_university = self.mapping_univs[s2.text]\n",
    "                                if other_university not in alreadyAddedUniv:\n",
    "                                    for sub in subjects_all:\n",
    "                                        subject = sub.get('content')\n",
    "                                        if university != other_university:\n",
    "                                            if other_university not in self.collabs[university]:\n",
    "                                                self.collabs[university][other_university] = {}\n",
    "\n",
    "                                            if subject not in self.collabs[university][other_university]:\n",
    "                                                self.collabs[university][other_university][subject] = 0\n",
    "\n",
    "                                            self.collabs[university][other_university][subject] += 1\n",
    "                                    alreadyAddedUniv.append(other_university)\n",
    "                            except:\n",
    "                                continue\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        except Exception as error:\n",
    "            print(\"[ERROR] Parse page: \", error) \n",
    "\n",
    "    # Method to preprocess university data\n",
    "    def preprocess_universities(self):\n",
    "        universities = {}\n",
    "\n",
    "        for entry, count in self.universities.items():\n",
    "            # Split the entry by commas and iterate through the parts\n",
    "            parts = entry.split(',')\n",
    "\n",
    "            # Look for patterns that match university names\n",
    "            university = None\n",
    "            for part in reversed(parts):\n",
    "                if any(keyword in part for keyword in [\"Polytechnic\", \"NIT\", \"MIT\", \"Politecnico\", \"Escuela\", \"École\", \"Institut\", \"Universitat\", \"Università\", \"Universität\", \"Universidad\", \"University\", \"Institute\", \"Ecole\", \"Universiti\", \"Université\", \"College\", \"School\", \"MIT\"]):\n",
    "                    university = part.strip()\n",
    "                    break\n",
    "\n",
    "            # If a university name is found, store the entry in the dictionary\n",
    "            if university:\n",
    "                if university in universities:\n",
    "                    universities[university]['init_names'].append(entry)\n",
    "                    universities[university]['count'] += count\n",
    "                else:\n",
    "                    universities[university] = {\"init_names\": [entry], \"count\": count}\n",
    "            else:\n",
    "                print(f\"Not accepted this uni: {entry}\")\n",
    "            \n",
    "        model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "        university_names = list(universities.keys())\n",
    "\n",
    "        embeddings = model.encode(university_names, convert_to_tensor=True)\n",
    "\n",
    "        similarity_matrix = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "        threshold = 0.92\n",
    "        similar_pairs = []\n",
    "\n",
    "        for i in range(len(university_names)):\n",
    "            for j in range(i + 1, len(university_names)):\n",
    "                if similarity_matrix[i, j] > threshold:\n",
    "                    similar_pairs.append((university_names[i], university_names[j]))\n",
    "\n",
    "        similar_groups = []\n",
    "\n",
    "        for pair in similar_pairs:\n",
    "            found = False\n",
    "            for group in similar_groups:\n",
    "                if pair[0] in group or pair[1] in group:\n",
    "                    group.add(pair[0])\n",
    "                    group.add(pair[1])\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                similar_groups.append({pair[0], pair[1]})\n",
    "\n",
    "        similar_groups = [list(group) for group in similar_groups]\n",
    "\n",
    "        for gr in similar_groups:\n",
    "            first_uni = gr[0]\n",
    "\n",
    "            for uni in gr[1:]:\n",
    "                if first_uni in universities and uni in universities:\n",
    "                    if 'init_names' not in universities[first_uni]:\n",
    "                        universities[first_uni]['init_names'] = []\n",
    "                    if 'count' not in universities[first_uni]:\n",
    "                        universities[first_uni]['count'] = 0\n",
    "                    universities[first_uni]['init_names'].extend(universities[uni].get('init_names', []))\n",
    "                    universities[first_uni]['count'] = universities.get(first_uni, {}).get('count', 0) + universities.get(uni, {}).get('count', 0)\n",
    "                    universities.pop(uni)\n",
    "\n",
    "        universities = {key.strip(): value for key, value in universities.items()}\n",
    "\n",
    "        self.universities = universities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network stuff :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges(data, u1,u2):\n",
    "    edges = []\n",
    "    overall_weight = 0\n",
    "    for collab in data.collabs[u1][u2].items():\n",
    "        #print(collab)\n",
    "        attr_dict = {'subject':collab[0], 'weight':collab[1]}\n",
    "        edge = (u1, u2, attr_dict)\n",
    "        edges.append(edge)\n",
    "\n",
    "        overall_weight+=collab[1]\n",
    "    #Edge with topic 'All'\n",
    "\n",
    "    #Edge with topic 'All'\n",
    "    overall_edge = (u1,u2, {'subject':'All','weight':overall_weight})\n",
    "    edges.append(overall_edge)\n",
    "    return edges        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subjects_distribution(subjects, first_n):\n",
    "    distrib = {}\n",
    "    for subject, weight in subjects:\n",
    "        if subject != 'All':\n",
    "            distrib[subject] = distrib.get(subject,0) + weight\n",
    "    width = 0.8 # width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ordered_items = sorted(distrib.items(),key=lambda x: x[1], reverse = True)[:first_n]\n",
    "    xvalues = [subject for subject, value in ordered_items]\n",
    "    yvalues = [value for subject, value in ordered_items]\n",
    "\n",
    "    pprint(xvalues)\n",
    "\n",
    "\n",
    "\n",
    "    rects1 = ax.bar(xvalues, yvalues, width, color='b')\n",
    "    ax.set_title(\"Subject histogram\")\n",
    "    ax.set_xlabel(\"Subject\")\n",
    "    ax.set_xticks(range(len(xvalues)),xvalues, rotation = 90) # set the position of the x ticks\n",
    "    ax.set_ylabel(\"# of link with this topic\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def degree_distribution(G):\n",
    "    degree_sequence = sorted((d for n, d in G.degree()), reverse=True)\n",
    "    dmax = max(degree_sequence)\n",
    "\n",
    "    fig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n",
    "    # Create a gridspec for adding subplots of different sizes\n",
    "    axgrid = fig.add_gridspec(5, 4)\n",
    "\n",
    "    ax0 = fig.add_subplot(axgrid[0:3, :])\n",
    "    Gcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\n",
    "    pos = nx.spring_layout(Gcc, seed=10396953)\n",
    "    nx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\n",
    "    nx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\n",
    "    ax0.set_title(\"Connected components of G\")\n",
    "    ax0.set_axis_off()\n",
    "\n",
    "    ax1 = fig.add_subplot(axgrid[3:, :2])\n",
    "    ax1.plot(degree_sequence, \"b-\", marker=\"o\")\n",
    "    ax1.set_title(\"Degree Rank Plot\")\n",
    "    ax1.set_ylabel(\"Degree\")\n",
    "    ax1.set_xlabel(\"Rank\")\n",
    "\n",
    "    ax2 = fig.add_subplot(axgrid[3:, 2:])\n",
    "    ax2.bar(*np.unique(degree_sequence, return_counts=True))\n",
    "    ax2.set_title(\"Degree histogram\")\n",
    "    ax2.set_xlabel(\"Degree\")\n",
    "    ax2.set_ylabel(\"# of Nodes\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scrapper.Scraper()\n",
    "data.load_universities(\"universities.json\")\n",
    "data.load_collabs(\"collabs.json\")\n",
    "\n",
    "inserted_universities = {}\n",
    "edges = []\n",
    "nodes = []\n",
    "\n",
    "for u1 in data.collabs:\n",
    "    inserted_universities[u1] = True\n",
    "    nodes.append(u1)\n",
    "    for u2 in data.collabs[u1]:\n",
    "        if not inserted_universities.get(u2,False):\n",
    "            tmp_edges = create_edges(data,u1,u2)\n",
    "            edges.extend(tmp_edges)\n",
    "\n",
    "G = nx.MultiGraph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "###THERE ARE 110 nodes that don't connect with anything soo\n",
    "not_connected_nodes = set(nodes)-set(G.nodes())\n",
    "########################################################\n",
    "\n",
    "\n",
    "#Analyzing subjects\n",
    "subjects = [(attr['subject'], attr['weight']) for node1, node2, attr in G.edges(data=True)]\n",
    "first_n = 10\n",
    "subjects_distribution(subjects, first_n)\n",
    "subject_set = set([subj for subj, weight in subjects])\n",
    "\n",
    "degree_distribution(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
