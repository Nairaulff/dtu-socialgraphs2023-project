introduction drug design process identifying biologically active compound relies efficient generation novel drug-like yet synthetically accessible compound far 10^8\ substance ever synthesized whereas total number realistic drug-like molecule estimated range 10^ 10^ deep learning particularly deep generative model believed helpful generative chemistry computational drug discovery application involving sampling scoring novel chemical structure large hitherto unknown distribution possible drug-like molecule see example benchmark ref fully developed generative model implicitly estimate fundamental molecular property stability synthetic accessibility generated compound intermediate product feature depend ability network architecture approximate solution underlying quantum mechanical problem computationally hard molecule realistic size quantum computer naturally good solving complex quantum many-body problem thus may instrumental application involving quantum chemistry moreover quantum algorithm speed machine learning therefore one expect quantum-enhanced generative model including quantum gans may eventually developed ultimate generative chemistry algorithm figure scheme dvae learning joint probability distribution molecular structural feature latent variable-representations discrete continuous q_\phi z|x p_\theta x|\zeta encoder decoder distribution respectively whereas p_\theta prior distribution latent variable space encoded rbm provide example reconstruction target molecule diaveridine using gibbs-300 model saved epoch training 0,1 tanimoto similarity initial molecule reconstruction t=1.0\ corresponds perfect reconstruction output probability full size image exploring full potential quantum machine-learning algorithm requires development fault-tolerant hardware yet accessible meanwhile readily available noisy intermediate-scale quantum nisq device provide test-bed development testing quantum machine-learning algorithm practical problem modest size example quantum annealing processor could potentially enable efficient solving quadratic unconstrained binary optimization problem approximating sampling thermal distribution transverse ising system application attractive context machine learning tool solving optimization problem sampling gate-based architecture also interest machine learning particular context quantum gans subject intensive research including recent demonstration learning generation hand-written digit image quantum processor work prototyped discrete variational autoencoder dvae see ref whose latent generative process implemented form restricted boltzmann machine rbm small enough size fit readily available annealers trained network d-wave annealer generated novel chemical structure medicinal chemistry synthetic accessibility property range typical molecule chembl hence demonstrated hybrid architecture might allow practical machine-learning application generative chemistry drug design hardware matures rbm could turned quantum boltzmann machine qbm whole system might transformed quantum vae qvae sample richer non-classical distribution result proposed characterized generative model see fig form combination discrete variational autoencoder dvae model restricted boltzmann machine rbm latent space transformer model model learns good representation chemical structure chembl manually curated database biologically active molecule drug-like property following ref used common smile encoding organic molecule trained system encode subsequently decode molecular representation via optimizing evidence lower bound elbo dvae log-likelihood aligned aligned denotes expectation value kullbackâ€“leibler divergence prior distribution latent variable space encoded rbm ref see two layer rbm contain unit rbm size sampled readily available quantum annealers used spike-and-exponential transformation smoothing probability distribution discrete continuous variable employed standard reparameterization trick avoid calculating derivative random variable respective encoder decoder function approximated deep neural network transformer layer depending set adjustable parameter modified divergence term =0.1\ avoid posterior collapse figure learning curve dvae trained classical gibbs sampling red yellow sample d-wave annealer blue cyan training d-wave suspended reaching convergence due resource limitation also learning curve simpler model continuous latent variable shown magenta green full size image trained network epoch apparent convergence using gibbs sampling see red yellow line fig representing total loss validation train set respectively follows discus two checkpoint fully trained gibbs-300 reference purpose intermediate model gibbs-75 appearing end 75th epoch expect improvement quantum hardware particular coherence time qubits training dvae quantum annealing technique could comparable overcome existing technique vae probabilistic model particular mean discrete state latent variable decoded probability distribution smiles-encoded molecule top fig provide example encoding particular molecule diaveridine reconstruction gibbs-300 network see structure bottom case target molecule reconstructed exactly 46\ run see reconstruction probability tanimoto similarity target molecule next reported structure dvae generative model produce novel molecule property presumably match training set fig table compare distribution basic biochemical property molecule training set among molecule generated model discrete latent variable trained discussed work novel molecule mainly valid 55\ 69\ gibbs-75 molecule gibbs-300 molecule model respectively kept track molecular weight water-octanol partition coefficient logp synthetic accessibility score quantitative estimation drug-likeness qed score common physico-chemical property benchmarking molecular generative model aside biochemical drug-likeness property also measured novelty generated molecule generated molecule 0.36\ 0.22\ gibbs-75 gibbs-300 model respectively tanimoto similarity larger 0.9 molecule training set 10\ generated molecule similar molecule training set 0.7\ model extra training time improved validity generated molecule brought molecular property closer found training set see relevant gibbs-75 gibbs-300 column table proposed network architecture sufficiently compact fit d-wave hardware hence able train network using annealer instead gibbs sampling learning hybrid model d-wave progressed slower classical computer using gibbs sampling see blue solid cyan dashed line fig corresponding total loss model validation training set however stop training reaching convergence 75th epoch due limited performance available quantum hardware improvement expect ability prolong training eventually used d-wave generate molecular structure grammatically correct see fig corresponding column table expected distribution basic property generated molecule close obtained gibbs-75 model could improved training time available table parameter distribution physico-chemical property molecule produced generative model discussed work full size table figure distribution physico-chemical property molecule produced proposed generative model table full size image discussion outlook vaes powerful generative machine learning model capable learning sampling unknown distribution input data first step towards building hybrid quantum generative model prototyped dvae along line ref rbm latent space provided large dataset drug-like molecule system learn implicit rule governing stability synthetic accessibility small molecule produce useful representation molecular structure could used generate novel still drug-like molecule drug design application virtual screening proof concept built dvae involving transformer layer encoder decoder component along additional preprocessing layer allowed model operate character-level rather word-level parse smile textual representation input molecule using smile necessarily best option since string 100\ valid property smile essential approach representation molecule term character string hence believe dvaes built operate alternative character string representation molecule selfies trained compact dvae rbm consisting two layer unit small subset containing almost 200,000-random molecule chembl database manually curated biologically active molecule training set classical hardware system could trained gibbs sampling able show training converged used network generate molecule distribution basic property logp qed closely matching training set simultaneously average size molecule increased training network progressing relatively harder-to-synthesize compound among molecule generated network generative model output drug-like molecule may deployed already existing quantum annealing device d-wave advantage processor training architecture network quantum annealer proceeded slower per epoch classical computer probably due noise nevertheless distribution molecular property generated molecule sufficiently close training set among molecule generated classical counterpart gibbs-75 certain discrepancy distribution present result computed limited number training epoch due restriction public access quantum computer computational drug design application depend limited generation novel synthetically accessible molecule focus work author original paper already proposed training additional property prediction binding constant particular target top autoencoder loss although direct extension vae task may challenging require refinement form network could used problem involving actual drug design i.e. generating novel compound binding specific medically relevant target attempt demonstrate capability however doubt dvae eventually hybrid implementation qvae appropriately refitted adding extra loss rbm could turned quantum boltzmann machine qbm whole system might transformed quantum vae qvae sample potentially richer non-classical distribution using genuine qbms speed training system vs. size network demonstration ref quantum sampler non-vanishing transverse field outperformed dvae assessed metric achieved number training cycle epoch construction qvae controllable non-zero transverse field principle performed existing generation d-wave chip however would require additional hardware tuning applying combination extra trick reverse anneal schedule pause-and-quench etc demonstrated useful vae built trained generate drug-like molecule keeping size latent representation small hence practically attainable already existing quantum annealing device expect development engineering quantum computing device hybrid architecture similar qvae would surpass classical counterpart specifically network architecture proposed work may provide baseline refinement required running genuinely quantum generative model benefit may especially large problem potentially involving rule quantum chemistry learning efficient representation molecular structure application related generative chemistry drug design method proposed characterized classical quantum annealer model combination discrete variational autoencoder dvae restricted boltzmann machine rbm latent space transformer model original transformer model proposed word-level natural language processing task encoder-decoder architecture used original transformer layer developed additional preprocessing layer allowed process character-level smile description molecule trained proposed model subset chembl dataset optimizing evidence lower bound elbo dvae log-likelihood modified additional coefficient multiplies divergence term see sketch architecture model illustrated fig describe detail dataset network architecture training parameter training schedule classical quantum annealer model also describe simpler classical model continuous latent variable used experiment shown fig dataset used subset molecule chembl release database dataset consisted 192,000 structure encoded smile string maximum length symbol containing atom organic subset focus relevant biologically active compound removed salt residual finally converted smile canonical format help rdkit processed molecule randomly assigned train validation set containing 80\ 20\ sample 153,600 38,400 molecule respectively training dvae using gibbs-sampling molecular smile string tokenized regular expression ref produced unique token standard trainable embedding layer positional encoding ref used implementation utilized combination embedding positional encoding positional encoding multiplied additional correction factor aligned emb emb emb emb aligned emb embedding tensor positional encoding tensor emb dimensionality embedding factor required make proportion embedding tensor positional encoding closer original model dimension embeddings model hyperparameter set employed layer one-dimensional convolution highway layer additional preprocessing layer embedding layer encoder component convolution layer filter kernel size equal developed based ref used highway layer since layer shown improve quality character-level model preprocessed 160-dimensional tensor passed highway layer encoder consisting stack transformer encoder layer width feed-forward part layer equal number head multi-head attention used gelu activation function dropout rate 0.1 original transformer encoder layer produce output tensor variable length length tensor equal size input string order reduce dimensionality latent space layer model construct fixed-length tensor transformer encoder output tensor calculating fixed number vector concatenate one tensor first two vector vector index transformer layer output vector equal arithmetic mean vector along length tensor next consider subset consisting vector index remainder division aligned mod m=0 ... n-1 aligned compute arithmetic mean concatenate calculated vector fixed-length output tensor restricted boltzmann machine rbm implemented latent space presented paper probability distribution rbm aligned -e_ -e_ z_l h_l z_l z_m aligned h_l\ bias weight unit z_l\ weight associated connection unit z_l\ z_m\ effective temperature supposed equal 1.0 presented formula rbm proposed model consists two layer unit rbm size sampled using existing quantum annealing device worth noting unit rbm dvae latent variable connected rest model hence distinction hidden visible unit standalone rbm informal description internal working model latent space follows output encoder vector probability discrete latent variable z_i\ equal conditioned input model probability sampled obtain latent binary vector continuous variable sampled using spike-and-exponential smoothing probability distribution vector used input decoder module training parameter rbm adjusted order memorize statistic binary vector appear latent space calculation gradient parameter rbm consists two part so-called positive negative phase positive phase calculated using backpropagation algorithm application reparameterization trick used avoid calculating derivative random variable negative phase gradient estimated using sampling rbm distribution molecule reconstruction generation similar molecule given molecule preprocessed smile description given molecule passed input encoder whole model executed example molecule reconstruction generation similar molecule depicted fig generation entirely new molecule encoder used trained rbm sampled obtain latent binary vector vector used calculate latent vector continuous variable given input decoder table fig show result newly generated molecule rbm sampled performing step gibbs update using persistent contrastive divergence pcd decoder work two mode training inference generation inference mode decoder preprocessing layer main part data processing training inference mode decoder consists transformer decoder layer altogether used transformer decoder layer size model =160\ gelu activation dropout 0.1 width feed-forward part layer equal number head multi-head attention train model used rebalanced objective function divergence term multiplied additional coefficient 0.1\ avoid posterior collapse problem employed adam optimizer contrast original transformer model used different learning rate schedule trained model epoch using multistep learning rate schedule initial learning rate equal 6\times 10^ learning rate subsequently reduced factor 0.5 point corresponding 50\ 75\ 95\ length training process estimation logarithm partition function boltzmann distribution used annealed importance sampling algorithm evaluation model end epoch using intermediate distribution sample due resource constraint chance optimize hyperparameters many architectural variant model presented variant network worked considered first step toward real effective solution training dvae quantum annealer used exactly network architecture quantum annealer difference classical case rbm latent space sampled using d-wave advantage processor also quantum model trained epoch constant learning rate equal 6\times 10^ estimating logarithm partition function boltzmann distribution evaluation model used different version annealed importance sampling see ref parameter classical case training model continuous latent variable model continuous variable latent space similar architecture discrete one smaller size latent space contains 32+32\ normally distributed continuous random variable preprocessing convolution layer consists filter kernel size equal encoder/decoder consists transformer encoder/decoder layer width feed-forward part equal fixed length tensor calculated similar way discrete model model trained using initial learning rate learning rate schedule discrete case calculation molecular similarity fingerprint fingerprint molecule generated using default function rdkfingerprint rdkit algorithm produce topological fingerprint represented bit vector size bit tanimoto similarity known reasonable metric matching molecule sharing similar fragment defined two fingerprint aligned =\frac a+b-c aligned number common non-zero bit number non-zero bit present respectively tanimoto distance could defined =1-t definition follows completely similar molecule shared identical set fragment tanimoto similarity equal dissimilar molecule common fragment t=0\