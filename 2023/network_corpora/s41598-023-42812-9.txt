introduction human various way exhibiting emotion placed highest level civilization among creature expression take form speech facial gesture physiological mode however interaction relationship among individual best sustained communication human speech human speech carry huge para-linguistic content reveal state emotion direct indirect communication therefore speech emotion classification occupying key position advancing affective computing speech research domain besides unlike method recognizing emotion speech emotion said reveal intent speaker without pretence hence reason sporadically attracting researcher within last decade sec cultural racial background may significant impact ground truth remains emotion universal peculiarity associated speech emotion domain effort made professional generate standardized synthetic dataset emotional corpus useful conducting research emotion classification among corpus iemocap interactive diadic motion capture toronto english speech set ravdess rayson visual emotion speech set emovo etc performance concerning speech emotion classification yielding appreciable result even sometimes compared real world dataset datasets came different language english spanish german chinese speech emotion classification application customer support management self-driving car psycho-medicine e-learning etc importance human-computer interaction overemphasized gordon opined affective behaviour may serve precursor emergence mental health condition like depression cognitive decline may aid development therapeutic tool automatically identifying tracking progress disease classical technique classifying emotion past follows extraction primitive acoustic feature low-level detector lld raw speech feature pitch energy etc represents frame-level feature speech analysis generate another level feature utterance-level thereafter concatenation feature vector form fed machine learning algorithm also referred classifier context actual classification emotion support vector machine svm gaussian mixture model gmm hidden makov model hmm k-nearest neighbour knn popular classifier figure show classical structure emotion recognition figure conventional speech emotion classification system full size image though approach proven efficient capacity however bewildered salient challenge rendered unsuitable achieving state-of-the-art result sec focus study enhance improve performance speech emotion classification attention-based network feature selection technique best knowledge first-time feature selection fused attention layer high dimensional feature extracted deep convolutional neural network accurate emotion classification attention based dcnn+rnca+rf utilized dataset study standard speech emotion corpus capture seven class emotion express human main contribution study experiment efficiency attention mechanism regularized feature selection regularized neighbourhood component analysis technique speech emotion classification pretrained transfer learning network set based model feature selection neutralizes additional parameter weight added attention layer thereby minimize complexity propose attention-based dcnn+rnca+rf exploratory thorough experiment three different classifier model achieved 97.8 accuracy dataset remainder article arranged follows overview related work presented section review related work proposed technique method described sect method technique result discussion given sect. experimental result discussion sect. conclusion conclusion future recommendation study review related work classification emotion history traced psychological submission human emotion grouped six main class sadness happiness anger disgust surprise fear however affective computing based primitive division computer perceiving mood interpreting set sequence technical parameter captured audio decoding process therefore speech emotion classification requires efficient learning paralinguistic information mitigates misclassification emotion machine learning classifier first explored sec application convolutional neural network model shortcoming conventional classification approach paved way convolutional neural network cnns lstm network occasionally two combined form robust model widely employed sequence modelling associated domain feature selection-based cnn utilized farooq combating artificial design influence hampered accurate description speaker emotional condition hajarolasvadi demirel proposed 3d-cnn speech emotion classification based overlapping frame segregation mfcc feature extraction 10-fold cross-validation parameter used evaluation three publicly available speech corpus ryerson multimedia laboratory rml survey audio-visual expressed emotion savee enterface convolutional model achieved 81.05 accuracy six emotion class deep belief network dbn svm proposed zhu extracting acoustic feature mfcc zero-crossing rate employed emotion classification wang combined deep neural network extreme learning machine elm speech emotion classification encoding speech feature pitch formants segmentation audio feature vector however conventional cnn performs woefully high-dimensional speech feature extraction many shortcoming paved way introduction recurrent neural network rnn model great milestone improvement cnn speech emotion classification address failure cnn time-series data extraction rnn hidden layer structure update output value respect time constant basis kerkeni proposed rnn speech emotion classification analysis speech signal using teager-kaiser energy operator tkeo combine empirical mode decomposition emd extraction speech cepstral feature svm classifier utilized multi-classification emotion achieved 91.16 berlin spanish based dataset nevertheless rnn also suffers dependency long-term gradient descent problem study cnn rnn combined form hybrid crnn convolution recurrent neural network model enhance speech emotion classification rnn isolated limitation way proffering quick fix issue peculiar long-short-term-memory lstm proposed hochreiter schmidhuber combination convolutional neural network yielded notable improvement lstm variant rnn consisting feedback connection dependency learning sequence prediction cnn combined lstm sec resulted appreciable accuracy 82.4 emo-db speech corpus zhao puri proposed hybridized lstm cnn dnn approach speech emotion classification mfcc mel-spectrogram fed eight contiguous convolutional sequential neural network layer model ravdess dataset used accuracy emotion recognition reported besides technique expensive train huge convolutional layer adopted lstm key component called forget gate research proven high probability forgetting emotional feature focus recent one hampered efficiency within sec domain recent advancement deep learning coupled incessant search way improvement addressing age long challenge sec made bahdanau introduce attention network able sieve irrelevant information peculiar speech data concentrate emotional rich information attention mechanism successfully adapted object recognition discipline notable improvement model performance attention-based network adopted work qamhan accuracy achieved iemocap dataset attention model emulate human way focusing important feature recognition object three-dimensional attention-based crnn used chen choose discriminative feature speech emotion classification proposed model input layer accepted mel-spectrogram delta-deltas employed delta-deltas reduced intrusion unimportant element result subpar classification performance keeping vital emotional data finally mechanism attention could take salient aspect account adopted accuracy report 82.82 emo-db 64.74 iemocap speech dataset experiment outcome supported efficacy attention technique emotion classification zhao utilized attention-based model comprises bidirectional lstm fully connected network fcn learning spatio-temporal emotional feature machine learning classifier speech emotion classification vain author utilized attention-based model 1dimensional cnn sec softmax activation function used top layer model feature extraction cross-modal sec carried seo kim using visual attention convolutional neural network vacnn partitioning spectral feature dataset combining speech dataset text video requires special technique extracting feature efficient prediction emotion zhang author applied attention head mechanism multimodal speech emotion classification novel model achieved 75.6 iemocap dataset zhang applied deep convolutional neural network attention-based network emotion classification method pre-trained dcnn used based model extracting segment-level feature introduction bidirectional lstm higher-level emotional feature thereafter attention layer introduced top layer model utmost focus feature relevant emotion recognition model evaluation achieved uar 87.86 68.50 respectively emodb iemocap dataset however experiment reflect influence speech enhancement carried raw speech augmented speech corpus used speed adjustment varying time-step extraction spectral feature fed dcnn chen proposed self global attention mechanism determining impact attention model speech emotion classification state-of-the-art approach achieved accuracy 85.43 emo-db speech corpus model built using sequential network requires computing resource train paper two pre-trainned dcnn model used attention model regularized feature selection sec often many researcher focused efficiency attention mechanism weight calculator sequence representation zhao however proposed model revealed performance attention-based network increased co-join regularized feature selection sec nevertheless paper concludes opportunity future research use attention mechanism feature selection improve accuracy classification fig method technique general description model proposed given section classification problem speech emotion categorized rather dimensional representation defined follows input acoustic feature dimensional output equivalent emotion classification also function representing emotional feature found classification study proposed unique framework speech emotion classification using attention-based mechanism pretrained dcnn regularized feature selection rnca algorithm shown fig four main phase model speech emotion classification includes efficient pre-processing pre-emphasis raw speech speech corpus feature learning extraction feature selection emotion classification noted literature performance sec model rest heavily dataset pre-processing carried work extracted log mel-spectrogram three channel weight height input channel original speech database containing wav file three channel mel-spectrogram usually comprises number mel-filter bank term frequency dimension frame number number channel number channel used paper three different colour used indicate magnitude short-term fourier transform stft three-channel mel-spectrogram low hz-blue mid khz-yellow high khz-red frequency range audio signal typically represented channel offer intuitive visual form spectral content audio signal latter used paper mel-spectrogram widely used speech-related task reason far-fetched fact representation involves time frequency speech signals.signal pre-emphasis stage amplification speech signal high frequency performed pre-emphasis filter using represent speech audio signal pre-emphasis utilized mel-filter bank frame content window obtain standard frame segment length processed fragment however frame segment confirmed posse enough paralinguistic information rich enough emotion classification speech signal framing adopted ensures breaking speech signal segment fixed-length length human speech varies framing required maintain size voice hamming window function length shift applied frame computed represents size window illustrated fig figure structure mel-spectrogram extraction full size image aligned t-1 0.9 1.0 aligned aligned 0.5 0.5cos 2\pi s-1 s-1 aligned fft fast fourier transform applied produce three channel mel-spectrogram suited input model raw speech signal sample frequency rate khz mel-spectrogram represented total number filter bank denoted term dimension frequency denotes length segment number channel figure proposed model architecture full size image feature extraction research study two pre-trained dcnn model serve based model vgg16 vgg19 experimented pre-trained network attention mechanism establish one yield better classification performance accuracy feature selection leverage weight two network already trained imagenet therefore convolutional layer comprised based model frozen training input model reshaped original 64\times 224\times required input size base model vggnet.this achieved using built-in python library called opencv bilinear interpolation approach base model comprises five convolutional layer relu reactivation linear unit activation function extracting segment-level feature input mel-spectrogram drop-out layer utilized prevent overfitting output based model feature extraction also reshaped make suited attention layer extracting high-level emotional feature fed rnca eventual feature selection carried block diagram fig depicts structure dcnn phase model pooling layer adopted max-pooling layer performs function aggregating feature sample several convolution convolutional layer produce unified output next layer fully connected layer used base model figure convolutional layer block diagram full size image attention layer attention mechanism application computer vision contributed immensely task image recognition mimic human mode paying closer look relevant information may contribute opinion conclusion see hear speech emotion task role attention network overlooked carefully concentrate focus model frame segment much emotional content attention mechanism lower training time ensures concentration feature much emotional information increase model performance silent semi-silent frame eliminated attention layer tendency impairing distorting model accuracy word attention give insight behavioural performance deep learning model calculates weight feature representation previous layer indicate attention mechanism utilized work computed given ... output feature convolutional layer aligned exp x_i j=1 exp x_i aligned aligned j=1 x_i aligned alpha represents weight attention network output feature representation attention layer first weight attention calculated obtained softmax function training process got weighted sum deeper feature utterance level attention mechanism proven tremendous help generating distinctive feature sec attention layer responsible dynamically highlighting weighting various input feature component according applicability emotion recognition task power model successfully learn represent attention weight depends number neuron attention layer used neuron increasing model capacity capturing fine-grained feature importance minimizing complexity regularized neighbourhood component analysis rnca feature selection rnca feature selection mechanism specific class feature weighting approach carry operation learning feature weight maximizing leave-one-out loo accuracy classification sample data loo provides unbiased estimate deep learning model performance rnca work assessing vector weight corresponds feature vector x_i\ optimization classifier based nearest neighbour scheme mechanism controlling complexity preventing overfitting density estimation rnca adopts framework selecting certain reference sample called x_j\ sample x_i\ emotion feature sample randomly however probability selected feature x_j\ rest heavily distance d_w\ exists two sample distance computed aligned x_i x_j m=-1 aligned mth feature weight denoted w_m\ kernel function established relation condition smaller larger value likelihood kernel function computed respectfully aligned d_w x_i x_j j=1 j\not d_w x_i x_j aligned aligned exp -\frac aligned kernel width represented influence likelihood reference point selected x_j\ sample therefore likelihood correctly classifying x_i\ computed aligned p_i j=1 j\not aligned indicate one y_i\ y_j\ equal average loo accuracy classification sum p_i\ sample divided total number sample indicated equation termed objective function required maximization nevertheless objective function defined insulated overfitting call introduction parameter termed regularizer prevent overfitting modified objective function represents rnca defined aligned obj i=1 m=1 aligned rnca algorithm adopted work operates output attention layer model aid feature selection therefore essential evaluate generalization error properly fine-tune regularization parameter obtain minimized classification loss aligned err i=1 aligned predicted label represented k_i\ t_i\ denotes real label feature sample rnca feature selection technique diagrammatically shown fig figure rnca framework full size image emotion classification study three primitive classifier utilized carrying classification emotion classifier take simplified input output feature selection layer model feature extraction essence employing three different classifier ensure robustness entire model aid analysis result multi-layer perceptron mlp classifier first introduced feedforward network-based classifier set suitable output mapped set input datasets feedforward artificial neural network model mlp made several layer completely connected one except node input layer node layer represent neuron nonlinear activation function secondly also utilized support vector machine svm svm operates discriminative classifier well-defined dividing hyperplane fit supervised unsupervised machine-learning task instance given set selected feature data algorithm output optimal hyperplane classifies new sample two-dimensional space hyperplane line dividing plane two part wherein class lay either side besides svm effectively handle multiclass problem obtainable emotion classification one distinguishable function svm selects hyper-plane large margin reducing likelihood miss-classification low sensitivity outlier lastly random forest also employed third classifier meta-estimator employ averaging increase classification accuracy reduce overfitting fitting several decision tree classifier different emotional feature subsamples random forest posse inbuilt mechanism managing class imbalance given edge classifier experimental result discussion dataset study benchmarked experiment one publicly available datasets named toronto english speech set northwestern university auditory laboratory speech sample recorded spontaneous event two actress asked recite handful word voice recorded resulting complete collection speech utterance seven different emotion comprise happy angry fear disgust pleasant surprise sad neutral observed scene experimental configuration study experiment carried using 64-bit operating system intel core processor ram python 3.9 environment deep learning software additional third-party library including tensorflow numpy audio processing also utilized audio sample first needed pre-processed input layer model meet requirement model voice signal scaled transformed log-mel spectrogram fft technique used separate mel-spectrogram feature original audio data dataset sectionalized training set testing set :20 exam practice set data normalized pixel implementation parameter implementing model compilation network utilized adam optimizer learning rate set 5e-5 notation one-hot encoding technique used vectorizing label ensures data point binarized adopted sparse categorical cross entropy loss function actualize objective increasing accuracy initialize model set epoch batch size however result training epoch yielded optimum accuracy utilized custom-early stopping mechanism monitor checkpoint loss accuracy value prevent overfitting corresponding curve obtained well experimental result result experiment using attention-based network regularized feature selection three classifier presented section first experiment vgg16 pre-trained network utilized confusion matrix emotion classification shown fig observed attention network model achieved highest accuracy 97.8 recognition classifier compared classifier svm:97.4 mlp 97.6 figure emotional class angry disgust fear sad accuracy reach attention-based network svm mlp neural emotion class got highest accuracy recognition classifier best accuracy obtained surprise emotion fig respectively fig performance evaluation chart fig show evaluation metric specificity sensitivity f1-score unweighted average recall used establish robustness model two experiment captured chart second experiment pre-trained model used vgg19 attention layer added result generated shown fig disgust emotion carry highest classification accuracy three classifier surprise emotion least classification accuracy neutral emotion differs accuracy three classifier optimum accuracy svm classifier overall model classification accuracy obtained second experiment 97.5 low compared previous experiment vgg16 used convolutional layer however impact attention network extraction emotionally related feature combined regularized feature selection improved classification accuracy speech emotion figure attention-based vgg16+rnca+rf full size image figure attention-based vgg16+rnca+mlp full size image figure attention-based vgg16+rnca+svm full size image besides accuracy obtained confusion matrix model loss roc return characteristic curve fig testify performance model paper loss value curve relatively low indicating model prevented overfitting loss curve decrease time model improved also loss curve show smooth convergence indication model prediction accurate acceptable level low initial loss value respect convergence point shown confirmed reduction model complexity training time roc curve show seven category emotion indicated table area curve auc demonstrates performance average across potential emotion classification threshold diagonal dotted line threshold closeness curve top left-hand corner seven emotional class indicates high true positive rate tpr low false positive rate fpr least auc score recorded 0.98 evidence good performance model emotion classification work mel-spectrogram used extract input feature producing feature vector dimensionality mel frequency bin feature record important detail speech signal spectral composition temporal dynamic feature space high dimensional feature selection used lower dimensionality concentrate useful feature task feature selection algorithm assessed feature relevance based contributed performance emotion recognition taking account measure like mutual information feature importance score thereby increasing model efficiency lowering amount computing power required improving interpretability learned representation experiment finding showed feature selection significantly enhanced speech emotion recognition model performance resulting increase accuracy 3.7 underscoring significance feature selection improving model discriminative power emotion recognition task figure attention-based vgg19+rnca+rf full size image figure attention-based vgg19+rnca+mlp full size image figure attention-based vgg19+rnca+svm full size image figure performance chart metric classifier full size image figure model loss curve full size image figure roc curve full size image table return characteristic description full size table performance comparison additionally proposed model study compared work carried others benchmarked speech dataset indicated table also carried comparative analysis proposed model without attention layer rnca attention mechanism rnca feature selection shown table table comparison proposed method full size table table emotional level comparison significance attention feature selection full size table term accuracy reduction complexity prevention overfitting method surpasses method utilized speech emotion classification recognition conclusion study proposed sec system using attention-based network regularized feature selection first foremost extracted mel-spectrogram dataset used study carried extensive speech processing analysis feed input layer model appropriate feature enhanced feature extraction subsequent layer pre-trained dcnn base model adopted attention network extract local feature attention layer deal emotionally rich feature global feature ultimately reduces misclassification barest minimum core principle attention network estimate feature weight attempt increase efficiency model regularized feature selection introduced attention layer actualize optimum result feature selection aided attention mechanism focus salient feature thereafter three classifier fed selected emotional feature rnca classification emotion comparison result experiment attention-based dcnn+rnca+rf model speech emotion classification proposed experimental result attained optimum accuracy 97.8 dataset seven class emotion comprised anger sad happy fear neutral disgust surprise reflect human major emotion accurately classified besides contrasting proposed model study method recently put forward obviously model outperforms many speech emotion classification task moreover computational cost peculiar deep learning task prevented study simply based model attention network requires training total number trainable parameter reduced barest minimum 101,480 total parameter 14,017,704 number floating-point operation per second flop model size 98mb memory requirement reduced minimize complexity top layer vggnet frozen average time taken emotional utterance classified proposed model 0.12 however though result obtained study undoubtedly provided insight researcher application attention mechanism feature selection sec task recommend future work carried using sequential network pre-trained based network low-level feature introduction speech emotion dataset