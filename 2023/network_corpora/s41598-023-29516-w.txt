introduction partial differential equation pdes especially second-order pdes extensively used physic engineering finance field simple pdes solved analytically complex pdes rely numerical solution usually divided forward backward problem common numerical method include finite difference finite element lagrange multiplier conventional method extensively applied solve forward pdes various practical problem however deep neural network dnn provides another solution complex nonlinear pdes without domain discretization used numerical method thus suitable forward backward problem dnn exhibiting major advancement solving pdes attracted increasing attention various research area due universal approximation however large amount labeled data usually required training dnn-based model solve pdes data often unavailable many physical application overcome disadvantage researcher proposed novel dnn-based neural network called physics-informed neural network pinn help reduce needed training time physics-informed manner network physics-informed loss function constructed based pde residual generally residual design play important role pinn approximation residual dnn highly effective type neural network pinn encode underlying physic law differential operator governing pdes approximated automatic differentiation advantage pinns applied extensively solve complex pdes various application area recent year one hand increasing number study conducted examine approach building improved pinn model incorporating model method example karniadakis introduced systematic pinn model first time presented series pinn variant including bayesian pinn fractional pinn extended pinn parareal pinn non-newtonian pinn hp-variational pinn nonlocal pinn extended pinn model constructed approximate various forward pdes either linear nonlinear application area ranging engineering finance hand pinn model also extended backward problem advection-dispersion equation stochastic problem flow problem conservation law backward problem training data inputted dnns screen unknown parameter pdes constructing pinn loss function dnn data sampling another important factor solving second-order higher-order pdes latin hypercube sampling filter variance associated additive component transformation powerful sampling method data analysis nearly every field computational science engineering mathematics sample space divided series subspace randomly paired algorithm iterates determine optimal pairing basis specified criterion solve pdes using neural network simple random sampling used sample data set usually employed solve pdes regular domain however sampling method based domain decomposition irregular domain also developed solve pdes study developed improved method namely glhs cartesian grid sampling merged optimized deal data set periodic boundary condition commonly appears theory simulation polymer chain bulk condition aware polymer physic especially self-consistent field theory scft modified diffusion equation mde parabolic-type second-order pde key equation scft gaussian wormlike chain several classical numerical method developed solve mdes scft achieved great success reproducing microstructures property self-assembled polymer chain recently chen trained traditional dnn solve mde diblock copolymer system static schrodinger equation quantum system efficiency solver analyzed work developed pinn residual unit combine glhs solve forward backward mdes used polymer physic several important issue addressed current study following section describe pinn residual unit mixed sampling method solve forward backward problem mdes example examine pinn solver based mixed data sampling optimizing parameter pinn mixed data sampling also compare pinn numerical result traditional verify accuracy efficiency pinn research summary presented final section neural network data sampling pinn residual unit describe pinn residual unit shown fig solve complex problem network convergence caused gradient disappearance network degradation traditional neural network apply neural network based residual unit solve second-order pdes describe residual unit fig input layer neural network layer weight bias activation function network layer output tensor i-1 fed network aligned i-1 i-1 aligned i-1 weight parameter network layer bias parameter network layer represents network width current layer number neuron activation function nonlinear key factor universal approximation neural network general activation function selects nonlinear function sigmoid tanh choose tanh activation function i.e. aligned tanh -\textbf -\textbf aligned output layer ordinary network layer weight bias input tensor i-1 output expressed aligned i-1 i-1 aligned figure pinn residual unit composition residual unit pinn based residual unit forward solution partial derivative physical constraint backward solution partial derivative physical constraint full size image residual unit constructed combining illustrated fig first input tensor i-1 residual unit obtain aligned i-1 i+1 i-1 +\textbf +\textbf i+1 aligned second input tensor i-1 connected jump identity output obtained tanh activation function accumulation i-1 namely aligned i-1 i-1 i-1 aligned residual unit design enables transmit input data directly lower network layer higher layer process differs common stacking two neural network layer therefore residual unit facilitate optimization process neural network solve problem network degradation certain extent parameterized parabolic-type pdes mde expressed general form follows aligned u_t aligned denotes solution respect time space variable represents differential operator parameterized spatial definition domain u_t\ denotes partial derivative respect time accordance universal approximation neural network solution pde equivalently expressed dnn current study use pinn residual unit solve pde shown fig instead common neural network pinn initial variable inputted input layer sent several residual unit fig solution assessed physics-informed loss function obtain solution output layer illustrate calculation partial derivative physical constraint solution process fig chain derivative rule based automatic differentiation used pinns forward backward solution pdes learned optimizing loss function physical information forward solution process pde neural network shown fig physical constraint differential equation defined aligned u_t aligned representation network need learn judgment mode solution differential equation simultaneously satisfies physical constraint periodic boundary condition pbc initial condition define total loss function aligned j_0 aligned _0\ denotes intermediate variable includes parameter appearing _e\ _p\ _i\ learning task stop forward process j_0 reverse problem shown fig network need satisfy constraint created existing data network described detail solving mde section backward process gradient certain underlying output expanded aligned j_1 i-1 j_1 i-1 j_1 i-1 aligned equation show gradient bottom output network decomposed two term first term indicates wrong signal directly transmitted bottom without intermediate weight matrix transformation thus alleviating problem gradient dispersion certain extent gradient disappear even weight intermediate layer matrix small residual unit successfully applied image recognition provides efficient tool solve backward problem pdes current study mixed method basic problem solving pdes using neural network produce result satisfy physical condition differential equation data point defined domain fed neural network thus selecting appropriate data point training process dnns crucial current work adopt mixed sampling method glhs i.e. mixture cartesian pinn solver figure data sampling distribution glhs take total data dimension dimension domain 0,1 example blue dot represent cartesian grid data point crossed dot denote data data sampling dimension overall data distribution mixing full size image figure comparison three type sampling blue y-axis left figure represents number data equal square data collection area _1= 0,1 0,1 purple y-axis right represents number data data collection area _2= 0,0.1 0,0.1 box represents data distribution within range -75 solid line box median line data distribution line cap represents maximum minimum value data distribution red dotted line represents ideal number data distribution 20,000 respectively full size image glhs method assume number data point cartesian total number data point proportionality coefficient i.e. 0,1 data located domain allocate data point grid point -dimensional cartesian grid equally divided int grid one dimension int denotes integer part number condition mean total n_g\ int data point sent grid -dimensional space one data point corresponds one grid point remaining data point n_l\ sampled method n_l int n_l\ data point sent n_l\ equal subdomains subdomain corresponds one data point effectively describe glhs method show simple example fig =0.5\ n=20\ n=2\ simplicity use definition domain _1= example example given fig supplementary information blue dot represent cartesian grid data point crossed dot denote data point clearly data point exist cartesian grid sampling data point present domain divided subdomains one dimension data point subdomain randomly sampled shown fig expect extracted random data evenly distributed definition domain linear cumulative density function cdf =\omega used dimension total data point two dimension calculated cartesian product i.e. total data point cartesian grid meanwhile data point randomly paired two dimension via cumulative linear density function _1= shown fig finally cartesian grid data randomly mixed obtain glhs data final input data pinn present example illustrate advantage glhs method fig three type sampling namely glhs compared sampling type two type two-dimensional definition domain i.e. _1= _2= 0.1 0.1 used example total 2,000,000 20,000 data point randomly imported two definition domain examine uniformity data distribution _1\ _2\ divide two definition domain equal square data collection area labeled number data point data collection area region _1\ _2\ subscript denote -th collection area two dimension plot detailed data distribution two-dimensional definition domain fig supplementary information thus data collection area distributed definition domain glhs sort sequence basis value label middle 25–75 sequence square box dotted line shown fig a–c among three sampling type largest box glhs smallest one _1\ domain indicating glhs posse uniform data distribution definition domain.for _2\ domain glhs also uniform data distribution data point middle sequence value average number data point collection data _2\ domain result may due reason amount input data large enough _2\ domain recently simple point transformation used increase uniformity data distribution study observe glhs obvious advantage although similar sampling procedure uncertainty analysis solving forward backward mde polymer physic mde key equation self-consistent field theory numerically solved many method adopt mde example illustrate use pinn based glhs solving forward backward problem first present general form forward backward mde second discus efficiency glhs pbc loss function solving mde lastly discus forward backward problem mde solution using pinn based glhs problem setup example take simple form forward backward mde initial condition pbcs expressed aligned 2\pi aligned initial condition x,0 periodic boundary condition two dimension used period dimension given parameter used work problem becomes forward problem unknown parameter solving equation becomes backward problem mde linear second-order parabolic-type pde forward problem addressed dnn however backward problem mde still need understood use pinn based glhs solve forward backward problem mde optimization scheme glhs pinn neural network sampling method optimized used solve special pde study core issue glhs find optimizing mixture coefficient special pde meanwhile depth width neural network also important parameter pinn optimized given two type parameter independent adopt independent variable method optimize glhs pinn parameter figure comparison training time standard error two data sampling method pinn residual unit mixture 1-\alpha ratio =0\ denotes simple random sampling mixture i.e. glhs 1-\alpha ratio =0\ denotes simple sampling purple bar left represent standard error network training result abscissa 0.001–0.5 amplified log yellow bar right represent time spent network training full size image primary issue glhs find optimizing mixture coefficient special pde solved pinn describe glhs comparing mixture sampling method solving mde using pinn shown fig comparison adopt two type mixture sampling namely mixture random sampling grid sampling shown fig glhs fig respectively data used solving mde pbc l=1.0\ number residual unit width neural network layer pinn use total n=301 301\ data point mixture random grid sampling well glhs number data point cartesian explain efficiency sampling method employ sampling standard error follows aligned i=1 u_i-u_ n-1 aligned u_i\ pinn solving value numerical value solved crank-nicholson method used mde previous simulation calculation sum take discrete data point definition domain used crank-nicholson method total number discrete data point mixture random grid sampling case result indicate difficult optimize varies corresponding training time shown fig particular standard error minimum value training time longest glhs case reach minimum value minimum value simultaneously shown fig optimize glhs =0.5\ indicates minimum error training time achieved data number equal figure also show result =0.0\ fig =0.0\ fig =1.0\ fig using pinn find term three sampling method good choice data sampling comparing glhs =0.5\ result agree data distribution described fig note used pinn try use glhs instead enhance efficiency solving pdes pinn pbc important pde especially handling bulk polymer system boundary condition strengthened previous study designing structure neural network thus consider pbc optimization choosing proper network parameter illustrated pbc spatial period regarded pbc numerical method done setting calculation cell size satisfies however pbc neural network satisfy left condition x-l right condition x+l squeeze period condition identify optimized pbc among three pbcs construct following type loss function aligned i=1 t_i t_i aligned aligned i=1 x_i t_i x_i-l t_i aligned aligned i=1 x_i t_i x_i+l t_i aligned aligned +j_ aligned x_i t_i summation performed two domain equation commonly used numerical method listed comparison figure comparison training process four type pbc loss function red line red y-axis left represent change loss function purple line purple y-axis right represent standard error network result analytical solution full size image training process four pbc loss function shown fig standard error result pinn i.e. result crank-nicholson numerical method i.e. u_0 also listed right side type shown fig although loss function converges desired value standard error large achieve correct solution type shown fig loss function converge desired value standard error still able converge desired value however use squeeze period condition loss function standard error converge desired value training process furthermore calculate pearson correlation coefficient standard error _p\ loss function j_p\ generally defined aligned =\frac cov _x\sigma aligned denote standard error _p\ loss function j_p\ respectively _x\ denotes standard error cov represents covariance list four type pbc table data indicate first three type pbc strongly correlation _p\ j_p\ _p\ j_p\ almost correlation squeeze boundary condition result indicates squeeze period method feasible effectively improve training accuracy solving pdes pbc pinn table pearson correlation coefficient standard error _l\ loss function j_l\ full size table optimize neural network parameter namely depth width pinn number residual unit number neural unit layer amount trained data great influence output accuracy neural network train pinn multi-parameter space 3,4,5,6,7,8 10,15,20,25,30 2000,4000,6000,8000,10000,20000 indicated table several typical combination shown indeed n=180\ combination exist full parameter space detailed combination found fig supplementary information table standard error loss function corresponding different parameter depth width number data full size table figure comparison pinn result numerical result overall view two dimensional space pinn result diagram difference pinn result numerical result accuracy one-dimensional space given 0.00,0.25,0.50,0.75,1.00 accuracy one-dimensional space given 0.00,0.25,0.75 blue dotted line represents numerical result red line represents pinn result full size image use loss function standard error screen desired parameter optimal parameter corresponds minimum combination d=6\ w=20\ n=20000\ generally increase lead decrease indicating simple relationship however optimal output relies complex combination data table reveal best optimization combination parameter combination d=6\ w=20\ n=20000\ generally using number hidden layer training result large precision loss optimization combination show case forward backward solution first discus forward solution mde pinn use optimal pinn parameter d=6\ w=20\ n=20000\ glhs parameter =0.5\ solve forward backward problem mdes appearing take definition domain 0,1 0,1 evaluate accuracy pinn solution define relative error respect numerical solution u_0\ follows aligned =\vert x_i t_j -u_0 x_i t_j u_0 x_i t_j aligned subscript x_i\ t_j\ correspond definition domain respectively plot neural network result compare numerical result fig adopt overall view two-dimensional space shown fig result indicate pinn result high accuracy within 10^ distributed defintion domain examine accuracy one-dimensional space given shown fig respectively data confirm pinn result vary given furthermore illustrate data given table relative error also listed table relative error corresponding different time space position full size table inverse problem mde discovering unknown parameter difficult due complex physical constraint gradient disappearance unlike case sparse regression method employed determine pde time series measurement spatial domain study design interleaved training method discontinuous double-loss function _e\ _b\ loss function _e\ defined _b\ defined aligned _b=\frac i=1 x_i t_i -u_0 x_i t_i aligned sum definition domain u_0 x_i t_i numerical solution taken standard value method search unknown parameter loss function _e\ optimize network solution _b\ _e\ optimized screen parameter first training stage lock parameter optimize network solution _b\ obtain high-accuracy network solution specifically construct pinn four residual unit full-connection layer width neural unit solve inverse problem optimize network parameter unknown parameter via loss function j_0 first stage lock parameter optimize using loss function j_1 second stage loss function reach 10^ shown fig label blue brown dotted line =10\ j_0 =0\ respectively result indicate parameter =10.00111\ t=108s\ j_0 10^ shown fig point lock parameter loss function j_0 automatically awitched j_1 network parameter determines network solution optimized second stage observe loss function discontinuous point shown inserted part fig generally loss function exhibit sudden drop optimizer switch training process also observe discontinuous loss function small region =0.05\ loss function switch current study obtain parameter high accuracy absolute error parameter 0.0011 relative error _\lambda =0.011\ standard error _\lambda =0.0046\ work present high-efficiency interleaved training method search unknown parameter reverse problem mde reasonably extended pdes figure training process solving unknown parameter inverse problem mde optimization process unknown parameter optimization process loss function blue brown dotted line represent =10\ j=0\ respectively full size image comparison pinn figure comparison training time standard error pinn residual unit traditional width neural network data sampling number mixture coefficient glhs 0.5 case yellow green bar denote training time pinn traditional case respectively blue dot bar represent corresponding standard error full size image illustrate advantage pinn residual unit compare result pinn traditional standard error training time shown fig traditional constructed without residual unit similar previous work convenient comparison condition set case including data sampling glhs =0.5 input data number n=20000\ width network w=20\ since residual unit consists two neural network layer take two network layer traditional layer convenient comparison network layer traditional change depth pinn output training time standard error case result indicate increase number neural network layer advantage pinn residual unit become obvious particular traditional neural network encounter gradient explosion leading meaningless output loss parameter shown fig traditional hardly optimized bacause problem gradient disapperance problem result extremely short training time large standard error addition even shallow network small training time pinn residual unit still superiority traditional shown fig furthermore compared efficiency pinn traditional similar result obtained please see tabel supplementary information expect pinn residual untis based mixed data sampling applied work especially three-dimensional pdes future summary developed pinn based glhs solve forward backward mdes optimizing corresponding parameter solver provides high efficiency accuracy solving forward backward problem one-dimensional mdes effectively avoids problem gradient disappearance network degradation existing traditional feedforward neural network neural network properly designed residual unit pinn solve mde squeeze pbc considered data sampling considered mixture believe method also used dimensional confirm view future optimized parameter used pinn considering loss function pbc specifically depth width neural network optimized result indicated squeeze condition suitable mde also optimized glhs data sampling adjusting mixture coefficient result revealed parameter combination optimized 0.5 given mde high precision demonstrated hybrid solver deal forward backward problem special mde compared neural network solver result numerical solution found good agreement forward mde obtained high-accuracy pinn solution within 10^ analyzing error pinn solution numerical result inverse problem mde designed itm method screen unknown parameter unknown parameter locked relative error _\lambda =0.011\ standard error _\lambda =0.0046\ pinn residual unit based mixed data sampling glhs generalized case pdes