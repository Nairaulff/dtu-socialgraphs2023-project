introduction deep learning made great achievement research automatic driving biometric recognition face recognition computer-aided diagnosis image recognition research technology requires large-scale high-quality labeled data supervised training however specific application scenario difficult obtain large-scale high-quality labeled data cost privacy security ethic etc dependence data seriously hinders development image recognition based deep learning therefore make deep learning model effectively learn generalize small number sample narrow gap artificial intelligence human intelligence become urgent problem deep learning problem called few-shot learning fsl also called few-shot image recognition field image recognition specifically purpose few-shot learning learn model good generalization sample model accurately predict sample category learning one sample few-shot learning model driven -way -shot learning task number category number labeled sample category -way -shot learning task pair support set query set s=\ x_i y_i i=1 n\times consists n\times sample q=\ x_i y_i i=1 n\times consists n\times sample label sample support set known query set unknown target task predict label sample query set annotated sample support set hence train model accomplish target -way -shot task usually leverage class massive labeled sample help model gain class-independent capability dataset composed class called base class dataset correspondingly dataset composed target -way -shot task called new class dataset overlap category base class dataset new class dataset recent year continuous development deep learning especially great success convolutional neural network computer vision research few-shot learning shown blowout development according base class dataset utilized solve -way -shot task new class dataset few-shot learning method broadly divided two category data augmentation metric meta learning following review present related work according two category metric meta learning method metric meta learning massive -way -shot task similar form different class constructed based base class dataset constructed task label sample query set known used ground truth train model paradigm learning target task model learns meta knowledge independent specific category model tested target -way -shot task make prediction query set sample specifically entire -way -shot construction process first randomly sample category data dataset randomly select sample category support set finally randomly select sample remaining sample category query set training massive -way -shot episodic task base class dataset allows model learn generalized capability meta-knowledge measurement model tested new class dataset without additional training step method divided two sub-categories based ability model going learn metric learning meta learning core idea metric learning learn pairwise similarity feature embeddings support set query set prototypical network used learn metric space classification performed utilizing distance prototype representation class query feature prototype rectification network prn performs prototype rectification perspective intra-class inter-class respectively prototype truly represent center support set sample prototypical network covariance metric network covamnet obtains covariance statistic category feature extraction module calculates distribution consistency query set support set complete classification dn4 asymmetric distribution measure adm compare local descriptor query set support set find category closest complete classification query set feature map reconstruction network frn estimate support set category prototype least square method euclidean distance query set sample support set category prototype classify core idea meta learning enable model master learning ability learn meta-knowledge refers additional knowledge learned meta training process hyperparameters model latent embedding optimization leo introduces low-dimensional latent space update parameter inner loop synthetic information bottleneck sib transductive method formulates variational posterior function support query set also single meta-learner optimizes learner running several synthetic gradient step query set ensemble epoch-wise empirical bayes model ^3\ generic method learns combine epoch-wise base-learners generate task-specific learning rate combination weight encourage robust adaptation data augmentation method data augmentation -way -shot task core idea train classification model directly using labeled support set unlabeled query set assign correct label query set base class dataset usually used pre-training non-specific recognition task make feature extracting model learn effective image representation backbone pre-trained model used feature extractor train simple classifier new class dataset annotated sample addition benefiting massive amount annotated sample base class dataset also provide effective base point class prototype new class dataset.in method data augmentation unlabeled query set new class dataset treated input -way -shot episodic task information contained unlabeled sample used data augmentation since ultimate goal produce label prediction query set using query set information without knowing label doe cause data leakage mangla proposed pre-training method s2m2 base class dataset backbone pre-trained model used feature extractor train simple classifier new class dataset annotated sample effectiveness s2m2 aroused interest researcher improve performance classifier effective feature extractor statistically significant class prototype base class annotated sample new class background many data augmentation method proposed solve -way -shot problem new class dataset core idea data augmentation increase training sample sample train robust classifier predict sample episodic task augmented sample enhance data diversity alleviate overfitting classifier model caused insufficient data attributed-guided augmentation aga learns mapping allows synthesis data attribute synthesized sample desired value strength trinet map sample feature semantic space encoder guide decoder synthesize new feature based semantic relation category semantic space attribute-based synthetic network abs-net construct repository attribute feature attribute learning process auxiliary dataset given attribute description one class probability based sampling strategy exploited select attribute feature repository synthesize combined feature diversity transfer network dtn learns transfer latent diversity known category composite support feature generate diverse sample new category feature space shrinking hallucinating generates data transferable intra-class transformation two sample belonging category delta-encoder extract transferable intra-class transformation auxiliary datasets applies new class synthesize new sample wang combine generative model classification model realize end-to-end meta-learning training optimization make generated image suitable classification task zhang utilize object detection algorithm separate foreground background image randomly combine foreground background different image generate image distribution calibration distribution statistic mean covariance base class data calibrate statistic new class extend feature new class sampling propose obtain distribution statistic new class maximum posteriori map base class extend feature new class sampling capture query distribution cqd support set sample capture distribution statistic query set sample generate sample feature according statistical information obviously two category few-shot learning method quite different main difference use base class dataset whether train new class dataset specific -way -shot episodic task despite huge difference method two category achieved satisfactory competitive performance report focus data augmentation method few-shot learning following research specifically bias base new class data well difference distribution different sample belonging class may bring limitation augmented feature generated current data augmentation method therefore new feature augmentation method proposed report basis information fusion rectification ifr few-shot image classification make full use relationship datasets specifically first model pre-trained base class dataset obtain general weight feature distribution support set rectified utilizing query set base class prototype obtained pre-trained model feature support set expanded rectified distribution finally simple classifier logistic regression support vector machine multi-layer perceptron trained utilizing expanded original support feature compared data augmentation method experimental result three few-shot datasets show accuracy proposed ifr method improved 1.84–4.66 5-way 1-shot task 0.99–1.43 5-way 5-shot task remainder report organized follows feature augmentation method based ifr few-shot learning framework ifr based feature augmentation introduced sect method experimental result analysis presented sect result discussion conclusion future work presented sect conclusion method core idea feature augmentation based information fusion rectification take relationship support dataset base dataset query dataset account using cosine similarity find relevant base class prototype query feature information fusion rectification module proposed efficiently fuse two kind information rectify distribution support dataset cosine similarity value taken corresponding weight make full use effective information base class data query set feature generated ifr closely related current few-shot task also make full use information base class dataset feature augmentation based information fusion rectification feature augmentation based ifr proposed study data augmentation method few-shot learning therefore problem set -way -shot problem given episodic task support set s=\ x_i y_i i=1 n\times consists n\times sample known class label query set q=\ x_i y_i i=1 n\times consists n\times sample unknown class label ultimate goal predict class sample learning however sample support set example annotated sample common setting 5-way 5-shot overfitting occur classifier trained directly support set classifier able effectively predict label query set sample ifr based feature augmentation method proposed increase diversity training data method illustrated fig figure feature augmentation based information fusion rectification full size image beginning sample base class dataset input feature extractor obtain corresponding deep feature prototype base class proto base obtained taking average feature vector sample aligned proto _i=\frac j=1 n_i f_\theta _i^j n_i proto base aligned proto _i\ prototype vector class base class n_i\ number sample class f_\theta _i^j ^d\ dimension feature vector sample class generation proto base need executed long base class dataset doe change ifr based feature augmentation method contains three important module tukey ladder power transformation module nearest neighbor prototype matching module information fusion rectification module tukey ladder power transformation proved effective reducing deviation distribution map cqd hence tukey ladder power transformation adopted ifr based feature augmentation method nearest neighbor prototype matching module similar idea used cqd however relationship base class prototype prototype support dataset considered relationship query feature prototype support dataset considered cqd differently method take relationship account using cosine similarity find closest base class prototype query feature information fusion rectification module proposed efficiently fuse two kind information rectify distribution support data cosine similarity value taken corresponding weight make full use effective information base class data query set expanded support feature effectively alleviate overfitting improve accuracy few-shot image classification tukey ladder power transformation module -way -shot episodic task new class dataset first sample support set query set input feature extractor obtain corresponding image feature f_\theta f_\theta tukey ladder power transformation adopted reduce deviation distribution transformation make distribution deep feature close gaussian distribution convenient subsequent rectification specifically input deep feature vector function tukey ladder power transformation expressed aligned array ^\lambda 0\\ log array aligned feature vector transformation hyperparameter adjust mapping distribution transformed deep feature support set class sample known prototype support set proto support obtained taking average feature vector sample aligned proto _p=\frac j=1 f_\theta _p^j proto support aligned number sample support class proto _p\ prototype vector class support set f_\theta _p^j ^d\ dimension transformed feature vector sample class support set transformed deep feature query set f_\theta directly used following process due unknowability class label nearest neighbor prototype matching module next nearest neighbor prototype matching used find close prototype vector base class close transformed deep feature query set specifically nearest neighbor prototype matching performed twice class prototype proto support proto _p\in proto support first nearest neighbor prototype matching top k_b\ prototype closest proto _p\ selected proto base second nearest neighbor prototype matching top k_q\ feature vector closest proto _p\ selected f_\theta cosine similarity considered distance measurement study distance proto _p\ prototype proto base calculated aligned disb=\left\ proto ^t\textbf proto proto _p||\cdot proto _i|| proto _i\in proto base aligned distance proto _p\ transformed deep feature f_\theta calculated aligned disq=\left\ proto ^tt f_\theta x_i proto _p||\cdot f_\theta x_i f_\theta x_i f_\theta aligned finally basetopk querytopk constructed aligned basetopk=\ proto _i\times disb_i i\in topk disb aligned aligned querytopk=\ f_\theta x_i disq_i i\in topk disq aligned basetopk top k_b\ base class prototype vector multiplied distance based disb querytopk top k_q\ query feature vector multiplied distance based disq information fusion rectification module information fusion rectification module ultimate goal construct suitable set gaussian distribution generate augmented feature class -way -shot task base class prototype vector set basetopk query feature vector set querytopk transformed feature vector sample class support set f_\theta vector concatenated new feature vector set dimension feature vector basetopk set feature vector dimension k_b querytopk set feature vector dimension k_q f_\theta set feature vector dimension hence dimension concatenated set feature vector k_b+k_q+k standard deviation target distribution constructed calculated aligned =\sqrt k_b+k_q+k i=1 k_b+k_q+k -\frac k_b+k_q+k j=1 k_b+k_q+k _j\right aligned ^d\ _i\ element d-dimension vector gaussian distribution support class constructed standard deviation query feature vector set querytopk aligned querytopk disq_i i=1 k_q aligned fusing information base class basetopk query sample querytopk distribution support sample f_\theta rectified set gaussian distribution class support set k_q\ rectified distribution finally sample distribution sampled expand feature corresponding support class overall process ifr based feature augmentation method shown algorithm few-shot learning framework ifr based feature augmentation framework few-shot learning ifr based feature augmentation shown fig framework few-shot learning ifr based feature augmentation includes two main stage pre-training stage few-shot learning stage figure framework few-shot learning ifr based feature augmentation 5-way 1-shot full size image pre-training stage shown fig pre-training stage feature extractor f_\theta trained prediction task base class dataset specifically wideresnet selected feature extractor study like method backbone network pre-trained classification model retained feature extractor addition common classification loss auxiliary loss self-supervised loss manifold mixup loss also used provide enough decision boundary among class make model generalize new class auxiliary loss shown better pre-training effectiveness study s2m2 self-supervised training image rotation prediction used unsupervised pre-training task study rotated image used input model model learned predict much degree input image rotated purpose rotation prediction make model understand position type attitude image object make backbone network model extract characteristic feature four rotation angle set first process image input image rotated degree c_r=\ 0,90,180,270\ rotated image input model extract deep feature finally rotated angle predicted model manifold mixup image augmentation method deep feature different image linearly combined expand training sample improve generalization ability model given two image sample label x_i y_i x_j y_j manifold mixup image label defined aligned aligned f_\theta x_i 1-\gamma f_\theta x_j y_i 1-\gamma y_j aligned aligned mixup feature mixup label f_\theta x_i f_\theta x_j output feature model f_\theta layer input image x_i\ x_j\ hyperparameter control degree mixup model trained manifold mixup capture advanced information different layer improve generalization ability model overall three loss pre-training stage including classification loss rotation self-supervised loss manifold mixup loss x^r denotes cross-entropy loss model batch rotated sample x^r\ corresponding label x^r denotes cross-entropy loss model batch rotated sample x^r\ corresponding rotated degree label denotes cross-entropy loss model batch mixup sample corresponding mixup label whole pre-training process divided two step first model trained using rotation self-supervised loss classification loss model trained fine-tuning three loss process pre-training stage shown algorithm few-shot learning stage few-shot learning stage -way -shot episodic task constructed new class dataset support set query set first input feature extractor obtain corresponding image feature f_\theta f_\theta ifr module proposed report used obtain augmented support feature aug finally simple machine learning classification model logistic regression support vector machine multi-layer perceptron trained augmented support feature query set predicted directly model training process few-shot learning stage shown algorithm result discussion experiment designed based following two aspect first comparative experiment conducted evaluate proposed method benchmark method three datasets second different variant component feature augmentation method different hyperparameters proposed method explored ablation experiment prove proposed method achieve best performance datasets order evaluate performance proposed ifr based feature augmentation three popular datasets miniimagenet tieredimagenet cub used experiment miniimagenet dataset subset imagenet contains class sample per class size image fixed resized 84\times 84\ report dataset split base class validation class new class tieredimagenet dataset also subset imagenet distinguishes class broader category corresponding higher-level node imagenet training class sufficiently distinct testing class training validation testing datasets semantic relationship viewed cross-hierarchical dataset contains 779,165 image class size image fixed resized 84\times 84\ report dataset split base class validation class new class cub dataset bird dataset often used benchmark fine-grained visual categorization task contains 11,788 image class size image fixed resized 32\times 32\ report dataset split base class validation class new class experimental setting detail pre-training stage wideresnet used pre-train setting s2m2 except activation function wideresnet set relu ensure input tukey ladder power transformation valid few-shot learning stage standard 5-way 1-shot 5-way 5-shot episodic task considered query image task particularly hyperparameters ifr based feature augmentation set =0.5 k_b=2 k_q=6\ m=1\ d=640\ logistic regression support vector machine svm multi-layer perceptron mlp adopted classification model few-shot learning stage svm training epoch set 1,000 rest parameter default mlp random state=123 training epoch=500 hidden layer=2 corresponding neuron respectively rest parameter default trained classification model scratch used adam optimizer 0.001 learning rate finally 10,000 task randomly generated new class top-1 average accuracy well confidence interval datasets reported comparative experiment comparative experiment result 5-way 1-shot 5-way 5-shot classification task compared related augmentation method state-of-the-art meta learning method including map cqd leo sib ^3\ s2m2 sake comparability feature extractor method set wideresnet comparison result shown table despite huge difference meta learning method leo sib ^3\ data augmentation method s2m2 map cqd proposed ifr achieved comparable performance seen ifr+lr achieves highest accuracy datasets specifically miniimagenet method achieved accuracy 73.40 85.29 5-way 1-shot 5-way 5-shot task respectively improves 2.00 ^3\ 0.99 map compared best accuracy comparison method tieredimagenet method achieved accuracy 80.69 89.77 5-way 1-shot 5-way 5-shot task respectively improves 4.66 map 1.18 s2m2 compared best accuracy comparison method cub leo sib ^3\ tested cub dataset original paper doe make comparison three method cub compared best accuracy comparison method cub method achieved accuracy 85.08 92.28 5-way 1-shot 5-way 5-shot task respectively improves 1.84 cqd 1.43 s2m2 brief compared relevant data augmentation method s2m2 map cqd experimental result three few-shot datasets show accuracy proposed ifr method improved 1.84–4.66 5-way 1-shot task 0.99–1.43 5-way 5-shot task method effectively alleviate domain shift base class new class also may reflect true distribution rectified support distribution evidenced large performance gain 5-way 1-shot task tieredimagenet 4.66 semantic correlation base class new class since tieredimagenet divided basis hierarchical information method performs well tieredimagenet prof better applied task cross-hierarchical classification table 5-way 1-shot 5-way 5-shot classification accuracy miniimanegnet tieredimagenet cub full size table ablation experiment ablation experiment different variant component feature augmentation method different hyperparameters proposed method explored ablation experiment prove proposed method achieve best performance figure experimental result different fusion information 5-way 1-shot task miniimagenet dataset x-axis coordinate represents number group fusion vector y-axis represents accuracy model shape point represents fusion combination number near shape point represents accuracy model blue dashed polylines connect model highest accuracy number group fusion vector full size image figure experimental result different module used ifr based feature augmentation method 5-way 1-shot task miniimagenet dataset x-axis coordinate represents three classification model y-axis represents accuracy model without represents nearest neighbor prototype matching module instead random select without tukey represents tukey ladder power transformation module used ifr based feature augmentation method without ifr represents query feature vector set querytopk used feature augmentation full size image exploration different fusion information proposed method information fused three set vector including base class prototype vector set basetopk query feature vector set querytopk transformed feature vector f_\theta support dataset ablation experiment conducted different combination vector set ifr+lr model taken example analyze setting miniimagenet 5-way 1-shot since ifr+lr model highest accuracy among model experimental result presented fig eight combination compared study none represents case none augmentation method conducted experimental result show information combination proposed study obtain maximum performance gain compared best accuracy single vector two group fusion vector accuracy model take three group fusion vector improves 1.6 1.49 respectively improves 8.47 model without augmentation method addition increase number group fusion vector blue dashed polylines positive slope trend dashed polylines indicate data augmentation fusion information appropriate fusion method improve performance compared fusion information fusion inappropriate method experimental result proved performance gain proposed method come information fusion rectification exploration different module proposed method three significant module tukey ladder power transformation module nearest neighbor prototype matching module information fusion rectification module order understand importance ablation experiment conducted setting miniimagenet 5-way 1-shot task experimental result presented fig four variation compared study without represents nearest neighbor prototype matching module instead random select without tukey represents tukey ladder power transformation module used ifr based feature augmentation method without ifr represents query feature vector set querytopk used feature augmentation represents training proposed ifr based feature augmentation method experimental result show nearest neighbor prototype matching module important module proposed method module ensures prototype feature vector used augmentation related task scenario effectively avoids generate unrelated feature task besides tukey ladder power transformation module also brings performance improvement using information fusion rectification module achieve best performance classifier small performance improvement mlp classifier however svm classifier using information fusion rectification module bring performance gain consistent result competitive experiment exploration different hyperparameters since three hyperparameters information fusion rectification module k_b\ k_q\ different parameter may different influence experimental result take ifr+lr example analyze influence hyperparameters model setting miniimagenet 5-way 1-shot since ifr+lr model highest accuracy among model figure effect hyperparameter k_b\ k_q\ miniimagenet 5-way 1-shot setting effect hyperparameter k_b\ effect hyperparameter k_q\ effect hyperparameter full size image effect k_b\ k_q=6\ m=1\ shown fig k_b\ ifr+lr achieves best accuracy increase k_b\ accuracy ifr+lr always remains 73.2 although fluctuation value bring much change classification result k_b\ little effect model useful experimental result different value k_q\ k_b=2\ m=1\ shown fig value k_q\ range study since query sample per class 5-way 1-shot setting k_q\ ifr+lr best classification accuracy k_q\ larger classification accuracy always remains 73.25 small fluctuation actually k_q\ small difficult estimate distribution accurately since query feature found k_q\ large noise may introduced since query feature support prototype might belong different class experimental result different value k_b=2\ k_q=6\ shown fig ifr+lr best classification accuracy accuracy gradually decrease increasing necessary generate new sample quantity much mean rectified distribution support class first k_q\ query feature closest support prototype found query feature support prototype may belong category feature generated rectified distribution may deviation number generated sample continues increase deviation generated sample may become larger larger resulting accuracy decreasing reason smaller selected m=0\ proposed model report turn s2m2 directly logistic regression train support set predict query set without data augmentation strategy conclusion study focus data augmentation method few-shot learning following previous study feature augmentation method proposed paper based information fusion rectification ifr few-shot image classification make full use relationship datasets relevant information base new class data well support dataset query dataset fully utilized proposed ifr module feature generated ifr closely related current few-shot task also make full use information base class dataset compared data augmentation method experimental result three few-shot datasets show accuracy proposed ifr method improved 1.84–4.66 5-way 1-shot task 0.99–1.43 5-way 5-shot task result ablation experiment proved effectiveness proposed method revealed influence hyperparameters proposed method future study carry research diverse way expressing information class prototype feature vector different fusion approach different way utilizing fused information improve performance few-shot image classification