introduction modern deep learning gravitates towards large neural network ever-increasing demand computational resource perform basic arithmetic operation multiplication used contemporary hardware known face limitation von neumann bottleneck result substantial energy consumption significant delay network inference time human brain capable inferring remarkable efficiency dismissing irrelevant signal connection consuming 10–20 basic cognitive task efficiency motivated study optimize inference taking account significant signal specifically consider inference task using popular atari game environment sandbox brain presence regular dense layer evident instead brain employ neural rewiring mean eliminate inefficient unnecessary connection time recent study demonstrate many case significant part neural connection excessive removed without drop neural network predictive power negligible one optimization technique known pruning represents method attain structural sparsity within neural network conceptualization approach traced back 1990s today multiple strategy identifying redundant connection within neural network including examination absolute value hessian matrix decomposition method others moreover neural network often anticipate input data form sequential highly correlated signal frame observed domain like video audio monitoring control problem including task information processed neural network time step closely resembles analyzed time step t-1\ phenomenon referred temporal sparsity also characteristic perceptual system brain study propose various optimization algorithm neural network handling temporal sparse data approach based idea asynchronous update state neuron changed significantly compared previous step updated worth noting brain neuron also operate asynchronously transmitting signal necessary typical neural network layer could represented matrix-vector multiplication combined application nonlinear transformation therefore input vector value output vector could represented aligned y^j w_1^j x_1 w_n^j x_n aligned nonlinear function applied multiply accumulate mac operation mac operation summation product input vector element respective weight least one operand multiplication zero multiplication could omitted work refer multiplication two number significant neither operand equal zero study propose combination two aforementioned approach optimize inference neural network task video input specifically optimize inference popular dqn algorithm atari game pruning approach yield 2–8 reduction number significant neural network multiplication second part approach achieves 10–50-fold reduction combined approach result 20–100-fold reduction number significant multiplication without notable performance loss case even enhancing original performance proposed combination method biologically inspired exhibit neuromorphic property fast information processing best knowledge approach never applied deep problem background deep q-network task agent receives current environment state input selects action new state s'\ receives certain reward agent goal maximize total reward strictly environment formalized markov decision process markov decision process mdp tuple set possible state set possible action function describing transition state t+1 s_t a_t i.e. probability get state s'\ next step selecting action state function describing receiving reward determines big reward agent receive transitioning state state s'\ selecting action strategy agent select action depending state called policy usually denoted letter _\theta denotes policy parameter one way solve task -function-based approach -function two parameter _\theta estimate cumulative reward agent following policy _\theta receive performs action state follows policy _\theta complete information environment available exact value -function calculated however knowledge world usually incomplete number possible state enormous therefore -function approximated using neural network approach named dqn deep q-network demonstrated deepmind present study use algorithm train neural network atari task pruning lottery ticket hypothesis author proposed lottery ticket hypothesis hypothesis state stage neural network training smallest weight absolute value pruned set zero frozen remaining weight reset value training neural network training resumed training capability neural network remain however doe happen set remaining weight random initial value furthermore investigation reveals sparse neural network undergone pruning training exhibit superior performance compared unpruned neural network practice neural network iterative magnitude pruning absolute value usually used approach involves training neural network followed pruning small percentage weight typically 10-20 remaining weight reset initial value process repeated specified number iteration denoted consequently given pruning rate fraction pruned weight computed follows aligned pruned\_weights total\_weights 1-v aligned pruning iteration figure display relationship fraction pruned weight various value shown phenomenon lottery ticket hypothesis observed task well particular author investigated phenomenon within context dqn algorithm figure dependency fraction pruned weight pruning iteration various pruning rate full size image deltanetwork algorithm output value neural network layer number k+1\ written aligned k+1 aligned aligned k+1 k+1 aligned k+1 r^n\ output k+1\ neural network layer x^k r^n\ input k+1\ neural network layer output layer nxn weight matrix bias conventional neural network every new input vector moment time total recomputation output value k+1 required require n^2\ multiplication however following noted aligned x^k x^k x^k t-1 aligned aligned k+1 k+1 t-1 aligned aligned k+1 k+1 k+1 t-1 aligned thus possible recompute layer output value moment time using using layer input change occur relative state moment t-1\ remark doe lead neural network optimization introduce threshold output value change recomputation succeeding neuron started output value exceeds threshold author call approach hysteresis quantizer implement necessary introduce additional variable x\_prev neuron variable used record last transmitted value thus following algorithm run neuron algorithm update neuron layer k+1 time full size image related work best knowledge present first attempt combine temporal structural sparsity optimizing inference task however several publication applied pruning optimization technique problem e.g. distillation also work employ deltanetwork algorithm convolutional recurrent neural network figure pipeline including training pruning stage full size image method previous section described two neural network optimization algorithm pruning deltanetwork algorithm following neural network optimization becomes possible approach applied jointly stage 1—training train neural network environment prune set zero freeze lowest 20\ weight reset remaining weight original train neural network environment repeat step 2–4 10\ time using algorithm obtain set structurally sparse neural network different degree sparsity number neural network set equal number pruning algorithm iteration general scheme learning stage algorithm shown fig stage 2—inference apply deltanetwork algorithm threshold 0.01\ inference trained neural network result get set new neural network using structure temporal sparsity selection one neural network set depends desirable balance number significant multiplication neural network performance scheme inference stage presented fig figure inference stage algorithm full size image experiment neural network architecture study conducted experiment using following neural network architecture matrix received environment input neural network consists sequential game frame first convolutional layer consists filter stride equal relu activation second layer consists filter stride equal relu activation third layer consists filter stride equal relu activation followed dense layer neuron relu activation output another dense layer number neuron equal number action video game depending video game number action may vary neural network structure shown fig figure optimized dqn architecture dqn consists three convolutional layer two dense layer architecture suitable video game number output last layer value change full size image environment experimented within following environment breakout spaceinvaders robotank enduro krull freeway figure show frame video game example breakout agent hit many brick possible hitting ball spacelnvaders agent eliminate alien spaceship figure environment experiment full size image significant operation counting number multiplication optimization let estimate number multiplication standard neural network without optimization following formula used estimate number multiplication convolutional layer aligned size\_x_ k+1 size\_y_ k+1 filters_ k+1 kernel\_size\_x_ kernel\_size\_y_ filters_ aligned size\_x_ k+1 —is k+1\ layer input size along axis size\_y_ k+1 —is k+1\ layer input size along axis filters_ k+1 —is number filter layer k+1\ layer input size along axis kernel\_size\_x_ —is convolution size along axis kernel\_size\_y_ —is convolution size along axis filters_ —is layer input size along axis following formula used count multiplication dense layer aligned input_ output_ aligned input_ —is layer input size number neuron k-1\ layer output_ —is layer output size number neuron layer general result layer presented table noted result universal atari environment dqn run table structure dqn network number multiplication layer optimization full size table number multiplication optimization clear degree weight sparsity affect number non-zero multiplication however delta algorithm provides different level temporal sparsity depending selected threshold layer input data impossible analytically estimate number non-zero multiplication therefore empirically calculated number significant multiplication calculating operation zero operand example number significant multiplication given next section table result figure show abovementioned neural network performance metric different sparsity level different environment reward metric result blue line similar result highest decrease reward pruning spaceinvaders breakout environment performance dropped quickly neural network sparsity grew time enduro reward pruning doe decrease even increase robotank freeway performance doe degrade seriously even sparsity level increase comparison unpruned version thus possible use sparse neural network environment without loss performance orange line demonstrates reward neural network delta neuron see similar sometimes little bit higher sometimes little bit lower performance network without delta neuron mean delta algorithm doe influence reward gravely gain number non-zero multiplication black dotted line dependent game agent playing example spaceinvaders fraction non-zero multiplication range 0.065 0.022 time breakout value range 0.018 0.009 delta algorithm alone without weight pruning lead decreasing number non-zero multiplication operation 5.5 time robotank time breakout time pruning lead modest gain environment fraction non-zero multiplication operation decrease result pruning others e.g. robotank fraction non-zero multiplication operation decrease pruning 0.184 0.059 x3.11 time breakout decrease 0.018 0.015 x1.2 anyway clear gain delta algorithm much higher gain pruning moreover pruning sometimes lead degradation performance made run every environment averaged metric presented supplementary material table number non-zero multiplication operation layer shown tested environment table one see highest gain occur first layer table number multiplication one breakout run 0.79 sparsity 0.01 threshold full size table table number multiplication spaceinvaders 0.79 sparsity 0.01 threshold full size table figure result freeway robotank enduro breakout krull spaceinvaders figure denote neural network sparsity degree left denote reward received agent right denote fraction significant multiplication averaged environment run orange line show performance pruned network blue line show performance pruned network additional application deltanetwork algorithm grey dotted line show fraction significant multiplication better pruned neural network enhanced deltanetwork algorithm blue dashed line demonstrate performance neural network without optimization full size image discussion variety performance gain different environment explained fact environment e.g. spaceinvaders much changing pixel time step others e.g. breakout breakout playground ball move spaceinvaders several object move once—shots ship alien well confirmed difference delta sparsity fraction neuron activation delta algorithm execution level playing breakout spaceinvaders see table fig visualized correlation level input sparsity caused delta algorithm fraction zero multiplication see tendency fraction zero multiplication increase inference increase fraction input zero figure correlation input sparsity zero multiplication fraction red square corresponds particular environment full size image part optimization approach pruning delta algorithm presented contribute reduction number significant multiplication see fig thus provided efficient combination structural temporal sparsity furthermore conducted examination parameter threshold affect performance operational gain assessed performance krull spaceinvaders using various value namely 0.001 0.005 0.01 0.05 result presented fig clearly illustrating 0.01 favorable parameter value among tested higher value despite winning multiplication option result significant decrease reward whereas lower threshold yield nearly identical reward modest productivity improvement figure comparison threshold krull spaceinvaders x-axis figure represents degree sparsity neural network left y-axes indicate reward achieved agent right y-axes represent fraction significant multiplication dotted line represent percentage significant multiplication lower better pruned neural network enhanced deltanetwork algorithm solid line depict reward agent different color correspond different threshold full size image multiplication expensive computational operation point view energy time desire reduce number operation obvious provided algorithm decrease number significant multiplication moreover due structural sparsity approach reduces number memory access also expensive energy time however presently opportunity use existing hardware effectively implement algorithm due modern gpus designed handling dense matrix nevertheless attempt turn situation nvidia began offering hardware support sparse matrix operation one tesla a100 gpus however maximum supported sparsity far author abovementioned deltanetwork algorithm introduced neuronflow architecture support delta algorithm loihi2 processor intel presented september also multi-core asynchronous architecture capable running sparse delta neuron-based neural network shown lottery ticket hypothesis pruning approach also work algorithm trained a2c method time deltanetwork algorithm could applied network working sequential data thus algorithm neural network training restricted dqn future would highly interesting apply proposed algorithm new environment real-world control task assessing performance real hardware measure real efficiency gain additionally would worthwhile explore potential enhancement algorithm incorporation neural network optimization method quantization lead reduced memory footprint integer arithmetic conclusion study demonstrates large redundancy operation multiplication inference neural network popular task minimizing number multiplication prove critical area computational energy efficiency key example edge real-time control robotics many others suitable equipment take full advantage benefit currently available consumer computational chip capable evaluating network inference proposed way currently researched developed part neuromorphic computing paradigm study highlight importance significant multiplication inference plethora neuromorphic computing application