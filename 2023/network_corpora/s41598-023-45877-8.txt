introduction success convolutional neural network cnn brought profound change field computer vision however order achieve performance improvement cnn continue scale compute storage requirement increase huge challenge deployment edge device limited resource order obtain efficient compact network model compression method low-rank decomposition network pruning quantization efficient network architecture design distillation rapidly developed among knowledge distillation promising method force student network extract knowledge larger teacher network order improve performance first proposed representation logic-based knowledge distillation recent year feature-based attention-based method emerged feature-based enables student network extract knowledge middle deep feature teacher network characterized high precision student network logic-based approach knowledge extracted logical output teacher network student network supervised truth label teacher logic attention-based method proposed force student network simulate attention map transferred teacher order learn valuable content paid attention classification process existing method divide knowledge distillation three type according type knowledge transfer feature based logic based attention diagram focus use single feature improve performance network unlike existing method goal investigate comprehensive inference information used improve performance distillation therefore re-examine cnn architecture find convolutional layer cnn spatially continuous also show time continuity forward transmission data continuous inference process input feature output category prediction feature map logical prediction represent inference result intermediate inference stage whole inference process respectively attention map reflects convolution layer pay attention inference process contains knowledge related inference process based finding reclassify existing method two type inference process based inference result based distillation method based feature logic traditional classification collectively called inference result based distillation method examine method new perspective find existing method use one inference process information inference result information ignore correlation two response phenomenon propose knowledge distillation method based attention feature transfer aft-kd fig transfer intermediate feature corresponding attention attempt student network time achieve better performance figure illustration aft-kd align teacher student network divide stage reasoning value varies different teacher-student structure example equal experiment section exploration afb full size image work mainly consists two part first part get inference information first find extensibility operation generating class activation map cam projecting weight output layer back convolutional feature map easily extended convolutional layer inspired use point convolution generate attention map corresponding intermediate feature map superimpose original feature map binarization fig call attention feature block afb according structure cnn divided different stage simulate different reasoning moment forced student network approach afb stage advantage student learn complete reasoning process block operation reduces required calculation amount second part balance loss function refer error generated approximating afb loss error predicted output truth label cross entropy loss loss order balance rate optimization loss loss prevent loss accuracy due continued training convergence one loss designed adaptive loss function adjust loss weight using ratio loss decay rate expected rate two loss decay rate figure illustration afb generation method full size image overall contribution summarized follows reclassify existing method inference process based inference result based reveal relationship existing method provide new perspective research knowledge distillation design efficient strategy balance optimal rate distillation loss cross-entropy loss easy scale multi-task learning scenario propose knowledge distillation method based attention feature transfer named aft-kd achieves state-of-the-art performance across multiple benchmark finally use total five chapter arrange content article second chapter introduces related work various knowledge distillation method according traditional classification method analyzes connection deficiency different method third chapter introduces proposed method detail including cam review aft-kd theoretical analysis adaptive loss implementation fourth chapter contains experimental content first introduced data set used experiment analyzed influence information contained afb distillation performance verified performance superiority afb-learned afb method finally verified analyzed actual performance adaptive loss last chapter summarize proposed method analyze limitation next related work concept knowledge distillation proposed hinton forced student network extract knowledge soft label ground truth label provided teacher order make full use dark knowledge contained soft label concept temperature introduced existing method mainly divided three type logic-based feature-based attention maps-based logic distillation transfer knowledge implicit output logic teacher model student network ban obtained superior performance teacher model directing parameterized network teacher dkd reformulates loss target-class knowledge distillation tckd non-target-class knowledge distillation nckd revealing coupling formula limit effectiveness flexibility knowledge transfer crosskd pass intermediate feature student network teacher detection head resulting cross prediction forced mimic teacher prediction addition several article logical distillation method feature-based method tend better performance forcing student extract valid content intermediate feature teacher network cost requiring computation logical distillation rkd transform relationship data example punish difference teacher student relevance similar transfer sample relevance study teacher student network pkt model teacher knowledge probability distribution divergence measure distance rkd multi-case relationship guide student learning crd combine comparative learning knowledge distillation comparative objective carry knowledge transfer reviewkd cross-layer connection path integrate knowledge implied feature different level method based attention diagram instructs student information network pay attention reasoning verifies validity shifting attention diagram class activation graph transfer knowledge student network cat-kd reveals ability distinguish category region key network classification prof ability acquired enhanced transferring cam cat-kd transfer knowledge transforming structure obtain attention force make attention-based knowledge distillation good competitiveness method based transfer logic feature good performance method based transfer attention diagram high interpretability previous study ignored link two characteristic paper aim solve problem proposing approach based attention feature transfer advanced several benchmark test method section first review cam analyze scalability propose attention-feature fusion afb apply knowledge distillation finally propose adaptive loss function combined decay rate loss loss review cam first consider commonly used cnn structure output feature last convolutional layer represents number channel represent height width feature respectively denotes feature i-th channel denotes activation spatial location channel point process generating prediction result regular cnn expressed follows aligned gap\left aligned represents logical prediction j-th category represents weight corresponding j-th category fully connected layer class activation graph cam generated projecting weight fully connected layer cnn back convolutional feature map get calculation formula corresponding cam j-th category cam_ equation also rewritten cam_ according relationship logical output logical output cam_ follows aligned cam_ gap\left cam_ aligned shown logical output class obtained calculating average corresponding class cam kind operation obtain inference result feature mapping common cnn example feature graph generated convolution kernel regularized activated obtain final conclusion therefore easily generalize cam calculation process location cnn generated cam-like map corresponds output feature map layer aft-kd gain knowledge related inference process extend cam convolutional layer cnn first formulate general operation convolution layer assuming cnn contains convolution layer input feature k-th convolution layer represents channel number height width input feature respectively input feature convolution layer aligned act_ norm_ conv_ act_ e\_f_ aligned act_ norm_ conv_ represents activation function regularization function convolution operation layer respectively e\_f_ represents pre-activated output feature approximate activation process pre-activated output feature mapping inference process inference result order extract knowledge inference process use pre-activated output feature e\_f_ compute class cam corresponding inspired use point convolution replace weight fully connected layer formula rewritten cam_ conv_ conv_ represents convolution kernel j-th channel corresponding output feature combined attention map corresponding expressed cam_ bin\left conv_ e\_f_ cam_ represents cam class corresponding bin\left stand binarization using binarization operation highlight convolution concerned reasoning finally superposition cam_ obtain output feature integrates inference process inference result called attention feature block afb calculation process attention feature block k-th convolutional layer follows afb_ cam_ basis propose aft-kd force student simulate knowledge afb transferred teacher order save computation align student teacher network divide cnn inference stage aft loss defined aligned aft aft afb_ afb_ aligned represents number channel output feature graph stage adjustment adjustment function used unify number channel resolution afb_ output feature map stage student network adaptive loss order balance optimization rate different loss design adaptive loss function based loss optimization rate determining aft loss define overall loss aft-kd aft represents standard cross entropy loss dynamic hyperparameter used balance aft optimization rate aft different phenomenon premature convergence certain loss training case continuing train network reduce unconvergent loss function cost sacrificing precision convergent task order avoid phenomenon introduce loss optimization rate balance decay rate two loss function first need record initial loss aft beginning training record current loss aft iteration point calculate loss attenuation rate iteration array dr\_ce dr\_aft aft aft array dr\_ce\ dr\_aft\ respectively attenuation rate aft relative initial loss round iteration average attenuation rate dr\_ce dr\_aft expressed array dr\_ce/\overline dr\_aft/\overline array according task optimized faster dynamic coefficient positive number round faster optimization smaller coefficient reduce optimization efficiency round achieve purpose balancing another task experiment show adaptive loss effectively shorten distance dr\_ce\ dr\_aft\ experiment datasets implementation detail datasets experiment mainly conducted two image classification datasets cifar-100 contains total 60,000 pixel picture category among training set verification set contain 50,000 10,000 picture respectively imagenet large-scale dataset containing classification object including 1.2 million training image 50,000 verification image implementation detail experimental setup cifar100 imagenet strictly followed lab cifar100 trained epoch using sgd optimizer batch size set initial learning rate 0.05 0.01 shufflenet mobilenet divided iteration experiment imagenet trained epoch batch size initial learning rate 0.2 every epoch decay one-tenth original addition conducted experiment various representative cnn network vgg resnet wideresnet mobilenet shufflenet table provides brief overview network table several common neural network briefly introduced full size table order ensure fairness experimental result result existing method either reported article obtained using code provided exactly setting result cifar100 average trial result imagenet average trial exploration afb section first explore effectiveness transferring afb propose learning continuous reasoning knowledge greater benefit student afb contains complete inference information cam output feature since reasoning process closely related reasoning outcome single transfer characteristic associated reasoning outcome teacher cause student imitate incorrect reasoning process afb contains complete inference information theoretically give student benefit gain directly observed student performance classification task transferred different information resnet32 resnet8 including output feature cam afb observed performance cifar-100 align teacher student network divide reasoning process three stage based resnet layer grouping shown table transferring cam output feature alone also achieve good classification accuracy transferring afb improve performance suggesting afb carry complete inference knowledge cam output feature knowledge directly lead student performance improvement addition moving afbs different location bring different improvement performance subnetwork table accuracy resnet8 trained using different knowledge cifar-100 transferred knowledge produced resnet32 full size table transferring continuous afb beneficial student forward propagation data network time continuity process input picture output prediction logic divided different stage reasoning result previous stage used reasoning input later stage therefore reasoning information including reasoning process reasoning result adjacent stage also closely related experimentally demonstrated delivering continuous complete afb student lead performance improvement similarly use resnet32 afb producer resnet8 learn different stage afb divided experiment three group afb generated layer1 studied learn afb generated layer1-2 learn afb generated three layer layer1-3 corresponds stage1-3 fig equal 3.as shown table learning part inference knowledge similar classification accuracy higher baseline network learning full afb result substantial improvement network performance table accuracy resnet8 trained cifar100 using knowledge different layer full size table evaluation aft-kd section compare aft-kd several popular knowledge distillation method including feature-based method logic-based method attention-graph-based method result cifar-100 similar work carried experiment cifar-100 using teacher-student framework different teacher-student framework respectively result reported table approach implement new sota teacher-student architecture experiment different teacher-student architecture among compared logic-based approach approach achieved better result experiment different architecture however student model mobilenetv2 performance aft-kd slightly lower speculate due large difference teacher-student architecture simplistic alignment used experiment architecture accuracy student model trained method exceeded teacher especially teacher model resnet32 accuracy aft-kd reaches77.14 4.64 higher teacher model table result cifar-100 teacher student different architecture full size table table result cifar-100 teacher student architecture full size table result imagenet table give top-1 top-5 accuracy image classification imagenet method doe achieve best performance due teacher ability still better method table result imagenet set resnet34 teacher resnet18 student full size table table result imagenet set resnet50 teacher mobilenet student full size table finally compare performance several sota method cifar-100 training set decay different proportion line practice ass dependence amount training data shown fig aft-kd least affected amount training data demonstrating excellent distillation efficiency method figure accuracy student trained several sota method cifar-100 set resnet32 teacher resnet8 student training set reduced various ratio full size image exploration adaptive loss analysis section adaptive loss see optimization rate imbalance loss function multi-task learning lead problem decreased accuracy later training period adaptive loss function automatically adjusts loss value according optimization rate aft loss cross-entropy loss effectively alleviates problem improvement measured distance loss decay rate curve multiple task fig illustrates comparison optimized rate curve adopting adaptive loss function sampled computed mean raw data optimizing rate resulting smoothed contrast curve observed balancing optimization rate two loss using adaptive loss function distance loss decay curve decreased loss optimized synchronized manner improvement also effectively enhanced model classification performance figure comparison optimized rate curve using adaptive loss function full size image conclusion analyze existing method reclassify method based inference process inference result provides new perspective study knowledge distillation based propose aft-kd attention feature transfer achieves competitive result several commonly used benchmark finally order balance loss optimization rate aft-kd propose adaptive loss function based loss decay rate improve performance aft-kd however experiment teacher student adopt different architecture performance aft-kd best among method guess caused large difference teacher student architecture alignment method used simple limitation improved designing specific alignment rule network structure addition compared feature-based method aft-kd brings certain improvement operation cost also something need continue explore future work