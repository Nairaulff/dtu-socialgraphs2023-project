introduction classification task typically solved using deep feedforward architecture architecture based consecutive convolutional layer terminate fully connected layer output layer size equal number input object label first function filter revealing local feature input whereas consecutive expected expose complex large-scale feature finally characterize class input deep learning strategy efficient consists many parallel filter layer depth differ initial convolutional weight depth typically increase along deep architecture resulting enhanced accuracy addition given deep architecture ratio depth consecutive accuracy increase function first depth deep learning strategy resulted several practical difficulty including following first although depth increase along deep architecture input size layer remains fixed second difficulty last output size depth layer input size becomes large serving first layer input consists large number tunable parameter computational complexity overload even powerful gpus limited accelerated utilization large number filter size layer one way circumvent difficulty embed pooling layer along pooling reduces output dimension combining cluster output e.g. 2\times one operation along deep architecture reduce dimension factor popular pooling operator max-pooling implement maximal value cluster average pooling implement average value cluster however type pooling operator exist core question work whether accuracy enhanced depending location pooling operator along given deep architecture instance vgg16 consists three layer five 2\times operator located along fig result indicate accuracy significantly increased smaller number pooling operator adjacent last optionally larger pooling size example advanced vgg16 fig optimized accuracy type advanced vgg architecture layer a-vggm first presented selected value 6\le m\le 16\ next underlying mechanism enhanced a-vggm accuracy discussed figure vgg16 a-vgg16 architecture vgg16 architecture 3\times 3\right 2\times operator followed layer cifar10 database consisting 32\times 32\ rgb input defined square filter dimension depth d\right a-vgg16 architecture cifar10 input consisting 3\times 4\times average pooling 3\times 2\times 2\right layer full size image a-vggm accuracy a-vgg16 consists 4\times 4\right average pooling 2\times 2\right 7th 13th respectively fig table maximal depth 512\ vgg16 maximum average accuracy 0.955 superior accuracy 0.935 obtained standard vgg16 fine-tuned optimized accuracy 0.94 fig comparable wide-resnet median accuracy consisting layer widening factor wrn16-10 table architecture accuracy a-vggm full size table note replacement pair pooling operator 4\times mp\left 2\times 2\right along a-vgg16 fig several option example 2\times 2\right ap\left 4\times 4\right 2\times 2\right mp\left 8\times 8\right also yielded average accuracy 0.95\ indicating superior robustness a-vgg16 accuracy vgg16 removing last three fifth block a-vgg16 resulting a-vgg13 average accuracy 0.955\ identical a-vgg16 first three leading digit table one possible explanation accuracy receptive field last three a-vgg13 7\times saturating 8\times layer input size also suggests accuracy mildly affected 13\ a-vgg8 architecture consists layer result 0.940\ averaged accuracy exceeding optimized vgg8 accuracy 0.915\ consists 2\times one also exceeds average accuracy vgg16 2\times 4\times placed 3rd 5th respectively table result indicates shallow architecture fewer pooling operator adjacent output imitate accuracy deeper architecture double number layer receptive field cover small portion layer input size using one layer reduces number layer two a-vgg16 a-vgg14 a-vgg8 a-vgg6 table result indicate accuracy mildly affected modification a-vgg6 achieves average accuracy 0.936 slightly exceeds vgg16 a-vgg14 exceeds 0.954 table note type architecture one layer consists fewer parameter mapped onto tree architecture unpublished similarly a-vgg13 a-vgg16 architecture linear activation function layer achieved similar averaged accuracy 0.954\ 0.955\ respectively small standard deviation supplementary information three linear layer folded one test procedure minimizing latency however training must performed three separated layer gap average accuracy a-vgg8 a-vgg6 0.004\ slightly greater enhanced accuracy a-vgg16 a-vgg14 table indicating gap decrease nevertheless comparable average accuracy a-vgg13 a-vgg14 a-vgg16 indicate removing two three layer removing three thirteen doe affect accuracy hence interesting examine average accuracy vgg11 two layer well last three removed optimized learning gain using pooling operator backpropagation learning step update weight towards correct output value given input typically learning step add noise destructive fraction training set however average accuracy increase epoch asymptotically saturates value identifies quality learning algorithm given architecture database one important ingredient downsizing input size layer progress done either pooling operator using stride although reduce size input pooling operator transfer specific output field maximal field operator aim select influential field small cluster node successive layer example 2\times 2\right underlying logic maximize learning step gain current input minimizing added noise zeroing route maximize learning minimal side-effect damage however local maximization doe ensure global one commonly several operator placed among example five time case vgg16 fig apparently solve simultaneously following two difficulty first although depth increase along fig input size layer shrink accordingly output size k\times grow linearly depth second successive operator appear select influential route first layer adjacent output layer however local decision following consecutive operator necessarily result influential route first layer elaborated using toy model assume binary tree random nodal value low medium high fig tree output equal branch maximal field calculated product three nodal value first strategy based local decision similar operator node maximal child among two selected gray circle fig selected route one composed gray node value m\cdot m\cdot brown branch fig however global decision among eight branch result maximal field h\cdot h\cdot green branch fig toy model indicates global decision differs local decision however probability event unclear figure comparison several small operator along large one end binary tree random nodal value low medium high e.g. 10\ 1000\ local decision selects path maximal nodal child gray resulting brown route connecting three gray node global decision selects green route maximizing product nodal value gaussian random 1024\times 1024\right input followed ten 3\times 2\times 2\right placed first four top similar architecture single 16\times 16\right placed 64\times 64\right output value denoted top bottom supplementary information probability function number 2\times 2\right n=4\ demonstrated full size image realistic model imitating deep architecture fig gaussian random 1024\times input followed ten 3\times unity depth fig two scenario local decision global decision discussed first 2\times operator placed first fig top exemplified n=4\ second one single operator placed ten fig bottom exemplified n=4\ scenario 10-n 10-n non-negative relu output denoted sequence pooling representing local decision top pooling representing global decision given 10-n 10-n ratio calculated averaged many gaussian random input several set ten randomly selected convolutional filter identical scenario probability indicates local decision consecutive result larger output global decision single supplementary information probability rapidly decreased possibly exponentially fig even n=2\ 0.1\ increase depth beyond unity doe qualitatively affect probability p\left 1\right indicated simulation vgg8 five consecutive 2\times operator single 32\times five five random 3\times 3\right convolution used architecture 512\ ratio single output filter calculated averaging cifar10 training input several selected set fixed random convolution result o\left probability result clearly indicate global decision selects influential route first layer hence pooling adjacent output layer superior selection following consecutive local pooling decision support using larger pooling operator adjacent output enhances accuracy table expected using pooling operator solely entire enhance accuracy even however validation simulation a-vggm architecture difficult running time per epoch large k\times deep architecture several time longer optimization accuracy currently beyond computational capability simpler architecture lenet5 much lower depth total number consisting two followed 2\times three layer fig optimized average accuracy cifar10 database 0.77\ advanced lenet5 a-lenet5 architecture consist pooling operator second fig particular two pooling option 2\times 2\right 2\times 2\right mp\ 2\times 2\right mp\circ 2\times 2\right ap\ examined example fig imitating dimension two 2\times 2\right mp\ lenet5 indeed a-lenet5 architecture enhance average accuracy 0.02 comparison lenet5 fig predicted abovementioned argument similarly using either 4\times 4\right mp\ 4\times 4\right ap\ second layer resulted 0.79\ maximized average accuracy shown shift one first second improves accuracy enhanced effect might expected skipping deeper architecture fig interesting aspect a-lenet5 accuracy improved although receptive field cover small portion input contrast a-vgg16 figure a-lenet5 accuracy architecture role non-commutative pooling operator a-lenet5 architecture pooling layer exemplified dashed rectangle placed second detailed architecture average accuracy five scheme a-lenet5 see supplementary information detailed parameter standard deviation non-commutative pooling operator number active backpropagation route 3\times 3\right ap\circ 2\times 2\right mp\ delocalized route 3\times 3\right mp\circ 2\times 2\right ap\ localized route 2\times 2\right ap\circ 3\times 3\right mp\ full size image another type a-lenet5 combination pair 2\times 2\right 3\times 3\right pooling operator two fig although input size first layer decreased 400\ lenet5 256\ average accuracy enhanced 0.011\ fig result exemplifies improved a-lenet5 accuracy even input size first layer decrease example fig consist pooling operator 2\times 2\right ap\ 3\times 3\right mp\ exchanged order operator average accuracy differ 0.016\ fig indicating pooling operator commute exchanged order another possible class commutation exchanged type operation color maintaining size exchanged mp\ ap\ fig average accuracy indicate pooling operator necessarily commute exchanged color two non-commutative class order type operation stem different number location backpropagation active route lower layer fig number locally active backpropagation route 6\times 6\right window 3\times 3\right ap\circ 2\times 2\right mp\ whereas 3\times 3\right mp\circ 2\times 2\right exchanged order operator fig number backpropagation active route case however route localized 2\times 2\right fig delocalized 6\times 6\right fig hence non-commutation pooling operator stem either different number active backpropagation route different location discussion aim pooling operator downsizing input size layer progress transferring specific output field maximal field operator selects influential local field doe ensure influential global field output proposed enhanced learning strategy based updating influential route maximal field output unit supported a-vggm a-lenet5 simulation average accuracy enhanced using pooling operator placed closer output layer fig table underlying mechanism aimed maximizing learning gain current input simultaneously minimizing average damage current learning entire training set learning step given input induces noise learning input hence increasing signal-to-noise ratio snr learning step average training set requires updating influential route current input maximize learning minimal side-effect damage realization proposed advanced learning strategy entail discussion following three theoretical practical aspect first selection influential route first layer necessarily equivalent selection influential route output unit however backpropagation step initiated influential input weight output unit update route since spatial structure disappears within one-dimensional layer hence proposed strategy approximates influential route output exceptional architecture a-vgg6 a-vgg14 table consisting one layer demonstrating accuracy slightly a-vgg8 a-vgg16 respectively second aspect concern computational complexity proposed advanced learning strategy selecting influential route fixed depth overloaded even advanced gpus since depth increase layer dimension doe decrease instance running time per epoch 32\times 32\right placed a-vgg16 slowed factor 10\ circumvent difficulty advanced learning strategy approximated placing first pooling operator maximal depth second operator fig table nevertheless interesting examine using advanced gpus whether placing pooling operator advance accuracy third aspect selection type dimension location pooling operator along deep architecture maximize accuracy given a-vggm several pooling arrangement result similar accuracy report one maximizes average accuracy given number epoch nevertheless maximized average a-vggm accuracy hint preferred combination placed maximal depth operates table fig might stem following insight carefully selects one significant backpropagation route among cluster route whereas close input layer spread incoming backpropagation signal multiple route arrangement found maximize accuracy several a-vggm architecture table however a-lenet indicated opposite trend top two adjacent pooling operator maximized accuracy fig role yet clear may depend database detail training architecture present argument indicating pooling decision adjacent output layer enhances accuracy table however one might attribute improvement increase number parameter first layer number parameter rest layer remain order pinpoint accuracy improvement location pooling operator obtained 0.954 a-vgg16 4\times 7th 8\times operator 13th architecture size first layer vgg16 therefore number parameter remain yet clear improvement accuracy non-commutative pooling operator feature exemplify sensitivity maximal average accuracy order type significantly enrich possible number pooling operator given dimension 8\times 8\right pooling dimension instance one find possible pooling operator 2\times 2\right xp\circ 2\times 2\right yp\circ 2\times 2\right zp\ equal either max average similarly number pooling operator dimension exponentially increase two type 2\times pooling operator allowed result a-lenet indicate enhanced accuracy achieved using combination consecutive pooling operator second fig however identification preferred combination maximize accuracy general deep architecture deserves research non-commutative feature pooling operator also stem different number backpropagation downstream updated route fig instance a-vgg16 32\times 32\right first layer consists single backpropagation downstream updated route per filter whereas 32\times 32\right 1024\ route nevertheless preferred pooling operator maximize accuracy need determined influential route favored correct output current input however induces output noise training input resulting low snr similarly updating 1024\ downstream route using including weak one increase correct output current input comparison however enhanced side-effect noise training input resulting possibly low snr hence given architecture dataset selection pooling operator maximize averaged snr per epoch yet unclear accuracy a-vgg6 a-vgg14 one layer slightly three layer a-vgg8 a-vgg16 respectively table architecture one layer characterized lower learning complexity number tunable parameter addition architecture mapped onto tree architecture generalizing recent lenet mapping tree architecture without affecting accuracy lower computational learning complexity tree mapping architecture comprising two inspired dendritic tree learning beyond scope presented work discussed elsewhere observed shallower architecture yield accuracy deeper one instance a-vgg13 a-vgg16 result attributed fact last receptive field completely cover input suggesting last three redundant another example a-vgg8 achieves accuracy vgg16 case last a-vgg8 receptive field doe fully cover input hence enhanced accuracy attributed advanced location pooling operator similarly a-lenet5 enhances accuracy lenet5 receptive field cover small portion input contrast a-vgg16 extension proposed idea deeper architecture cifar-10 e.g densenet efficientnet result following several difficulty moment beyond computational capability accuracy deeper architecture approach one thus enhancement accuracy preforming pooling decision adjacent output expected sub-percentage increase observation minor accuracy improvement require fine-tuned optimization high resolution hyper-parameter space well large statistic note running time per epoch even a-vgg16 32\times 32\right mp\ placed slowed factor made optimization beyond computational capability using datasets higher complexity class lesser number training example per class e.g. cifar-100 imagenet result significant fluctuated accuracy among sample fluctuation make observing effect pooling operator adjacent output layer much difficult deserves careful examination using advanced computational capability original vgg architecture constructed large input image size 224\times 224\ presented work demonstrates enhanced accuracy using a-vggm architecture small input image size 32\times 32\ extrapolating enhancement large image much beyond computational capability nevertheless preliminary result using online learning one epoch image size 128\times 128\ indicate slight improvement average accuracy a-vgg16 comparison vgg16 general one might expect kernel size a-vggm might require scaling size input image order entire input covered receptive field finally reported average accuracy a-vgg16 a-vgg13 approach wide-resnet16 widening factor median accuracy consisting architecture three main ingredient skip connection stride 8\times similarity hint ingredient dominating enhanced accuracy among three pooling operation