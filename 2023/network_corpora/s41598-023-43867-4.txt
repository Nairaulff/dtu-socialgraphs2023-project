introduction meta-analysis valuable technique used various field including medicine biology social science ecology combine information multiple study increase inference reliability random-effects model rem popular choice meta-analysis assumes actual effect size component study follow normal distribution overall mean variance often referred heterogeneity parameter =0\ rem reduced fixed-effect model fem rem preferred fems scenario account heterogeneity among study therefore applicable broader range scenario among rem generic model widely employed binary continuous outcome normal-normal hierarchical structure study let observed effect size binary data typically log odds ratio denotes within-study variance i.e. sampling variation study generic model specifies however rare binary outcome normal approximation given may work well due sparsity small sample size alternatively binomial-normal hierarchal structure popular substitute normal approximation assumes number observed event treatment control group study denoted follows binomial distribution total number subject event probability logit transformed probability assumed distributed normally second hierarchy log odds scale measure effect size several variation framework proposed bhaumik assumed logit =\mu logit =\mu +\theta denotes logit-transformed background incidence rate study smith spiegelhalter thomas considered equal variance control treatment group defining logit =\mu -\theta logit =\mu +\theta wang proposed flexible model defining logit =\mu -\omega logit =\mu 1-\omega new parameter adjusts variance ratio two arm previous two model viewed special case assigning =0\ 1/2 respectively avoid assumption independency houweilingen zwinderman stijnen proposed use bivariate normal distribution modeling logit logit allows correlation structure logit logit order test effect variable model discussed make common assumption true effect size 's\ follow normal distribution assumption convenient mathematical purpose may always hold reality distribution true effect size across different study could shape therefore conducting goodness-of-fit gof test crucial drawing conclusion making inference since misspecified model may yield misleading result researcher come various solution test normality model recently chen zhang proposed parametric bootstrap-type gof test mainly focused generic rem subsequently wang lee developed standardization framework evaluate normality assumption avoids need generate reference distribution therefore computationally efficient however method require continuity correction encountering single double-zero study impact type error rate statistical power furthermore previously proposed method investigate approach numerically different background incidence rate especially dealing rare binary outcome interesting important aspect explore meta-analyses binary outcome term bayesian alternative bayesian approach considered gof testing meta-analysis knowledge propose novel gof test meta-analysis utilizes pivotal quantity methodology proposed bayesian model assessment adapts cauchy combination test combine dependent value computed using posterior draw markov chain monte carlo mcmc inform final conclusion pivotal quantity function data model parameter whose distribution doe depend unknown parameter instance suppose random vector drawn density generates normal data ... =\sum i=1 -\mu pivotal quantity let sample corresponding posterior distribution method constructed based fact identically distributed proposed method called improved pivotal quantity ipq detect model failure level hierarchical model without extra computational cost additionally easily incorporated standard bayesian implementation automatically account available data without requiring artificial correction rare binary event method suitable general purpose focus primarily application meta-analyses rare binary outcome rest article organized follows first review bayesian technique ass model adequacy followed brief introduction pivotal quantity cauchy combination test introduce proposed method based generalized rem houweilingen zwinderman stijnen meta-analysis binary event also describe bayesian implementation method including adapting proposed ipq method within mcmc algorithm considering different bivariate covariance prior simulation section conduct simulation study evaluate method performance term type error rate statistical power compare existing gof method also evaluate four different covariance prior based estimating overall treatment effect inter-study heterogeneity correlation coefficient data example section illustrate method using three real data example first example utilizes handedness eye-dominance data study second one employ type diabetes mellitus gestational diabetes data study third gstp1 gene lung cancer data study end paper conclusion discussion review related bayesian work current practice bayesian model diagnostics mainly fall three category prior predictive posterior predictive pivotal quantity-based approach see figure illustration figure schematic diagram different model diagnostic method full size image prior posterior predictive check suppose distribution function specified p\left represents parameter model say study let denote observed data rep denote replicated data generated mimic real data box recommended using prior predictive distribution p\left =\int p\left p\left d\varvec reference distribution generate rep comparing step obtain prior predictive distribution illustrated figure given rep drawn prior p\left j=1 draw rep sampling distribution p\left rep use t\left t\left function data model parameter function data alone measure discrepancy data model assumption take t\left example simplicity evaluated rep model misfit concluded t\left unlikely reference distribution formed t\left rep however prior predictive check might problematic using improper weakly-informative prior commonly used practice gelman meng stern proposed model assessment using posterior predictive distribution defined p\left rep =\int p\left rep p\left d\varvec shown figure replicated data rep generated using rep posterior distribution p\left reference distribution based chosen discrepancy function t\left computed bayesian posterior predictive value obtained p\left t\left rep t\left quantitatively detect model misfit posterior predictive check gained increasing popularity bayesian model checking due straightforward implementation via monte carlo markov chain mcmc algorithm however two major limitation associated type approach firstly unlike traditional value posterior predictive value doe follow uniform distribution null hypothesis lack fit making difficult interpret ass level evidence null hypothesis secondly method almost power detect failure second deeper layer hierarchical model overcome non-uniformity problem potential solution calibrate posterior predictive value calibrated value follows uniform distribution asymptotically however statistical power using calibrated value investigated yet avoid issue bayarri berger proposed two new type value conditional predictive value partial posterior predictive value bayarri castellanos extended partial posterior predictive method test second layer hierarchical model avoids using data twice. however mentioned johnson partial posterior strategy typically straightforward implement beyond normal-family problem recently gosselin zhang recommended randomly drawing single value single posterior distribution generate rep namely sampled posterior check figure corresponding value distributed uniformly data model correctly specified approach achieved higher power original posterior predictive check detecting model misfit pivotal quantity methodology johnson pioneered use pivotal quantity detect model misfit yuan johnson extended upon methodology applied level hierarchical model since doe involve replicated data need distinguish rep directly used observed data pivotal quantity denoted d\left function data model parameter posse sampling distribution known invariant evaluated true data-generating value d\left johnson show d\left d\left identically distributed drawn posterior distribution p\left based result approach model assessment involves two main step first select pivotal discrepancy measure d\left known reference distribution second step evaluate model fit determining whether d\left considered draw however conducting gof test second deeper layer hierarchical model one may encounter difficulty since d\left depends layer usually involve data reason yuan johnson extended method defining pivotal quantity function model parameter showed d\left d\left identical distribution allows application pivotal quantity corresponding reference distribution diagnose model inadequacy level hierarchical model shown figure drawing p\left d\left evaluated i=1 ... d\left distribution d\left example suppose d\left 0,1 null hypothesis lack fit d\left 0,1 marginally test normality johnson suggested using formal approach shapiro-wilks test different value test calculated i=1 ... m.\ however combining value straightforward using fisher combination test value derived posterior sample using dataset dependent unknown covariance structure address issue johnson suggested one could avoid generating multiple draw dataset utilizing prior-predictive distribution dey suggested generating replicated datasets rep i=1 ... ,1000\ p\left illustrated figure replicated dataset rep bayesian data analysis performed obtain corresponding posterior distribution rep single rep randomly sampled posterior one value testing normality using pivotal quantity computed result independent value standard approach fisher test employed draw conclusion however method may suffer two limitation firstly using replicated datasets computationally intensive since mcmc procedure need run time draw independent posterior sample secondly non-informative prior may necessarily generate reasonable datasets considering difficulty johnson recommended finding probabilistic bound dependent value using property order statistic derived gascuel caraux rychlik let denote order statistic dependent sample random variable distribution function let denote distribution function k-th order statistic bound written aligned t\right mf\left t\right -k+1 m-k+1 aligned let ... dependent value m=1 ... null value distributed uniformly 0,1\right implying f\left t\right =t\ let =-p_ feng showed becomes aligned t\right t\frac aligned mean value upper bound observed k-th order statistic avoid choosing value suggested reporting minimum upper bound min =\text min k=1 ... yuan johnson advocated using rule-of-thumb value 0.25 cutoff declaring model misfit practice reject null hypothesis min 0.25\ however proposal may liberal simulation study simulation section show 0.25 necessarily good choice hard select optimal cutoff balance trade-off type error power method generalized rem meta-analysis binary event suppose meta-analysis contains independent study study let number observed event control treatment group follows binomial distribution total number subject corresponding event probability let denote logit-transformed i.e. 1-p_ generalized binomial-normal rem houweilingen zwinderman stijnen written aligned array bin bin logit =\phi logit =\phi array array array array array array array aligned modeled bivariate normal distribution arbitrary covariance structure define treatment effect =\phi -\phi study follows univariate normal distribution overall mean effect =\mu -\mu heterogeneity =\sigma +\sigma -2\rho generalized rem build strong connection many well-established model example model wang zhang special case model yielding aligned array array array -\omega +\left 1-\omega array array +\sigma -\omega 1-\omega -\omega 1-\omega 1-\omega +\sigma array aligned =\omega +\sigma =\left 1-\omega +\sigma =\frac -\omega 1-\omega +\sigma 1-\omega +\sigma mentioned introduction let 0.5 reduces model one bhaumik smith spiegelhalter thomas respectively thus model regarded generalized binomial-normal model fewer assumption choose basis design gof test detecting non-normality let =\left\ parameter =\left\ ... j=1,2\ =\left array array let =\left\ i=1 data full probability model given aligned p\left =\prod i=1 aligned aligned =\left array array array array 1+e^ 1+e^ aligned density function bivariate normal distribution joint prior distribution hyper-parameters introduced bivariate normal distribution proposed gof test inspired previous research propose novel gof test demonstrate applicability context meta-analysis rare binary event approach involves defining null hypothesis denoted assumes normality true effect size prevailing assumption made meta-analysis alternative hypothesis denoted formulated departure word aim detect specific departure bivariate normal model assumed second layer generalized rem draw conclusion presence overall treatment effect between-study heterogeneity let =\left data-generating parameter value study corresponding draw joint posterior distribution p\left m=1 ... define discrepancy measure capture deviation namely aligned d\left =\frac -\phi -\theta =\frac -\phi -\left -\mu +\sigma -2\rho aligned pivotal quantity follows standard normal distribution furthermore pointed one reviewer measure viewed study-specific effect unit standard deviation study posterior draw according yuan johnson aligned d\left =\frac -\tilde -\tilde =\frac -\tilde -\left -\tilde +\tilde -2\tilde aligned distribution identical d\left d\left 0,1\right every marginally conducting standard normality test using pivotal quantity based single draw straightforward sampled posterior approach problematic since vagary randomness produce sample seems unwise alternatively combining multiple mcmc draw draw conclusion recommended johnson yuan johnson probabilistic bound order statistic value used combine dependent value propose use cauchy combination idea combine dependent value consider value obtained i-th statistical test corresponding nonnegative weight sum liu xie introduced cauchy combination test demonstrated subject certain regularity condition tail test statistic linearly combine individual transformed value well approximated standard cauchy distribution null hypothesis specifically value test statistic given t=\sum i=1 0.5-p_ weight typically set absence prior information cauchy combination test several salient feature firstly test leveraging cauchy distribution test simple analytical formula compute value next unlike classical fisher test common test combining value minimum value test berk-jones test higher criticism test cauchy combination test handle value correlated statistical test remains valid arbitrary correlation structure finally test work well even one main assumption required test bivariate normality two test statistic generating value satisfied thus non-normal typed test i.e. test statistic normally distributed shapiro-wilk test cramer-von test anderson-darling test summary proposed gof test namely improved pivotal quantity ipq outlined following step step given independent study randomly sample joint posterior distribution p\left via mcmc i=1 ... m=1 ... step calculate d\left draw use d\left i=1 conduct formal normality test e.g shapiro-wilk test get value say step compute test statistic =\sum m=1 0.5-p^ calculate corresponding value using formula =\frac -\frac arctan reject pre-specified significance level e.g. =0.01,0.05,0.1\ bayesian implementation different covariance prior pivot discussion prior specification bayesian implementation use hamiltonian monte carlo hmc algorithm via stan version 2.19.1 conjunction fit model different prior discussed dataset run algorithm burn-in iteration additional sampling iteration convergence mcmc chain detected using gelman-rubin diagnostic start prior choice logit-transformed mean effect control treatment group consider diffuse uniform prior j=1,2\ define range get rough estimate study =\ln +0.5 -x_ +0.5 define lower bound =\min -c\ upper bound =\max +c\ let c=5\ bai prior conservative enough contain plausible value regarding prior covariance matrix several commonly used conjugate prior available including independent prior ind assumes mutual independence priori among element inverse wishart prior hierarchical inverse wishart prior hiw alternative include scaled inverse wishart prior siw prior based separation strategy bayesian inference covariance matrix highly sensitive different choice prior several study compared performance various prior example alvarez niemi simpson compared four different prior hiw siw multivariate normal model found prior performed worst among four especially true variance small rúa mazumdar strawderman conducted extensive simulation comparing prior including hiw ind different hyper-parameter specification multivariate bayesian meta-analysis model found prior overall poor performance hiw prior much consistent performance across scenario examined akinc vandebroek focused prior used alvarez niemi simpson investigated bayesian inference covariance matrix mixed logit model suggested using different prior check robustness result recommended avoiding prior best knowledge impact different covariance prior model context meta-analysis rare binary event investigated thus aim address gap access prior perform rare binary setting briefly review four class prior including hiw siw performance assessed simulation section inverse wishart prior due conjugacy property prior often used default choice covariance matrix density function prior defined p\left -\frac +3\right -\frac trace number degree freedom symmetric scale matrix two dimension marginal distribution correlation parameter p\left 1-\rho -3\right diagonal matrix =3\ follows -1,1\right model conditional posterior distribution given p\left +\varvec aligned array =\left array i=1 -\mu i=1 -\mu -\mu i=1 -\mu -\mu i=1 -\mu array .\end array aligned prior popular choice bayesian analysis due mathematical convenience also limitation one issue selecting appropriate degree freedom scaled matrix challenging although often set default value identity matrix respectively recent study rúa akinc vandebroek shown choice may always suitable another limitation prior implies strong relationship variance correlation bias inference specifically smaller variance associated correlation coefficient around larger variance correspond approaching dependency problematic interpreting result drawing conclusion statistical analysis hierarchical inverse wishart prior huang wand proposed two-layer hierarchical prior build upon work wand armagan artin showed half-t distribution expressed scale mixture inverse gamma distribution case dimension two hierarchical prior defined p\left =2\nu diag 1/a_ ,1/a_ inverse-gamma 1/2,1/a_ j=1,2\ typically assigned large value e.g 10^ indicate non-informativeness also showed marginal distribution correlation coefficient uniform -1,1\right bivariate case =2\ compared prior hiw prior provides increased flexibility choice scaled matrix retaining conjugacy property model conditional posterior distribution j=1,2\ become aligned array p\left +i+1 p\left ind inverse-gamma +a_ array aligned denotes j\right entry set =2\ however alvarez niemi simpson pointed compared prior hiw prior capable reducing eliminating dependency variance correlation separation strategy barnard mcculloch meng introduced prior class known separation strategy decomposes covariance matrix diagonal matrix standard deviation correlation matrix resulting =\mathbf specifically bivariate data =\text diag =\left array 1\\ array prior assigns independent prior correlation eliminates association variance correlation setting apart hiw prior posterior computation prior usually done via hamiltonian monte carlo hmc algorithm later improved no-u-turn sampler stan stan manual recommended hyperprior setting prior cauchy 0,2.5\right constrained j=1,2\ lkjcorr 1\right lkjcorr 1\right denotes lkj prior lewandowski shape parameter however implementing specific prior still requires intensive posterior computation hand ind prior simplest among class assigns independent prior bivariate case 0.01,0.01\right j=1,2\ -1,1\right reflect lack information term posterior computation involved ind prior much compared prior suggested stan manual done via gibbs sampler thus throughout simulation real data analysis ind prior used class computational efficiency scaled inverse wishart prior malley zaslavsky developed scaled inverse wishart siw prior decomposes covariance matrix differently =\delta bivariate case two dimensional unscaled matrix j\right element =\text diag siw prior defined ind implies standard deviation =\delta j=1,2\ =\frac compared prior siw prior avoids problematic transformation step yielding efficient sampling process following specification gelman hill akinc vandebroek set =\textbf =3\ =0\ =1\ j=1,2\ simulation conducted two simulation study focusing meta-analysis rare binary event first compare performance bayesian model various covariance prior choice estimation key model parameter term bias mean squared error mse coverage second ass performance proposed ipq method comparison existing gof test term type error rate statistical power code publicly available http numerical experiment used default continuity correction factor 0.5 frequentist method unless otherwise stated hand adopt continuity correction eliminate study containing zero event bayesian method since handle study automatically via incorporation prior information data analysis comparison different covariance prior choice simulated data using generalized rem evaluate performance bayesian model four covariance prior choice i.e. hiw siw ind estimating overall treatment effect heterogeneity correlation coefficient based three metric bias mse coverage specifically parameter interest say define bias =\frac j=1 -\kappa mse =\frac j=1 -\kappa corresponding estimate replicated dataset estimated using posterior mean using posterior median due heavily skewed distribution coverage probability computed using 95\ equal-tail credible interval generate data set =-5\ =-5+\theta =\sigma =0.5\ varied -0.5,0,0.5,1\right\ fixed =0\ =1\ recall =\sigma +\sigma -2\rho varied 0.2,0.4 ... ,1\right\ =1-\tau fixed =0\ varied =\left\ -0.8 -0.6 ... ,0.6,0.8\right\ =1-\rho\ fixed =0\ setting simulated probability =\frac 1+\exp =\frac 1+\exp study i=1 ... i.\ considered three meta-analysis size i=20,50,80\ allowed different sample-size allocation ratio across study setting =r_ 0,0.5\right number subject control group randomly drawn study number event generated bin bin figure report result based replicates setting i=20\ three row give bias mse coverage result three column correspond respectively observe performance ipq estimating seems insensitive choice different prior ind prior generally outperforms choice produce smaller bias mse well coverage closer nominal level scenario among three prior although tends better point estimation give worst coverage especially small close surprising mentioned prior induces dependency variance correlation bias inference i=50\ result omitted brevity discrepancy bias coverage result using different prior become salient worth noting still yield unsatisfactory coverage gof testing using proposed ipq method paper ind prior used due demonstrated better performance simplicity figure comparison bias mse coverage result bayesian estimate using inverse-wishart prior hierarchical inverse-wishart prior hiw scaled inverse-wishart prior siw independent prior ind meta-analysis rare binary event i=20\ study setting result generated using replicates nominal coverage level set 95\ full size image performance evaluation gof testing interest lie conducting gof test detect departure common assumption meta-analysis true effect size component study normally distributed using generalized rem null case generated normal distribution non-null case generated one four pre-specified non-normal distribution exponential distribution rate gamma distribution shape parameter scale parameter 0.5 unimodal right-skewed distribution iii mixture two equal-weighted normal distribution 1,0.5\right 4,0.5\right distribution number degree freedom z\sim 0,1\right given generated =\rho +\sigma 1-\rho z+\mu -\rho correlation coefficient mean variance j=1,2\ respectively word conditional distribution given set normal simulation note four distribution cover different type violation normality assumption first two cover skewness next cover multimodality last cover heavy-tailedness generating data way would pas type violation distribution without loss generality set mean equal overall effect =0\ =\mu =\mu -2\right\ corresponding 0.67\ ,4.74\ ,11.92\ probability scale set =0.5 =0.8\ null case set =0.8\ determined distribution type specified non-null case e.g. =1\ exponential distribution set number component study i\in 20,50,80\ -0.5,0,0.5\ compared proposed method ipq six approach including three frequentist-based approach naïve method conduct shapiro-wilk test estimated effect size log odds ratio directly parametric bootstrap method standardization method std three bayesian method pivotal quantity method using two cutoff 0.25 0.1 posterior predictive check ppc using discrepancy function recommended sinharay stern defined t\left =\left| -\text median -\left| -\text median i=1 sampled posterior check spc using discrepancy function set significance level =0.05\ frequentist approach std naive proposed ipq reject normality assumption value 0.05 ppc spc reject null posterior predictive value ppp 0.025 0.975 minimum value upper bound min chosen cutoff 0.25 0.1 reject null mentioned earlier either ppc reference distribution ppp min uniform 0,1 even asymptotic sense expect maintain type error rate however cutoff value 0.1 min method chosen via preliminary simulation offer error rate much closer 0.05 simulation scenario compared rule-of-thumb value 0.25 simulated replicates setting reported type error rate data null case statistical power otherwise figure comparison empirical type error rate proposed ipq parametric bootstrap method standardization method std naïve method naive posterior predictive check ppc sampled posterior check spc pivotal quantity method cutoff 0.1 0.25 pq-0.1 pq-0.25 data generated null case different combination replicates test conducted significant level =0.05\ full size image figure report type error rate method various setting ipq std spc method demonstrate superior performance maintain error rate close nominal value 0.05 regardless conversely pq-0.25 method recommended cutoff 0.25 frequently produce severely inflated type error rate particularly event interest becomes rarer pq-0.1 performs much better general therefore exclude pq-0.25 power result among remaining three method ppc often conservative rarer event i.e. =-5\ exhibiting type error rate 0.05 naive tends inflated rate rare event figure comparison empirical power proposed ipq parametric bootstrap method standardization method std naïve method naive posterior predictive check ppc sampled posterior check spc pivotal quantity method cutoff 0.1 pq-0.1 data generated non-null case exp different combination replicates test conducted significant level =0.05\ full size image figure comparison empirical power proposed ipq parametric bootstrap method standardization method std naïve method naive posterior predictive check ppc sampled posterior check spc pivotal quantity method cutoff 0.1 pq-0.1 data generated non-null case gamma 4,0.5 different combination replicates test conducted significant level =0.05\ full size image figure comparison empirical power proposed ipq parametric bootstrap method standardization method std naïve method naive posterior predictive check ppc sampled posterior check spc pivotal quantity method cutoff 0.1 pq-0.1 data generated non-null case 0.5\text 1,0.5 +0.5\text 4,0.5 different combination replicates test conducted significant level =0.05\ full size image figure comparison empirical power proposed ipq parametric bootstrap method standardization method std naïve method naive posterior predictive check ppc sampled posterior check spc pivotal quantity method cutoff 0.1 pq-0.1 data generated non-null case different combination replicates test conducted significant level =0.05\ full size image figure display power result different underlying distribution observe approach tend report higher power increase decrease also difference power among method become smaller perhaps method bottom group improve significantly proposed ipq best overall method appears much sensitive change figure present power result skewed distribution i.e. exponential gamma distribution ipq best nearly scenario followed pq-0.1 std naive often stand somewhere middle among tends perform poorly except larger smaller spc report slightly better result ppc method provide worst overall result particularly large figure present outcome multimodal distribution i.e. normal mixture ipq clear winner provides highest power setting among others std pq-0.1 usually perform better followed naïve spc ppc consistently give worst result show almost power figure display power result symmetric heavy-tailed distribution ipq outperforms method ppc spc tend perform worst summarize proposed ipq maintains type error rate target level well offer highest statistical power various departure normality assumption compared alternative approach data example applied ipq method along six method std naïve ppc spc pq-0.1 three real data set meta-analysis testing normality assumption distribution true effect size across component study first involves hand-eye dominance data second involves diabetes data third involves lung cancer data bourassa conducted meta-analysis study investigate hand-eye dominance association see table detailed data supplementary material study found hand-eye concordance larger one indicating left-handed people tended left-eyed dominance true right-handed people meta-analysis included 54,087 subject summarized 2\times table four category left-handed/left-eyed left-handed/right-eyed right-handed/left-eyed right-handed/right-eyed considered event interest left-handed control case group left-eyed right-eyed respectively overall incident rate control case group 18.5\ -2.75\ -1.48\ logit scale equivalently left panel figure display histogram density quantile-quantile plot observed log odds ratio revealing left-skewed distribution bellamy conducted meta-analysis study investigate association type diabetes mellitus gestational diabetes see table detailed data supplementary material analysis revealed woman gestational diabetes increased risk developing type diabetes study included 675,455 subject 31,867 type diabetes among control group gestational diabetes 6,862 subject type diabetes indicating overall incident rate 1.1\ -4.53\ logit scale case group gestational diabetes type diabetes resulting incident rate 12.5\ -1.94\ logit scale middle panel figure show histogram density quantile-quantile plot observed log odds ratio suggesting unimodal symmetric bell-shaped curve feng conducted meta-analysis study evaluate association gstp1 gene polymorphism risk lung cancer see table detailed data supplementary material event interest considered genotype gstp1 study included 26,516 subject genotype among control lung cancer case lung cancer group subject genotype implying overall incident rate 10.0 -2.19\ logit scale 10.8 -2.11\ logit scale respectively right panel figure reveals histogram density quantile-quantile plot observed log odds ratio showing roughly symmetric curve heavy tail side table show hand-eye dominance data significance level =0.05\ method except std reject null hypothesis indicating departure assumed normality note among existing method std quite competitive nevertheless failed specific example hand ppc spc tend conservative rejecting null worked diabetes data table show method conclusion evidence normality gstp1 gene polymorphism lung cancer data table show naïve pq-0.1 ipq reject null hypothesis method provide evidence normality shown figure given symmetric heavy-tailed distribution ipq offered highest power across case =-2\ similar overall incidence rate dataset naïve pq-0.1 performed relatively well however std ppc spc performed poorly case =-2\ therefore recommend avoiding normality assumption example summary ipq performs consistently well across three real data example std ppc spc sometimes fail although naïve pq-0.1 also demonstrate good performance satisfactory simulation study ipq recommended method choice context figure histogram density plot top quantile-quantile plot bottom observed effect size measured log odds ratio lnor left panel hand-eye dominance data middle panel diabetes data right panel lung cancer data full size image table value gof test three meta-analyses involving hand-eye dominance data type diabetes mellitus gestational diabetes data iii gstp1 gene polymorphism lung cancer data note ppc spc posterior predictive value reported pq-0.1 minimum value upper bound min reported full size table discussion meta-analysis commonly assumes actual effect size component study follow normal distribution mathematical convenience despite lack formal justification assumption practice however assumption frequently violated potentially leading inaccurate conclusion address issue propose novel goodness-of-fit gof test called improved pivotal quantity ipq testing assumption context meta-analysis rare binary outcome effect size measured log odds ratio proposed ipq method build upon strength original approach conceptually simple efficient detecting model misfit level hierarchical model without additional computational cost however original method employ probability bound criterion determine model misfit result inflated type error rate used rule-of-thumb cutoff 0.25 highlight need selecting new cutoff value tailored different application address limitation ipq method improves decision-making process adopting cauchy combination idea account dependent value addition given sparse data table zero event ipq naturally incorporates data without requiring artificial correction due bayesian model formulation fact ipq hybrid approach adopts frequentist framework hypothesis testing since cauchy combination test obtain value final conclusion drawn hand construct test statistic incorporating bayesian idea markov chain monte carlo method note use pivotal quantity sampling distribution proposed test statistic evaluated posterior sample known invariant i.e. 0,1 null hypothesis set posterior draw used construct test statistic data i.e. true observed data rather fake data simulation result indicate ipq maintains well-controlled type error rate achieving higher statistical power approach scenario demonstrate effectiveness method provide example three real datasets specifically result suggest normality assumption avoided hand-eye dominance dataset gstp1 gene polymorphism lung cancer dataset likely hold diabetes dataset situation normality assumption doe hold becomes imperative explore alternative distribution characterized heavy tail e.g. distribution skewness e.g. gamma distribution order accurately capture characteristic observed data alternatively one employ nonparametric method estimating treatment effect estimating heterogeneity furthermore scenario meta-analysis involves small number study situation commonly encountered practice alternative framework bayesian model averaging may yield reliable outcome although focus primarily rare binary event ipq method directly applicable meta-analysis binary data however believe gain performance common binary event may significant rare binary event demonstrated simulation study difference power method approach diminish increasing background incidence rate moreover ipq extended beyond testing normality scenario appropriate test statistic designed measure discrepancy conclusion ipq method useful detecting model misfit selecting appropriate statistical model different application particularly scenario sparse data present normality assumption question