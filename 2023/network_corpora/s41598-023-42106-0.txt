introduction outlier detection technique widely used variety practical application domain fraud detection credit card cyber-attack detection cancer diagnosis critical improve outlier detection order develop better strategy domain definition outlier vary different perspective fundamental definition outlier observation deviate much observation arouse suspicion generated different mechanism another way defining outlier outlier observation subset observation appears inconsistent remainder dataset definition motivate computer science community design new technique detect outlier among two typical method distance-based density-based method basic assumption distance-based approach outlier far away neighboring object basic assumption density-based method density outlier low existing full space outlier detection method work well high dimensional data distance-based density-based full-space approach work well low-dimensional data however dimension increase approach likely lose effectiveness due distance concentration data point become similar dimension increase phenomenon also known curse dimensionality make detecting outlier ineffective detection based either distance density high dimensional data space outlier may exist subspace full space detection outlier full space impossible since many irrelevant dimension involved figure show example outlier may detected full space easily discovered subspace example explains many classical outlier detection method work well relatively low-dimensional data set fail high dimensional data figure outlier clear subspace outlier difficult detected full space full size image subspace outlier detection become new promising approach finding outlier high dimensional data set subspace outlier detection method project data point lower dimensional space discovery outlier performed projection data point lower space alleviates problem curse dimensionality furthermore subspace outlier detection able detect outlier undetectable full space due irrelevant attribute interference however subspace outlier detection introduces problem firstly sheer number subspace make exhaustive search impossible finding outlier subspace using exhaustive searching algorithm problem even problem dimension selection i.e finding appropriate set dimension form subspace np-hard problem secondly large number repetitive test whether data point outlier subspace may produce many false discovery type error word test process decrease precision subspace outlier detection summary discovery right subspace outlier detection still open challenge general consensus dense subspace density distribution deviate significantly expected density distribution good outlier detection distance data point dense region used benchmark work subspace outlier detection sod subspace outlier degree measure outlierness data point using distance subspace closest neighbor exhibit low variance word preferred subspace closest neighbor dense region indicates likely generated mechanism data point deviating greatly closest neighbor likely generated another mechanism hence considered outlier density distribution used select right subspace outlier detection following work statistically selection relevant subspace project high contrast subspace key concept work density contrast outlier residual data point contrast maximized residual data point dense region density distribution significantly deviated expected density distribution uniformly distributed subspace measure outrank outres directly make use information cluster subspace cluster formed dense data point subspace already density distribution crucial criterion assessing whether data point subspace form cluster early work subspace clustering studied criterion searching subspace exhibit good clustering property first criterion high coverage requires data point cluster leaf small number possible outlier second criterion high density enforces quality cluster subspace entropy-based criterion ensures density distribution deviate expected distribution uniformly distributed subspace two criterion consistent measure outlier detection subspace discussed third criterion correlation dimension criterion used remove redundant subspace high dimensional subspace improve quality cluster lower dimensional subspace hence considered author derived set coherent entropy-based criterion searching selecting subspace satisfying three criterion hypothesize entropy-based criterion developed good finding quality subspace outlier detection simple coherent entropy-based criterion support efficient algorithm work propose intuitive definition measuring comparing outlierness data point subspace link outlier detection subspace entropy-based criterion searching clustering subspace integrating subspace produced entropy-based criterion outlier ranking measure propose efficient subspace outlier detection method epod ntropy-based subspace utlier etection epod accurate benchmark outlier detection method least accurate faster subspace outlier detection method parameter epod easy set demonstrate performance epod stable wide range parameter setting problem definition section firstly formalize problem investigate interaction subspace clustering criterion outlier detection afterwards explicate main motivation paper come intuitive method purpose outlier detection discover record deviate majority record data set record constituted number attribute depicting feature numerical information record preliminarily focus unsupervised method thus goal construct approach detect outlier interest attribute datasets instead attribute given data set dimension following definition based full space well subspace definition k-distance k-distance data point farthest distance among k-nearest neighbor data point denoted distance data point sorted distance ascending order k-th value -distance indicates closeness data point neighbor general data point close neighbor -distance small vice versa data point extremely far away neighbor reason suspect outlier definition value let value defined note use differentiate since relative origin whereas relative mean -distances subspace consider origin expected center -distances define -values relative ideal center instead real mean location process make -values affected mean location vary lot different subspace hence -values comparable different subspace objective find top data point largest -values subspace data point distant closest neighbor data space hence consider outlier improve chance detecting data area equally detecting point large value need find subspace small firstly given -th distance small make -value large secondly given fixed value small make probability data point occurring large assume group dense data point subspace slice cluster remainder spare data point belong cluster rewrite first part stand data point cluster second part stand data point outside cluster second part larger first part since data sparse outside cluster obtain small following two option reduce distance first part requirement equivalent high density subspace cluster minimize number data point second part requirement equivalent high coverage subspace cluster therefore subspace look outlier containing cluster high density high coverage cheng zhang studied relationship subspace entropy cluster coverage density concluded subspace entropy good indicator cluster density coverage shown h\left p\left p\left dimension data set mean subspace data space partitioned form grid probability estimated density cell low entropy generally indicates high density high coverage cluster subspace interesting subspace low entropy work another criterion correlation dimension also discussed correlation mentioned previous discussion necessary finding interpretable outlier note correlation paper doe mean statistical correlation correlated fully independent huge number subspace median dimensional data set huge number result noticeable number interesting subspace chance data object happen project dense high coverage space cluster interested user neither outlier subspace correlation attribute measured entropy interesting criterion subspace presented interest\ h\left h\left higher interest stronger correlation interest criterion prefers low dimension subspace many case need higher dimension subspace revised criterion called interest gain measuring increase correlation introduced interest gain\left ... interest\left ... max_ interest\left ... interest gain used criterion finding subspace strong correlation among attribute forming subspace jointly considering density coverage cluster subspace correlation attribute forming subspace following searching heuristic heuristic searching subspace low entropy high interest gain easy description call subspace low entropy high interest gain interesting subspace theoretically search outlier high -values subspace however unable search subspace need consider meaningful subspace dense high coverage cluster strong correlation among attribute called subspace interesting subspace condorcet jury theorem one another judgment observation might wrong majority might still right long judgment overall somewhat reliable every number decides independently others alleviate type error problem introduced top-n style outlier detection method definition top outlier given object let tbe subset subspace rank value sum z-values size exist object said set top outlier search top outlier becomes search top outlier interesting subspace note top outlier equivalent top data set process make searching possible also make detected outlier meaningful interpretable varying quantity data moreover reduce chance false discovery multiple test huge number subspace subspace outlier detectionmethod entropy-based clustering algorithm section discus proposed algorithm discover top outlier interest subspace based intuition behind candidate generation procedure algorithm also investigate criterion ranking interesting subspace use interest gain calculate entropy property clustering subspace use distance-based outlier score identify candidate top-n outlier concept described previous subsection develop major step proposed method outlined follows search potential subspace good clustering rank interesting subspace analyzing entropy characterization may higher percentage outlier rank top outlier using distance-based density-based algorithm interesting subspace search potential subspace good clustering bottom-up method used find interesting subspace using apriori-like method similar method procedure searching method follows algorithm first find one-dimensional subspace whose entropy threshold value based subspace construct candidate -dimensional subspace check whether entropy threshold value keeping subspace interest gain greater repeat process subspace found whose entropy refer subspace candidate interesting subspace ranking interesting subspace entropy criterion especially good clustering due lacking intuitive definition distance high dimension record value hence utilize entropy property subspace main criterion sorting subspace discussed previous section sorting entropy value interesting subspace able obtain list subspace possible outlier embedding additionally set dimensional pruning rule algorithm dimension candidate greater iteration algorithm stop sorting entropy value interesting subspace able gain list subspace possible outlier embedding majority voting top outlier algorithm typically far smaller cardinality dataset since outlier normally take small proportion whole record instead binary outlier indicator top-n outlier method provide ranked list object represent degree outlierness object user domain expert re-examine selected top-n outlier locate real outlier since detection procedure provide good interaction data mining expert user top-n outlier detection method become popular real-world application distance-based top-n kth-nearest neighbor distance typical top-n style outlier detection approach order distinguish original distance-based outlier detection method denote th-nearest neighbor distance outlier top-n paper top-n outlier distance object -distance indicates outlierness object intuitively larger -distance higher outlierness object top-n outlier regard object highest value -distance outlier integrated outlierness score point different subspace summing standardized top-n complexity algorithm step adopt enclus baseline algorithm since number interesting subspace normally large selection interesting subspace doe take much time apart cost time complexity algorithm determined enclus_sig procedure addition enclus sig shown reasonably scalable size dimension step 2–3 pruning interesting subspace could done linear complexity step querying k-nearest neighbor take majority computational load naively runtime step fortunately implementation elki optimized -tree index perform nearly log overall proposed algorithm heavily affected threshold interest interest gain significantly since two parameter set proper range value balance effectiveness efficiency discussed iv-d. pseudo code algorithm listed algorithm1 addition time complexity finding top outlier interesting subspace log number interesting subspace number data point data point every interesting subspace sorted find top outlier parameter used compute distance algorithm sensitive choice -nn based method demonstrate impact choosing performed experiment real-world data set order find best value full space -nn outlier detection method result listed section viii addition guideline determining good value discussed semi-supervised clustering field beyond article indicates proportion subspace used define appropriate amount interesting subspace entropy threshold indicates subspace dense clustering good coverage interest interest gain used threshold prune subspace attribute giving certain correlation subspace selection process iteration interesting subspace candidate subspace included list since subspace candidate set regarded deviant object interesting list may regarded interesting corresponding iteration one iteration generating next dimensional candidate subspace could find additional attribute form new interesting subspace constitute primary three step proposed method along analyzes algorithm complexity subsequently pseudocode epod algorithm isshown table table epod entropy based mining top outlier interesting subspace full size table experimental result evaluate effectiveness scalability proposed method prepare real-world data set group synthetic data set comparison proposed method state-of-the-art subspace method full-space method experiment performed intel® quad-core cpu clocked 4.5 ghz 16g memory running ubuntu 64bit 14.04 lts java jvm memory allocated algorithm utilized experiment programmed java language additionally due algorithm proposed method lack detailed implementation employed implementation famous outlier detection framework elki evaluate performance scalability implemented algorithm java performed workstation although partially parallelize algorithm sake fairness deploy method plug-in single working thread elki comparison algorithm following experiment show able discover outlier small number interesting subspace full space proposed method promising benchmark method data utilize real-world benchmark data set uci machine learning repository shown table synthetic data benchmark proposed method real-world datasets three medium dimension data set breast cancer wisconsin-diagnostic bcwd radar data goose bay ionosphere breast cancer wisconsin-prognostic bcwp preparation data set outlier detection act preceding experiment remove data missing meaningless especially attribute value across data record value furthermore label minority data point outlier class pendigits test digit rest digit keeping frequency class equal reduced number record preliminarily process data set group object wrong duplicate value large enough interference experiment may make result interpretable preparation bcwd data sethas attribute ionosphere data set attribute bcwp data set attribute preprocessed 129-dimensional data set arrhythmia note label information outlier used discovery process used drawing performance curve choose euclidean distance distance function throughout experiment find similarity objects.data preprocessing affect result substantially brief description data set used experiment shown table table brief description data set used experiment full size table relatively large absolute value easily dominate distance calculation lead loss information hidden important attribute counter effect min–max normalization deployed demonstrate normalization formula performed attribute individually number attribute norm x\left data obtained uci machine learning repository brief description data set illustrated table data set used experiment normalized interval attribute normalization method described synthetic datasets synthetic datasets suitable evaluating performance scalability since convenient manipulate deviation mean subspace dimensionality data moreover generating artificial data another advantage shape correlation coefficient attribute experiment synthetic data first use collection benchmarking data set published keller generated data set consist large number record evaluate data size scalability algorithm experiment utilizing elki data generator modify data generation script http decreasing dimension dimension generating record dimension scalability experiment collection synthetic datasets dimension range dataset record outlier generated hiding subspace 2–5 dimension outlier generated way easily observable lower dimensional subspace projection make detection challenging synthetic data set arff format file available supplementary material repetitive hics data size scalability experiment produce artificial data set attribute size 20,000 instance data set outlier point outlier data set deviated 4–5 dimensional subspace metric evaluation section describe metric measurement used evaluation different outlier detection algorithm previous study proposed confusion matrix classify result classification algorithm scenario outlier detection outlier marked positive class i.e algorithm identified observation outlier correctly credited true positive proportion true positive increase outcome better otherwise wrong classification incorrectly identified benign record outlier considered false positive rate decrease show algorithm accurate similarly also corresponding definition false negative case outlier misclassified normal point true negative defined case normal point correctly classified table show confusion matrix table confusion matrix full size table nevertheless hard reflect effective detector calculating single pair sensitivity specificity outlier detection known class imbalance problem since real-world data set mined common imbalanced class distribution roc receiver operating characteristic curve illustrated follows confusion matrix trade-off sensitivity false-positive rate 1-specificity imbalanced problem effective visual mean evaluating comparing performance predictive function example given ranking object according outlier score perfect outlier detection method would first return outlier followed remaining object using outlier score ranking criterion roc curve receiver operating characteristic mean choice compare performance different method roc curve start bottom left corner top right end top right corner diagonal line bottom left top right corner indicates random guess prediction ideal roc curve start along sensitivity-axis ending however unrealistic reach curve real-world outlier detection process common scenario practical outlier detection application knowing little number outlier increasing number outlier retrieve sensitivity 1-specificity increase simultaneously thus order compare performance different algorithm predictive function auc area receiver operating characteristic curve admitted tackling issue useful depict sensitivity method particularly target distribution imbalanced outlier detection task consequently use auc following experiment evaluate proposed method auc value 1.0 mean perfect separation area ideal roc curve equal auc value 0.5 corresponds random guessing since competitor rely parameter specifying number nearest neighbor considered compare roc auc value condition fixed value order measure statistical uncertainty result introduced confidence interval auc value also performed bootstrap hypothesistest proposed hanley mcneil experimental result obtain corresponding p-value python programming language utilized mathematical statistic panda scipy.stats proc sklearn.metrics package used complete statistic data result experiment real-world data validate approach hereby present experimental result real-world datasets compare epod state-of-the-art subspace outlier detection method hics outrank sod outres additionally also compared epod full-space distance-based density-based lof approach entropy-based subspace searching lof experiment comparison full space local outlier factor lof hics proposed method performed author publish source code hics therefore take result hics paper shown table moreover set parameter lof entropy-based subspace searching reported supplementary material repeatability parameter set follows minpts method search interesting subspace use 9.0 0.02 demonstrate epod able select small number interesting subspace good top outlier detection make result comparable restrict number interesting subspace subspace i.e method experiment produce subspace proportion subspace parameter disabled top subspace kept process experiment result reflects two subspace method perform well data set minpts parameter lof algorithm property upper lower bound however experiment fixed value chosen comparison purpose result reported hics little different result reproduce nonetheless hics still good performance datasets proposed method epod also get good result except data set breast diag 89.53 however comparing full space method lof breast diag 86.94 epod still show better quality see roc auc value epod consistently higher lof data set measurement interestingness hics based implicit notion density may appropriate density-based outlier score hence following section instead utilizing lof choose discriminate outlier conceal datasets outlier detection lof auc real-world data shown table table outlier detection lof auc real-world data full size table initial experimental result show proposed validation method concretely using subspace generated epod identify top-n outlier work effectively finding interesting subspace outlier detection lof competitive lower dimensional data set performance considerably deteriorates higher dimensionality epod remains stable optimal value entropy-based subspace searching knn paper main focus evaluate outlier detection quality epod distance-base outlier scoring function use distance score outlierness observation additionally subspace method sod outrank added comparison hics ensemble method outlier detection high-dimensional data suggests decoupled subspace search outlier ranking process perform outlier scoring function lof also distance-based scoring function like -nearest neighbor algorithm parameter hics set candidate cutoff monte carlo iteration limit alpha parameter setting suggested since hics integrated random sampling process perform experiment dataset time pick average value result sod experiment parameter sod sharednearestneighbor sod knn fair comparison algorithm set value choose fixed algorithm entropy threshold subspace selection 8.5 interest gain threshold 0.1 interesting subspace proportion top percentage object voting experiment parameter set yield consistently good result following section iv-d recommend choosing accordingly comparison effective performance different outlier detection method real-world data set using -nearest neighbor distance roc auc value shown table table comparison effective performance different outlier detection method real-world data set using -nearest neighbor distance roc auc value full size table result real-world experiment listed table pendigits data algorithm receive high score confirm well-chosen value performs best best data set diabetes auc designate equal much closer datasets similarly result data set pendigits also demonstrated phenomenon addition best value found viii concluded table proposed method outperforms overall preforms best algorithm four data set thyroid 98.62 breast 57.91 glass 82.33 ionosphere 84.22 compared approach rest four experiment divergence auc value general proposed method show high average quality auc implies favorable sensitivity parameter section discus impact parameter selecting epod algorithm effect parameter epod conduct experiment single data set study entropy threshold interest threshold affect performance epod top subspace ranking parameter help select subspace among subspace dense cluster lying inside subspace significant correlation pair-wise attribute respectively choose perform experiment arrhythmia database uci machine learning repository data set contains instance sample two class first class normal contains record marked outlier instance procedure experiment setting follows step described previous section note section subspace pruning ranking applied experiment result shown fig fig see data set auc epod monotonically increasing 3,5 reached stable state 5,12 fig see auc epod increase increase subspace produced selection process note number interesting subspace reached auc decreased slightly afterwards auc value stayed 0.84 number interesting subspace noted due effect algorithm pruning case increasing parameter produce subspace matter fact outlier detection task prefer higher auc stable result hence practical application one select value large possible acceptable computational cost guarantee effectiveness experiment epod work well 6,12 combining fig experiment suggest value suitable within 6,12 figure auc w.r.t parameter data set arrhythmia full size image effect ranking method epod order investigate parameter regard interesting subspace ranking method jointly impact outlier detection result conduct experiment data set previous subsection 6,12 incremental stepping 0.1 0.01,0.5 incremental stepping 0.01 result depicted fig utilize entropy interest gain subspace criterion ranking selected subspace two metric employed identify interesting subspace efficiently obtain value subspace mining procedure without additional calculation figure auc w.r.t subspace ranking method full size image fig see increase auc value epod relatively stable fig see auc value epod generally stable except around0.3 according experiment fig result indicate algorithm affected considerably outcome auc fig manifest stability need investigate result datasets conclude proposed algorithm epod doe need host subspace subsequent step thus changing two parameter number subspace returned entropy-based subspace selection also varies slightly real-world datasets overall change number subspace bring significant impact overall accuracy algorithm effect parameter real-world datasets experiment choose range parameter suggested previous result set entropy-based subspace selection process investigate impact quality outcome eight uci real-world datasets entropy threshold suggested set range 7,9 interval 0.1 interest gain threshold 0.2 value set 0.1 0.01 0.02 afterwards order confirm hypothesis experiment real-world datasets restricted parameter conducted result experiment shown table table impact entropy-based subspace parameter selection full size table scalability experiment evaluate runtime epod experiment performed synthetic data high dimensionality integrating proposed algorithm elki perform scalability toward increasing dimension ranged ranking subspace ascending order entropy shown fig ranking subspace descending order interest gain shown fig figure ranking subspace ascending order entropy full size image figure ranking subspace descending order interest gain full size image parameter set previous experiment experiment repeated time average runtime recorded depict runtime algorithm regard increasing dimensionality fig figure runtime w.r.t dimensionality full size image significant difference observed epod hics fig two algorithm exponential incremental performance overhead however runtime hics increased drastically dimension larger noted implement hics alternatively utilized elki framework conduct experiment using hics method epod typically performs better hics besides direct application lof method high dimensional problem often result unexpected performance qualitative cost due curse dimensionality discussion outlier detection approach density-based outlier detection method founded assumption density surrounding normal data point comparable neighbor commonly used technique include local outlier detection given rise numerous variation shown fig compared approach density-based method locally sensitive tend achieve higher accuracy notable representative local outlier factor lof computes relative density score using extended k-nearest neighbor approach lof effectively indicates unusualness instance serf outlier index density-based approach demonstrated strong performance across various application influenced subsequent work literature figure runtime w.r.t dataset size full size image hand distance-based approach assume normal data example emerge dense neighborhood outlier correspond isolated point method assign outlier score instance using robust variant mahalanobis distance measuring distance instance main body data distribution instance located far rest data identified outlier method falling category include knorr unified approach limitation distance-based method susceptibility curse dimensionality problem number parameter model grows quadratically number dimension rendering suitable high-dimensional data case high-dimensional approach proposed address sparsity data sample lack meaningful neighborhood typical method class either adopt invariant distance measurement angle-based outlier detection project data lower-dimensional subspace seen grid-based subspace outlier detection sparse pca subspace outlier detection approach generalized dimensionality reduction technique principal component analysis pca commonly used classification achieve lower-dimensional subspace projection however technique specifically designed preprocessing step outlier discovery word outlier algorithm often considered byproduct even disturbance performing selection lower-dimensional manifold consequently lack sensitivity outlier due focus obtaining robust eigenvectors making approach unsuitable direct outlier detection high-dimensional data one earliest approach address high-dimensional subspace outlier detection presented aggarwal approach bear resemblance grid-based subspace clustering approach differs search sparse grid cell instead dense one method identifies object contained within sparse grid cell outlier evolutionary search process aggarwal assume data point uniformly distributed projected subspace outlier associated data cube projected subspace significantly sparser expected however major issue approach arises dealing increasing dimensionality expected value grid cell quickly becomes low identify significantly sparse grid cell effectively furthermore critical drawback approach lie unrealistic assumption distribution data point uniform reality data point uniformly distributed sparse data cube normal dense data cube leading unreliable result additionally approach doe take account relationship attribute correlation among attribute significantly impact density data cube projected space compromising interpretability result nevertheless pioneering work represents first attempt tackle problem novel perspective enclus subspace clustering method based clique algorithm unlike directly measuring coverage density enclus calculates characteristic using entropy algorithm leverage understanding subspace denser cluster exhibit lower entropy cluster ability subspace defined using three criterion coverage density correlation measured using entropy cell density increase entropy decrease additionally certain condition entropy decrease increasing coverage correlation measured using interest metric interest defined difference sum entropy measurement set dimension entropy multi-dimensional distribution higher interest value indicate stronger correlation dimension interest value zero indicates independent dimension similar apriori enclus employ bottom-up approach used clique mine significant subspace pruning performed exploiting downward closure property entropy threshold upward closure property interest identify minimally correlated subspace subspace highly correlated threshold superspaces must also correlated since non-minimally correlated subspace might still interest enclus search interesting subspace calculating interest gain identifying subspace entropy exceeding interest gain surpassing interesting subspace identified cluster determined using methodology clique existing clustering algorithm enclus also share much flexibility seen clique another algorithm called high-dimension outlying subspace detection highdod proposed identify outlying subspace determines outlier degree based sum distance point nearest neighbor data set randomly sampled search begin data point high outlier degree outrank analyze result various subspace clustering algorithm including grid-based density-based method focusing stability cluster across different subspace outrank aim mitigate statistical bias identify outlier greater accuracy outlierness determined frequency object recognized part cluster also related correlation among subspace object resides however outrank assumes strong redundancy clustering treat outlier merely side-product clustering algorithm potentially leading large set outlier sod subspace outlier detection identifies outlier subspace without explicitly referencing clustering result instead reference set used possibly define subspace cluster part cluster implicitly query point significantly deviate subspace reference set considered subspace outlier respect corresponding subspace subspace distance outlier score based binary decision outlier vs. inlier rather normalized score however limitation sod include challenge finding good reference set oversimplification score normalization high contrast subspace hics refers subspace high contrast based correlation among attribute hics aggregate lof score single object across high contrast subspace suggesting use alternative outlier measure instead lof subspace outlier non-trivial deviate correlation trend exhibited majority data subspace however combining lof score subspace varying dimensionality without score normalization introduces bias score problem moreover naive combination score could benefit ensemble reasoning identifying interesting subspace may yield different outcome depending outlier ranking measure used hics measure interest relies implicit notion density making suitable density-based outlier score despite decoupling hics offer valuable insight issue subspace selection main focus study conclusion paper explored application entropy-based clustering notable sensitivity mining cluster characterized denseness small size presented intuitive definition outlier detection high-dimensional space thenwe introduced epod efficient algorithm outlier detection utilizes entropy criterion subspace selection experimental comparison existing method epod exhibited superior performance proposed algorithm exhibit remarkable effectiveness medium-dimensional datasets showcasing excellent scalability high-dimensional datasets employing entropy-based selection epod rank numerous meaningful subspace subsequently leveraged standardized k-nearest neighbor knn distance calculate outlierness observation across different subspace although epod performance lof doe match knn considering trade-off efficiency scalability remains promising approach drawing statistical implication outlier score propose algorithm vote outlierness point significant interesting subspace best knowledge previous work utilized entropy metric mine cluster embedded subspace full space prior method harnessed specific problem outlier detection subspace highlight novelty potential significance approach field outlier detection