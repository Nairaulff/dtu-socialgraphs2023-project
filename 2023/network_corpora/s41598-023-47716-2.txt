introduction every aspect day day requirement often necessary sensibly organize data relevant group give clarity whereabouts also help pick respective assemblage much faster important thing need considered correct grouping among consequently order expedite retrieval relevant information group sub-group numerous consistent practice developed one data clustering odell duran main goal create division whole data set reasonably smaller homogenous subdivision object present subgroup similar characteristic reasonably differ present subgroup clustering technique come practice first foremost step selection initial cluster center defines number cluster created various way initial cluster center initialized initialization play crucial role end result clustering random selection initial centroid preferred many clustering algorithm add simplicity approach also computation time algorithm initial centroid chosen randomly roughly time spent selection step lessens overall execution time well time complexity algorithm however random initialization centroid different run produce different clustering result sometimes result show excellent subgroup formation case resulting cluster often poor possible obtain optimal clustering two randomly selected initial cluster center fall somewhere pair cluster cluster center reorganize among one cluster unfortunately case number cluster likely case one pair cluster merely one initial cluster center case since pair cluster farther apart cluster within pair clustering algorithm fail reallocate centroid among pair cluster simply local minimum result word empty cluster may achieved point allocated cluster assignment phase clustering problem using randomly selected initial centroid even repeated run may overcome need develop way better initialization initial center clusters. case found clustering model designed purpose face difficulty identifying natural cluster case arise cluster widely different shape size compactness example considering dataset grouped three cluster one cluster formed relatively bigger two bigger cluster broken one smaller cluster combined section bigger one another instance clustering model may fail create well separated subgroup two smaller cluster much compact bigger one obviously lead inaccurate conclusion structure data need design clustering model efficient enough perform required formation subgroup improved reliability accuracy importantly achieving result much faster varying datasets two motivational factor gave direction continue work aspect although individual exceptional cluster seeker necessity excellent clustering algorithm operate versatile data set provide effective cluster formation bunch clustering ensemble method proposed eminent researcher projected last year present solution selection initial cluster center well obtaining good cluster formation discussed conducted examination constraining facet k-means algorithm proposing alternative approach assigning data point distinct cluster method mitigates computational time required k-means comprehensive survey scrutinized diverse array clustering methodology along practical application additionally deliberated various proximity criterion validity metric influence resultant cluster configuration cheung introduced tailored k-means variant capable achieving precise clustering without need initial cluster assignment technique demonstrates notable efficacy clustering elliptically-shaped data pivotal aspect research innovative endeavor advocated adoption nearest neighbor pair concept determining initial centroid k-means algorithm method identifies two closely neighboring pair exhibit significant dissimilarity reside separate cluster represents one among several approach aimed advancing determination initial cluster centroid nazeer proposed progression towards ascertaining nearly accurate initial centroid subsequently assigning object cluster albeit stipulation initial number cluster must specified input order mitigate stochastic selection initial cluster center k-means algorithm cao introduced model wherein cohesion degree within data point neighborhood coupling degree among neighborhood defined model complemented novel initialization technique center selection kumar suggested kernel density-based technique determine initial center k-means plan pick initial data denser part data set since actually reflects characteristic data set presence outlier avoided performance method tested different data set using various validity index result showed given method superior clustering performance traditional k-means k-means++ algorithm kushwaha proposed clustering technique based magnetic strength objective locate best location centroid respective cluster data generate force directly magnetic force best possible position centroid force data draw nearer zero result experiment imply suggested method getaway local optimum however limitation need prior information number cluster created mohammed introduced wfa selection modified weight-based firefly selection algorithm designed attain optimal cluster algorithm amalgamates selection cluster generate cluster superior quality result demonstrate algorithm yield newly condensed cluster compared subset alternative approach recent study fahim conducted comprehensive review classical density-based spatial clustering application noise dbscan algorithm scrutinizing inherent limitation proposing method mitigate suggested technique identifies maximum allowable density level within cluster enabling dbscan evaluate cluster varying density comparative analysis confirmed effectiveness proposed method accurately determining actual cluster fahim suggested technique discover optimal value initial center k-means algorithm pre-processing step used purpose k-means applied density-based method used purpose doe need initially mention number cluster also calculates mean data cluster suggested method also dbscan algorithm pre-processing step experimental data suggested method ultimately improves final result clustering also reduces number iteration k-means khandare presented modification k-means dbscan clustering algorithm proposed approach enhances clustering quality establishes well-organized cluster incorporation spectral analysis split-merge-refine method notably algorithm address minimization empty cluster formation experimental assessment conducted taking account parameter cluster index computation time accuracy datasets diverse dimension yao grouped non-numeric attribute present dataset according property discovered analogous similarity metric order proposed method finding initial centroid based dissimilarity compactness data center obtained clustering performed modified inter-cluster entropy miscellaneous data result concluded since initial center determined optimal resulted good clustering accuracy rate car ren suggested two-step structure scalable clustering initial step determines frame structure data final step doe actual clustering data object initially placed across 2-d grid clustered using different algorithm giving set partial core point point correspond dense part data form center center-based mode density-based mean probability-based type clustering method speed-up computation produce robust clustering result shown usefulness method franti suggested better initialization technique improves clustering efficiency k-means overlapping cluster using farthest point heuristic malicious cluster may reduced method repeated time reduction noted however remarked dataset well separated cluster depends mostly proper initialization center mehta discussed analyzed several proximity measure suggested way choosing proximity measure used hierarchical partitioned clustering concluded average performance clustering change diverse proximity measure implemented mehta researched document clustering text mining proposed method hybridizing statistical semantic feature technique fewer number feature hybridization improves textual clustering provides better precision within acceptable time limit shuai proposed improved feature selection clustering framework model initially doe data processing feature selection obtain important feature dataset followed hybridizing k-means som neural network perform actual clustering finally collaborative filtering used cluster datasets constituted missing data make sure sample acquire result result obtained showed high accuracy clustering interpretability nie proposed clustering technique need calculate cluster center iteration addition proposed technique provides efficient iterative re-weighted approach solve optimization problem show faster convergence rate ikotun broadly presented summary taxonomy widely used k-means clustering algorithm different variant created various researcher record k-means recent development different issue challenge suggested potential research viewpoint discussed found research work carried solving initialization issue k-means however little focus given addressing problem mixed data type survey help practitioner work aspect method one major drawback traditional k-means approach initial center selection done randomly random selection center may perhaps result incorrect creation cluster due issue suggestion mentioned work aim minimise limitation addition suggestion reducing time complexity actual computation time convergence criterion projected method subsequently method evaluated verify good creates subgroup method choosing initial cluster center discussed method-1 k-means unsupervised learning algorithm mac queen find sub-groups given set data value defined user initially random data point selected cluster center data point assigned nearest cluster center form disjoint sub-groups cluster center overwritten value obtained taking mean data point present inside process reassigning data point nearest center updating cluster center repeated change center step followed k-means take random data point initial center assign data nearest cluster center update cluster center taking mean repeat step 2–3 convergence k-means simplest way getting different sub-groups given input however taking random center step-1 make unpredictable every time algorithm executed produce different result precision clustering outcome even possibility time may result creation empty cluster indefinite time complexity since computationally np-hard problem step-2 procedure may repeat uncertain amount time however constraint clustering loop fixed value complexity algorithm become number cluster data point respectively dimension data practically value dare fixed significantly smaller hence complexity method-2 far efficient k-means fekm overcome limitation selecting initial cluster center randomly mishra suggested innovative approach central idea approach obtain distant point initial center result disjoint tight precise cluster formation pseudo code representation algorithm follows algorithm work follows distance data point point present data set computed data pair lie farthest considered first two initial center step step step assigns data point nearest center pre-defined threshold value reached step center updated taking mean partial cluster formed previous step remaining k−2 center computed step process obtaining center illustrated fig figure computation center farthest pair two initial center distance point nearest center calculated selected third center since maximum distance nearest center full size image due brute force comparison data point find farthest pair step loop run time making complexity hence overall complexity fekm major drawback fekm worst case running time complexity method like fekm quadratic time complexity feasible small data set method-3 modified center k-means mckm considering quadratic time complexity fekm mishra ijisa suggested mckm selecting initial cluster center complexity idea sort data point respect fixed point reference last data present data set matrix divide data set equal subgroup compute mean group find initial center procedure presented pseudo code follows step set last element present data set matrix first center point reference step-4 distance element determined point make complexity step sort data using sorting technique complexity log step sorted list splitted equal subgroup cause loop run time making complexity center updated mean group give rest k-1 center step whose complexity thus overall complexity mckm log method able provide systematic efficient procedure obtain initial center unlike fekm better running time complexity however fekm upper hand mckm obtaining distant initial center result better cluster formation hastening fekm constructing convex hull fekm brute force technique find farthest data pair considers first two initial center result quadratic time complexity algorithm order reduce complexity fekm concept convex hull cormen used convex hull smallest convex polygon encloses data point given data set point selected way exist point remains outside hull computing convex hull required compare data point form vertex hull instead considering every data point obtain farthest pair two approach considered research achieve farthest center using convex hull follows farthest center using graham scan fcgs graham scan approach graham maintains stack contains vertex hull procedure push data point stack ultimately pop point vertex distance vertex determined obtain farthest pair considered first two center hence required compare distance point reduces complexity algorithm algorithm fcgs follows step algorithm chooses point smallest y-coordinate value leftmost x-coordinate value two point equal y-coordinates step need traverse reach every data point make complexity step data point excluding sorted per polar angle around anti-clockwise order using sorting algorithm complexity log first three point sorted data set i.e pushed stack step iteration step data point pushed stack orientation cormen formed top three element stack checked orientation clock wise non-left pop operation performed traverse n−3 point step complexity obtaining stack hull vertex vertex distance-wise compared find farthest pair step assuming number element stack complexity step remaining k−2 center computed step following fekm always overall complexity fcgs log therefore method able obtain initial center boost clustering performance reduced complexity log however graham scan designed data set two attribute dataset consists two attribute algorithm fails multiple value polar angle dimensionality reduction way may increase running time algorithm drastically one solution fcgs using method construct convex hull multiple dimension modified fekm using quick hull solution aspect modified fekm using quickhull mfq quickhull proposed bradford barber may used construct convex hull n-dimensional data computes convex hull divide conquer approach recursively pseudo code algorithm follows first seven step algorithm used construct convex hull method list named hull declared store vertex convex hull point maximum minimum coordinate value assigned variable max_x min_x respectively line constructed joining min_x max_x line divide data set two part point found farthest added hull triangle formed joining two end point point lying inside triangle considered construction hull remaining point present outside triangle side triangle assigned steps5 6of algorithm repeated recursively point left outside last triangle formed obtaining convex hull vertex lying compared find farthest pair considered first two initial cluster center finally step8 fekm repeated obtain k−2 remaining center procedure illustrated fig figure illustration mfq step 1–7 algorithm used construct convex hull step algorithm compare vertex hull find farthest pair first two initial center step fekm used find remaining k−2 center full size image step method cost traversing data set find minimum maximum x-coordinate value step computes point farthest line making complexity step repeated recursively point left outside new triangle formed worst case complexity quickhull log dimension data equal three number attribute three complexity method increase next step8 fekm called determine remaining k-2 initial center make complexity unlike fcgs method able effectively operate datasets number attribute worst case log .however worst case complexity algorithm may exceed large number attribute data set case method prove inefficient fekm reason suggest method called farthest leap center selection flcs compute farthest data pair quadratic time complexity proposed method farthest leap center selection flcs due limitation graham scan quick hull mentioned came new approach called flcs solve farthest pair problem flcs take greedy approach solve problem instead traversing every data point leap next point maximum distance current point method stop leaping next farthest point previous point pseudo code algorithm follows initially mean data point calculated assigned data prev farthest point data computed comparing data point assigned data data assigned data data assigned prev farthest point data determined assigned data step repeated data prev refers data point step illustrated fig farthest pair data data obtained assigned first two initial center obtain remaining k−2 center step-8 fekm repeated figure selection first two initial center discovering farthest pair mean data set assigned data prev farthest point data data farthest point data found new farthest point labeled data original data labeled data data labeled prev step repeated prev data refers data point data data farthest pair two initial center full size image step algorithm traverse data point find mean making complexity step used leap current data point farthest point order skip redundant comparison needed compute farthest pair assumes number leap step iterates time however much smaller complexity step5 .then step fekm used compute remaining k−2 center making complexity step7 flcs therefore overall complexity flcs lemma number leap flcs always total number data point given set data point number leap prove proof let construct convex hull data set set vertex exist data point present outside hull cormen 2/3 suggested jarnik exactly two space method leap case leaping within convex hull farthest point point inside convex hull vertex hull exist point beyond convex hull case leaping vertex convex hull farthest point vertex hull another vertex considering two case concluded procedure start leaping first point present inside convex hull first point obtained taking mean data point possible leap point present inside hull possibility leaping point belongs therefore number leap ≤|v|≤ ^\frac hence proved parameter evaluation quality cluster formed clustering process evaluated using validity index dbi index measure goodness cluster basis inter-cluster intra-cluster distance similarity instance present created cluster data set contain class label used ground truth label compute clustering accuracy rand index clustering outcome assessed verify generate perfectly homogenous complete subgroup measure chosen parameter evaluation execution time considered method recorded input data set practically verify efficiency validity index cluster validation technique used measure quality cluster appropriately validity index used purpose given follows dunn index measure dunn used minimize intra-cluster maximize inter-cluster distance defined follows- min min max given distance function used purpose set consisting data point assigned ith cluster usually method produce larger value determines cluster formed compact well-separated cluster davies–bouldin index dbi dbi davy ratio sum data currently present within cluster data remaining outside data present within ith cluster distribution given data ith jth partition given denoted ith cluster center numerical value chosen independently number element present subsequently determined given equation finally davies–bouldin index obtained specified follows objective clustering method focus obtaining minimum value dbi achieving proper clustering silhouette coefficient rousseeuw silhouette data point initially average distance data point belonging cluster determined denoted minimum average distance data point present cluster determined silhouette coefficient calculated follows =\left\ array 1-\mathrm if\ =\mathrm array silhouette value range close indicates sample well-clustered silhouette value almost equal zero indicates sample lie equally far away cluster silhouette value equal indicates sample somewhere cluster number cluster maximum average silhouette width taken optimal number cluster clustering accuracy accuracy cluster measured using rand index rand given data point set two clustering compare define rand index +\mathrm +\mathrm +\mathrm +\mathrm number data pair subset number data pair different subset number data pair subset different subset number data pair different subset subset value representing two clustering agree pair data point representing clustering precisely identical liu guo zou investigated data collection wireless powered underground sensor network assisted machine intelligence matrix algebra directed network limited sensing deep data mining respectively shen cao sheng respectively examined modeling relation path knowledge graph completion optimization based mobile data dataset semantic segmentation urban scene xie considered multiscale feature extraction fusion image pattern across mobile app usage simple monte carlo method estimating chance cyclone impact recently liu fan developed multi-labeled corpus twitter short text studied long-term evolution mobile app usage proposed axial data modeling via hierarchical bayesian nonparametric model homogeneity completeness v-measure cluster said perfectly homogenous contains data point belonging single class label homogeneity reduces data point belonging different class label present cluster similarly cluster said perfectly complete data point belonging class label present cluster number data point particular class label distributed different cluster completeness cluster decrease data set contains number data different class label separated cluster data point belonging class cluster homogeneity given ~\left -\sum_ log -\sum_ log completeness given ~\left -\sum_ log -\sum_ log thus measure rosenberg hirschberg ,2007 given set favour homogeneity completeness favour homogeneity completeness equally greater completeness favoured favour homogeneity result discussion order analyze practical performance k-means fekm mckm mfq flcs method evaluated result various real world data set uci repository characteristic data set given follows diverse range datasets considered evaluate performance discussed method data set table vary dimension property classification iris data set consists three flower classes–setosa virginica versicolor fifty instance class total hundred fifty instance instance four attributes- petal length width sepal length width similarly seed data set consists two hundred ten instance seven attribute contains wheat kernel three classes- kama rosa canadian adult data set consists fourteen attributes- age work class final weight education marital-status occupation relationship race sex capital gain capital loss hour per week native-country data set classified two classes- person earning equal per year balance data set contains six hundred twenty-five instance psychological experiment instance four feature include left weight left distance right weight right distance instance divided three classes- scale tip right tip left balanced haberman record study patient survived surgery breast cancer contains record three hundred six patient patient classified two categories- died within five year survived five year longer three attributes- age patient time operation year operation number positive auxiliary node detected tae contains teaching performance one hundred fifty-one teaching assistance categorized into- low medium high per performance wine data set result chemical analysis wine growth italy three different cultivator one hundred seventy eight sample instance thirteen features- alcohol malic acid ash alkalinity ash magnesium total phenol flavonoid flavonoid phenol proanthocyanins color intensity hue od315 dilute wine proline mushroom data set record eight thousand one hundred twenty four mushroom sample twenty two characteristic data set classified two category mushroom- edible poisonous table characteristic data set full size table discussed earlier section parameter evaluation greater value smaller value dbi suggests better quality cluster formation table contains dbi score respectively clustering loop restricted twenty iteration table observed method fekm mckm mcq flcs perform better form quality cluster k-means mcq flcs show promising performance seen table table best performing method highlighted overall observed majority data set mcq flcs performed better method table score k-means fekm mckm mfq flcs number iteration full size table table dbi score k-means fekm mckm mfq flcs number iteration full size table table score k-means fekm mckm mfq flcs number iteration full size table accuracy cluster formed next parameter experimental evaluation rand index score discussed section parameter evaluation considered aspect normally value rand index lie value nearer signifies better accuracy table table homogeneity score k-means fekm mckm mfq flcs.data setsk-meansfekmmckmmfqflcsiris0.73640.75020.73640.75140.7514balance0.08450.15890.07770.34510.3451abalone0.06750.05880.12090.09160.0916seed0.66450.70750.69340.70750.7075tae0.01950.02650.02520.01860.0186wine0.35610.37840.42880.39870.3987glass0.11560.12460.12130.11760.1176mushroom0.14550.17590.16540.19350.1935adult0.00020.000310.000280.000310.00031haberman0.00060.00080.00080.00120.00125 accuracy clustering method observed referred data set mfq flcs scored better method majority data set rand index value closer case like seed wine glass mushroom fekm mckm better rand index value flcs mcq figure show accuracy different clustering method performed different datasets table clustering accuracy score k-means fekm mckm mcq flcs using rand index full size table table homogeneity score k-means fekm mckm mfq flcs full size table table completeness score k-means fekm mckm mfq flcs full size table table v-measure score k-means fekm mckm mfq flcs full size table figure clustering accuracy performed different datasets full size image next analysis made verify whether random selection center performed k-means generate stable accurate cluster using approach center initially chosen innovative way better visualization four datasets preferred purpose viz iris seed tae haberman whose accuracy score varies significantly method including k-means fekm mckm mfq flcs executed seven time taking four datasets input fig unpredictability k-means clearly observed every execution variable accuracy score fig due random initial center unpredictability formation cluster due varied accuracy score generated fig confirms case random center chosen accurately produce well-organized cluster whereas case random center precise create malicious subgroup conversely method like fekm mckm mfq flcs give stable predictable output execution figure rand index analysis stability check performed iris seed tae haberman datasets k-means fekm mckm flcs executed seven time full size image following parameter evaluation based determining homogeneity completeness cluster using v-measure table illustrates fact data set v-measure closer higher side mfq flcs implies cluster obtained different data set precise however using k-means effective since center randomly chosen may near optimal one figure present analysis flcs vs. k-means fekm mckm respectively based v-measure score different datasets graph indicates v-measure score flcs comparatively higher k-means fekm mckm suggests cluster obtained flcs homogenous complete figure v-measure score analysis different data set flcs vs. k-means flcs vs. fekm flcs vs. mckm full size image experiment conducted calculating time taken method determining initial cluster center convergence clustering loop algorithm executed machine 5th gen intel processor 1.9 ghz clock speed ram table represents actual execution time algorithm determining initial centroid table seen k-means obtain initial center earlier others selection done randomly average time taken selection cluster center method plotted fig horizontal line graph represents average time taken select initial center method graph observed fekm mfq take slightly time finding center rest mfq far take maximum complexity increase increase dimension data mckm flcs show similar performance yet flcs take time compute initial center mckm due linear time complexity table execution time determining initial center using k-means fekm mckm mfq flcs full size table figure average execution time deciding initial center using k-means fekm mckm mfq flcs full size image clustering loop convergence time i.e time taken cluster formation method given datasets noted table best performing method particular data set highlighted observed result flcs least convergence time five ten data set mfq performs similarly initial center fekm also smaller convergence time datasets average clustering loop convergence time method plotted fig horizontal line indicates average clustering loop convergence time method plot show k-means take time formation cluster due bad random initial center k-means method average line method decide subgroup formation average line flcs perform faster method due significantly distinct initial cluster obtained time table execution time clustering loop convergence k-means fekm mckm mfq flcs full size table figure average execution time k-means fekm mckm mfq flcs clustering loop convergence full size image finally overall execution time method employed given datasets recorded k-means fastest center selection random initial center result large convergence time table fig seen fekm mfq overall larger execution time others since much computation spent selecting near-optimal center hand mckm flcs show promising result table actual execution time k-means fekm mckm mfq flcs full size table figure average execution time k-means fekm mckm mfq flcs full size image performance accuracy score mfq flcs equal outcome method always due fact method obtain exactly initial center however process computing initial center totally different fig difference clearly observed flcs efficient mfq execution time conclusion initial centre cluster decisive factor final formation erroneous centroid may result malevolent clustering research approach suggested decide near optimal cluster center fekm proposed idea obtain well separated cluster quite effective datasets complexity issue higher side order get solution mckm suggested improved computational time extend still higher k-means randomly selects center reason fcgs mfq used selects center present convex hull thereby trim chance considering data point candidate form center however found clustering process effective datasets two three attribute due reason flcs suggested simple effective deciding center lesser complexity method thoroughly analyzed considering clustering effectiveness correctness homogeneity completeness complexity actual execution time convergence reason performance index like dbi index used correctness rand measure used homogeneity completeness v-measure used factor considered testing quality cluster formation showed excellent result proposed algorithm compared k-means clustering signifies emergence individual group data present within group remain closer proximity separation data one group another far zhou cheng investigated water depth bias correction bathymetric lidar point cloud data situation-aware iot service coordination iot service coordination using event-driven soa paradigm respectively quantifiable privacy preservation destination prediction spatio-temporal analysis trajectory data energy-efficient framework internet thing underlaying heterogeneous small cell network respectively examined ref peng bao liu recently examined community structure evolution opinion formation limited real-world training data robust online tensor completion iot streaming data recovery respectively liu worked federated neural architecture search medical science ref exhibit recent newly development iot field like next-generation wireless data center network fusion network transportation detection wireless sensor network energy harvesting relay work guided quite exciting innovative research opportunity explored viz time complexity computation time suggested method reduced concept convex hull still properly used obtain better cluster center versatile data set