introduction often science engineering measurement data dynamical process space time available first-principles mathematical model may numerical simulation method used predict system behavior situation particularly prevalent area biology medicine environmental science economy finance therefore much interest using machine-learning model data-driven forecasting space-time dynamic unknown governing equation recent advancement data-driven forecasting technique using artificial neural network include odil framework mesh-free variant using graph neural network addition feasibility data-driven forecasting chaotic dynamic demonstrated using neural network recurrent connection common goal approach learn discrete propagator observed dynamic propagator combination value finitely many discrete sampling point certain time explains value next time point t+\delta approach neither learn governing equation observed dynamic like sparse regression method pde-nets data-guided approximation solution known governing equation like physics-informed neural network instead learn discrete rule explain dynamic data make equation-free forecasting data feasible number application including prediction extrapolation coarse-graining de-noising usefulness data-driven forecasting method however hinge accuracy stability propagator learned ideally propagator fulfills constraint valid numerical scheme discretization used represent data learned propagator expected posse generalization power accomplished governing equation known explicit time-integration scheme moreover work solution- resolution-specific discretization nonlinear pdes shown convolutional neural network cnn filter able generalize larger spatial solution domain trained however over-complete set cnn filter used render training computationally expensive data-demanding remains challenge achieve data-efficient self-consistent forecasting general spatio-temporal dynamic unknown governing equation generalizes coarser grid order accelerate prediction present stencil-net architecture equation-free forecasting nonlinear and/or chaotic dynamic spatiotemporal data across different grid resolution stencil-net inspired work learning data-adaptive discretizations nonlinear pdes shown generalize coarser grid generalization ability derives inductive bias gained multi-layer perceptron mlp architecture combined known consistent time-stepping scheme runge-kutta method total variation diminishing tvd method regular cartesian grid straightforward represent spatial discretization neural network architecture stencil-net relies sliding small mlp input patch perform cascaded cross-channel parametric pooling enables learning complex feature dynamic illuminate mathematical relationship neural network architecture eno/weno finite difference connection classic numerical method constrains propagator learned stencil-net consistent average sense numerical analysis i.e. prediction error average decrease increasing spatial resolution increasing stencil size therefore stencil-nets used produce prediction beyond space time horizon trained chaotic dynamic coarser grid resolution trained show numerical experiment also show stencil-nets better convolutional neural network cnn based local nonlinear convolution fourier neural operator fno rely combination global fourier mode nonlinear convolution consequence accuracy stability stencil-nets extrapolate better coarser grid resolution computationally efficient training inference/simulation cnns fnos comparable number trainable parameter stencil-net consider following dynamic process discrete space time aligned u_i x_i i=1 n_x aligned u_i x_i t_j n_x\ data point space n_t\ time unknown parameter subset x_i x_j x_j s_m x_i 2m+1 data point within finite stencil support s_m\ radius around point x_i\ time grid resolution spatial discretization 2m+1 nonlinear discrete propagator data integrating side one time step yield discrete map x_i x_i t+\delta aligned u_i^ n+1 u_i^ t+\delta _m^ x_i aligned n+1 x_i t+\delta u_i^n x_i _m^n x_i x_j x_j s_m x_i approximating integral right-hand side quadrature find aligned u_i^ n+1 _m^n x_i t\bigg n=0 n_t aligned n_t\ total number time step _d\ explicit discrete time integrator time-step size due approximation integral quadrature discrete time integrator converges continuous-time map temporal convergence rate popular example explicit time-integration scheme include forward euler runge-kutta tvd runge-kutta method work consider runge-kutta-type method tvd variant stencil-net approximates discrete nonlinear function neural network leading aligned _i^ n+k _d^k _m^n x_i t\bigg aligned 2m+1 nonlinear network layer weight superscript _d^k\ denotes discrete propagator map time step future assuming point-wise uncorrelated noise data i.e. v_i^n u_i^n _i^n\ extended mapping noisy data v_i^ v_i^ n+k aligned _i^ n+k _d^ _i^n _i^n _m^n x_i _m^n x_i t\big _i^ n+k aligned _m^n x_i x_j x_j s_m x_i given noisy data _m^n x_i x_j x_j s_m x_i noise estimate stencil s_m\ centered point x_i\ time neural network architecture figure stencil-net architecture equation-free forecasting data mlpconv unit performs parametric pooling moving small stencil size s_m\ mlp network across input vector ^n\ time generate feature map feature reaching output layer stencil weight discrete propagator runge-kutta time integrator used evolve network output time step forward backward time used compute loss full size image stencil-net single mlp convolutional mlpconv unit n_l\ fully-connected hidden layer inside represent discrete propagator discrete runge-kutta time integrator _d^ result architecture shown fig sliding mlpconv unit input state-variable vector inference map input local stencil patch discretization feature computation thus performed mlpconv unit aligned _1\right n_l n_l n_l-1 n_l aligned q=1,2 n_l trainable weight bias respectively nonlinear activation function usual choice activation function include sigmoid relu sliding mlpconv unit across input vector amount cascaded cross-channel parametric pooling cnn layer allows trainable interaction across channel better abstraction input data across multiple resolution level contrast conventional cnn higher-level abstraction achieved over-complete set filter cost requiring training data incurring extra workload downstream layer network composing feature therefore provide direct comparison cnn architecture feature map generated convolving input followed nonlinear activation i.e. aligned _k^m aligned _k^m\ circulant dense matrix represents convolution depends size filter 2m+1 _k\ bias term comparison also benchmark mlpconv stencil-net architecture global operator-learning method fourier neural operator fno fno computation aligned n_l n_l n_l-1 n_l n_\ell n_l-1 aligned q=1,2 n_l trainable weight bias nonlinear activation function operator forward inverse fourier transforms respectively loss function training derive loss function learning local nonlinear discrete propagator aligned mse n=1 n_t i=1 n_x k=-q n+k _d^ v_i^n _m^n t\big _i^ n+k aligned loss compare forward backward prediction _d^ stencil-net training data n+k computed detailed algorithm positive integer number runge-kutta integration step considered optimization refer training time horizon scalar _k\ exponentially decaying training time horizon weight account accumulating prediction error treat noise estimate _i\ latent variable enable stencil-net separate dynamic noise case initialize estimate obtained tikonov smoothing training data initialization step algorithm higher-dimensional problem however learning noise latent variable becomes infeasible instead directly learn smooth time propagator noisy data demonstrated section stencil-net learning smooth dynamic noisy data penalize noise estimate _m^n order avoid learning trivial solution _i^ n+k _i^ _i^ n+k minimization problem _\theta noise account data also impose penalty weight network i=1,2 n_l order prevent over-fitting total loss becomes aligned loss mse _f^2 i=1 n_l _f^2 aligned n_l\ number layer single mlpconv unit n_x n_t matrix point-wise noise estimate _f\ frobenius norm matrix perform grid search hyper-parameter space order identify value penalty parameter find choice 10^ 10^ work well problem considered paper alternatively method like drop-out early-stopping criterion jacobi regularization used counter over-fitting problem data-augmentation strategy unroll training data forward backward time evaluating loss numerical experiment shown confirm training mode average lead stable accurate model trained solely forward time regardless inference done forward time training done full-batch mode using adam optimizer learning rate 0.001\ activation function use exponential linear unit elu smooth function interpolation ability one could readily use relu leaky-relu adaptive activation function learnable hyper-parameters find elu consistently performs best benchmark problem considered complete training procedure summarized algorithm generate similar cnn architecture comparison replace mlpconv unit fig local nonlinear convolution map similarly comparison fno replace mlpconv unit feature map order approximate discrete operator convolution frequency space forecasting accuracy classic numerical analysis stability accuracy connected consistency via lax equivalence theorem play central role determining validity discretization scheme data-driven setting learning propagator value grid node time later time t+\delta assumes discrete stencil propagator valid numerical discretization unknown ground-truth dynamical system section therefore rationalize choice network architecture algebraic parallel known grid-based discretization scheme specifically analyze approximation property stencil-net architecture argue accuracy analogy well-known class solution-adaptive eno/weno finite-difference scheme show weno scheme involve reciprocal function absolute value switch statement rational function stencil value figure approximation property sharply varying function best rational polynomial mlp fit spike function 0.5\ 0.25\ 0.1\ rational function fit newman polynomial approximate x\vert\ full size image mlps particularly effective representing rational function consequence mlps efficiently represent weno-like stencil approximating nonlinear discrete spatial propagator _d\ illustrate compare best polynomial rational mlp fit spike function fig rational mlp approximation closely follow true spike black solid line whereas polynomial fail capture switch statement i.e. division operation quintessential resolving sharp function figure numerical solution advection sharp pulse red dashed line using different discretization scheme data-adaptive fifth-order weno stencil second- fourth-order central finite difference first- third-order upwinding scheme stencil-net 4\times\ coarser grid compared fifth-order weno grid plot advection velocity c=2\ one-dimensional advection equation u_t cu_x plot time t=3\ full size image also explains result shown fig weno stencil-net i.e. mlp method accurately resolve advection sharp pulse remarkably stencil-net advect pulse 4\times\ coarser grid weno scheme able fig polynomial approximation using central-difference fixed-stencil upwinding scheme fail faithfully forecast dynamic based empirical evidence posit relationship neural network architecture used known class numerical method beneficial equation-free forecasting particular presence sharp gradient multi-scale feature therefore next analyze numerical-method equivalence stencil-net architecture relation finite-difference stencil spatial derivative location x_i\ grid spacing approximated convergence order using linear convolution aligned x=x_i x_j s_m x_i u_j x^r aligned u_j x_j _j\ stencil weight determined local polynomial interpolation stencil radius s_m x_i i-m i-m+1 x_i i+m-1 i+m size s_m 2m+1\ spatial domain size spacing l/n_ n_x\ number grid point discretizing space following proposition define discrete moment condition need fulfilled order stencil consistent linear quadratic operator respectively proposition 3.1 nonlinear convolution nonlinear term order including product derivative e.g. u_x u^2 u_x approximated nonlinear convolution l_1 l_2 r_1 r_2 write aligned l_1 l_1 l_2 l_2 x=x_i x_i x_i u_j u_k r_1 r_2 aligned stencil size s_m l_1 r_1 1\vert\ s_m l_2 r_2 1\vert\ convolution volterra quadratic form described ref linear convolution nonlinear convolution based local polynomial interpolation thus useful discretize smooth solution arising problem like reaction-diffusion fluid flow low reynolds number elliptic pdes fail discretize nonlinear flux term arising e.g. problem like advection-dominated flow euler equation multi-phase flow level-set hamilton-jacobi equation higher-order interpolation near sharp gradient lead oscillation decay refining grid see fig fact known gibbs phenomenon moreover fixed stencil weight computed moment condition fail capture direction information flow data upwinding leading non-causal hence unstable prediction relaxed biasing stencil along direction information flow constructing weight smooth flux-splitting method like godunov lax-friedrichs scheme counter issue spurious oscillation artificial viscosity also added cost lower solution accuracy alternatively data-adaptive stencil eno essentially non-oscillatory weno weighted eno weight available seems correct choice data-driven forecasting since adhere moment condition also adapt data figure illustration data-adaptive stencil schematic stencil s_3 x_i centered around point x_i\ x_i s_3^ s_3^ full size image eno/weno method discretely approximating continuous function point 1/2 written linear convolution aligned i\pm 1/2 x_j x_i _j^ x_j x^r aligned i\pm 1/2 x_i x/2 function value coefficient stencil stored x_j x_j s_m x_i x_j respectively stencil s_m^ x_i s_m^ x_i s_m x_i i\pm illustrated fig example m=3\ unlike fixed-weight convolution coefficient eno/weno stencil computed based local smoothness feature approximated smaller sub-stencils turn depend function value lead locally data-adaptive stencil weight allows accurate consistent approximation even data u_i\ highly varying key idea behind data-adaptive stencil use nonlinear map choose locally smoothest stencil discontinuity avoided result smooth essentially non-oscillatory solution weno-approximated function value i\pm 1/2 used approximate spatial variation data follows aligned x=x_i i+1/2 i-1/2 o\left x^r\right aligned finite-difference method function polynomial flux e.g c_1 u^2 c_2 whereas finite-volume method function grid-cell average x-\delta x/2 x+\delta x/2 clear polynomial i.e. nonlinear convolution approximate division operation thus method using fixed data-independent stencil weight multiplication filter fail approximate dynamic sharp variation however shown fig achieved rational function straightforward see eno/weno stencil rational function stencil value _m\ indeed stencil weight computed x_i x_i polynomial 2m+1 strictly positive polynomial 2m+1 summary depending smoothness data u_i\ propagator either approximated using fixed stencil polynomial stencil value using solution-adaptive stencil rational function stencil value stencil-net represent either lead conclusion continuous dynamic smooth approximated polynomial rational function local stencil value x_i desired order accuracy cartesian grid resolution thus aligned x_i x^r ~~for~~ x_i x_j x_j s_m x_i aligned discrete propagator numerical experiment apply stencil-net learning stable accurate equation-free forecasting operator variety nonlinear dynamic problem discussed use single mlpconv unit n_l=3\ hidden layer counting input output layer exponential linear unit elu activation function input network data _m\ stencil radius m=3\ example present numerical experiment demonstrate distinct application stencil-net architecture using data deterministic dynamic data chaotic dynamic noisy data stencil-net equation-free forecasting first demonstrate capability stencil-net extrapolate space-time dynamic beyond training domain different parameter regime consider forced burger equation one spatial dimension time forced burger equation nonlinear forcing term produce rich sharply varying solution moreover choose forcing term random order explore generalization different part solution manifold forced burger equation aligned u^2 x^2 aligned unknown function diffusion constant d=0.02\ use forcing term aligned i=1 a_i 2\pi l_i x/l aligned parameter drawn independently uniformly random respective range -0.1,0.1 -0.4,0.4 2\pi n=20\ domain size set 2\pi\ i.e. x\in 0,2\pi periodic boundary condition l_i 2,3,4,5\ use smooth initial condition t=0 x-3 generate data n_x 256\ evenly spaced grid point fifth-order weno discretization convection term second-order central difference diffusion term time integration performed using third-order runge-kutta method time-step size chosen large possible according courant-friedrichs-lewy cfl condition equation larger domain adjust range l_i\ preserve wavenumber spectrum dynamic e.g. 8\pi\ use l_i 8,9 ,40\ use spatial resolution domain size i.e. total number grid point n_x\ grows proportionally domain size train stencil-net different spatial resolution x_c c\delta sub-sampling factor use sub-sampling factor 2,4,8\ space sub-sample data simply removing intermediate grid point example c=2\ keep spatial grid point even index training time integration done step size satisfies cfl condition sub-sampled mesh i.e. t_c x_c ^2/d\ x_c c\delta t_c\ training space time resolution respectively training done full spatial domain data time independent resolution test well stencil-net able forecast solution time due steep gradient rapidly varying dynamic burger equation present challenging problem testing stencil-net generalization capability figure comparison stencil-net 4-fold coarsened fifth-order weno forced burger comparison stencil-net prediction weno data entire domain time 40\ i.e. end training time horizon comparison nonlinear discrete operator learned stencil-net ground-truth weno scheme time t=40\ full size image figure compare stencil-net prediction four-fold downsampled grid i.e. c=4\ end training time interval training full-resolution training data figure compare learned nonlinear discrete propagator _\theta\ stencil-net true discrete _d\ simulation generated data thanks good match propagator stencil-net able generalize parameter domain-sizes beyond training condition indeed observe fig compare fifth-order accurate weno data fine grid l/n_x n_x l=2\pi stencil-net prediction coarsened grid x_c c\delta sub-sampling factor 2,4,8\ longer time stencil-net produce stable accurate forecast coarser grid time beyond training data dashed black box point-wise absolute prediction error right column grows leaving training time domain doe explode concentrated around steep gradient solution expected figure forced burger forecasting stencil-net coarser grid left fifth-order weno ground-truth data spatial resolution n_x 256\ right power spectrum prediction different architecture stencil-net cnn fno compared ground-truth left column output stencil-net 2\times\ 4\times\ 8\times\ coarser grid dashed box contain data used training resolution right column point-wise absolute error stencil-net prediction compared ground-truth data beyond training domain full size image compare inference accuracy computational performance mlpconv-based stencil-net cnn operator-learning architecture fno cnn relies local nonlinear convolution whereas fno learns continuous operator using combination nonlinear convolution global fourier mode previously proposed dynamic forecasting data comparison use cnn hidden layer filter filter radius m=3\ match stencil radius stencil-net fno use hidden layer local convolutional filter per layer rule spectral bias verifying fully trained network represent fourier spectrum ground-truth different sub-samplings tested result fig show three neural-network approximation able capture power spectrum true signal next compare mean-square error network prediction different sub-sampling result table show fno performs best low sub-sampling stencil-net best generalization power coarse grid consistent expectation fno achieves superior accuracy trained sufficient data however fno prediction become increasingly unstable inference high sub-sampling ﻿table prediction mean square error mse comparison stencil-net convolutional neural network cnn fourier neural operator fno comparable size i.e. number trainable parameter different sub-sampling forced burger test case training 10,000 epoch q=2\ full size table contrast generative network proposed ref used time frame training parametric pooling local stencil patch enables stencil-net consistently extrapolate also larger spatial domain shown fig furthermore shown fig stencil-net able generalize forcing term i.e. dynamic different one used generate training data seen point-wise error architecture perform well within training window also largest error always occurs near steep gradient jump solution expected fno however develops spurious oscillation long prediction interval figure stencil-net extrapolation larger spatial domain longer time stencil-net prediction 4\times\ coarser grid comparison ground-truth discrete propagator _d\ solid stencil-net layer output dashed time within training data past training data marked dashed vertical line stencil-net trained data within domain marked solid rectangle full size image figure stencil-net generalization different forcing term without re-training panel corresponds forced burger dynamic different forcing term see every second row panel correspond point-wise error associated stencil-net cnn fno left right stencil-net trained using data training domain marked dashed box nevertheless able accurately predict qualitatively different dynamic forcing term since learned discrete propagator dynamic rather solution value plot first column show ground-truth data different forcing term obtained fifth-order weno scheme three subsequent column show prediction 4\times\ coarser grid without additional re- training corresponding absolute error w.r.t weno plot three column compare stencil-net cnn fno comparable complexity see table full size image addition improved generalization power stencil-net also computationally efficient cnn fno approach confirmed timing reported table training inference/simulation respectively time measured nvidia a100-sxm4 gpu network comparable number trainable parameter table top average epoch training time per epoch compared stencil-net cnn fno table 4\times\ spatial sub-sampling different training time horizon bottom average time step simulation inference time per time-step forced burger test case different sub-sampling best performance highlighted bold measured nvidia a100-sxm4 gpu full size table finally analyze influence choice stencil-net architecture result figure show effect choice mlp size weight regularization parameter see different training time horizon figure compare accuracy prediction grid varying resolution different size space time domain respectively training always done l=2\pi\ t=40\ figure also notice prediction error average decrease increasing stencil complexity fig increasing mesh resolution fig empirical evidence stencil-net learns consistent numerical discretization underlying dynamical system figure architecture choice choice generalization power model trained spatio-temporal data contained dashed box fig training box encompasses entire spatial domain length l=2\pi\ time duration t=40\ stencil-net prediction accuracy measured mean squared error mse w.r.t ground truth different network architecture hidden layer counted input output layer data point corresponds average mse accumulated stable seed configuration effect weight regularization parameter see prediction accuracy different training time horizon plot produced 3-layer 64-nodes network architecture showed best performance used also experiment paper plot illustrate generalization power trained stencil-net larger domain data point show mse prediction error best model run grid respective resolution sub-sample factor different domain length final time training always done data l=2\pi\ t=40\ full size image summary find stencil-net model trained small training domain generalize well longer time larger domain dynamic containing different forcing term numerically consistent way highlight importance using network architecture mathematically relates numerically valid discrete operator stencil-net autonomous predictor chaotic dynamic next analyze well generalization power stencil-net forecasting transfer inherently chaotic dynamic nonlinear dynamical system like kuramoto-sivashinsky model describe chaotic spatio-temporal dynamic equation exhibit varying level chaos depending bifurcation parameter domain size system given high sensitivity numerical error equation usually solved using spectral method however data-driven model using recurrent neural network rnns also shown success predicting chaotic system mainly rnns able capture long-term temporal correlation implicitly identify required embedding forecasting figure equation-free forecasting chaotic kuramoto-sivashinsky spatio-temporal dynamic top spectral solution equation domain length l=64\ system behaves chaotically data within dashed rectangle used training stencil-net model middle stencil-net forecast 4\times\ coarser grid 4\times\ longer time bottom point-wise absolute difference ground-truth data top row stencil-net forecast middle row full size image figure statistical characteristic chaotic system left comparison power spectral density psd ground-truth solution stencil-net prediction 4\times\ coarser grid right growth distance nearby trajectory stencil-net prediction characterizing maximum lyaponuv exponent slope dashed red line compared ground truth value 0.084\ full size image challenge result using stencil-net long-term stable prediction chaotic dynamic training data obtained spectral solution equation domain size l=64\ equation unknown function aligned u^2 x^2 +\frac x^4 aligned domain x\in -32,32 length l=64\ periodic boundary condition discretized n_x 256\ evenly spaced spatial grid point use initial condition aligned t=0 i=1 a_i 2\pi l_i x/l aligned parameter drawn independently uniformly random respective range -0.5,0.5 0,2\pi l_i 1,2,3\ use spectral method numerically solve equation using chebfun package time integration performed using modified exponential time-differencing fourth-order runge-kutta method step size 0.05\ stencil-net prediction use grid sub-sampling factor c=4\ space i.e. n_c=64\ train data time 12.5 prediction time-step size chosen large possible satisfy cfl condition spatio-temporal dynamic data chaotic system shown fig stencil-net prediction 4\times\ coarser grid diverge true data time see bottom row fig due chaotic behavior dynamic domain length 22\ cause small prediction error grow exponentially rate proportional maximum lyaponuv exponent system despite fundamental unpredictability actual space-time value stencil-net able correctly predict value maximum lyaponuv exponent spectral statistic system see fig evidence equation-free stencil-net forecast consistent correctly learned intrinsic ergodic property dynamic generated data fig show stencil-net forecast 4\times\ coarser grid different initial condition obtained different random seed longer time 8\times\ trained show statistically consistent propagator learned stencil-net also run autonomous predictor chaotic dynamic beyond training domain figure autonomous prediction chaotic spatio-temporal dynamic stencil-net run autonomous predictor long-term chaotic dynamic different initial condition longer time trained training data dashed rectangle fig 4\times\ coarser grid full size image stencil-net learning smooth dynamic noisy data discrete time-stepping constraint force stencil-net prediction follow smooth time trajectory property exploited filtering true dynamic noise demonstrate de-noising capability using numerical solution data korteweg-de vries kdv equation artificially added noise kdv equation unknown function aligned u^2 x^3 aligned use 0.0025\ use spectral method generate numerical data kdv equation using chebfun package domain -1,1 periodic boundary condition initial condition t=0 spectral solution represented n_x 256\ equally spaced grid point discretizing spatial domain figure stencil-net learning smooth dynamic noisy data korteweg-de vries data point-wise data-dependent additive gaussian noise =0.1\ used training stencil-net prediction training 10,000 epoch 4\times\ coarser grid using data time 1.0 dashed vertical line point-wise absolute error stencil-net prediction noise-free ground-truth kdv dynamic full size image corrupt data vector x_i t_j n_x n_t element-wise independent additive gaussian noise aligned aligned element std normal distribution mean variance std empirical standard deviation data rendering noise data-dependent parameter magnitude noise use =0.1\ train stencil-net entire space-time extent noisy data -fold sub-sampling space training time-step size 0.02\ use third-order tvd runge-kutta method time integration final time t=1\ training configuration stencil-net able learn stable accurate propagator discretized kdv dynamic noisy data enabling separate data noise shown fig also noisy case compare stencil-net cnn fno architecture burger case without noise result table show low sub-sampling factor cnn better predicts ground-truth signal stencil-net stencil-net generalizes significantly better coarser grid however unable train stable stencil-net parameter used 4\times\ 8\times\ downsampling prediction 2\times\ downsampling difference parameterization could cause spectral property neural network differ error estimate compared across resolution case like forced burger case stencil-net computationally efficient cnn fno table taken together example show potentially powerful capability stencil-net extract consistent dynamic noisy data table prediction mean square error mse comparison korteweg-de vries kdv test case different spatial sub-sampling q=4\ 10,000 epoch training full size table table top average epoch training time per epoch compared stencil-net cnn fno table 4\times\ spatial sub-sampling different training time horizon bottom average time step simulation/inference time per time-step kdv test case different sub-sampling best performance highlighted bold measured nvidia a100-sxm4 gpu full size table conclusion presented stencil-net architecture equation-free forecasting data stencil-net patch-wise parametric pooling discrete time integration constraint learn propagator discrete dynamic multiple resolution level design stencil-net architecture rest formal connection mlp convolutional layer rational function solution-adaptive weno finite-difference scheme render stencil-net approximation valid numerical discretizations latent nonlinear dynamic accuracy prediction also translates better generalization power extrapolation stability coarser resolution convolutional neural network cnns fourier neural operator fnos computationally efficient training inference/simulation spectral analysis also found neural architecture capture power spectrum true dynamic discounting effect spectral bias checking consistency thus shown stencil-net used fast accurate forecaster nonlinear dynamic model-free autonomous prediction chaotic dynamic detecting latent dynamic noisy data stencil-net provides general template learning representation conditioned discretized numerical operator space time combine expression power neural network inductive bias enforced numerical time-stepping leveraging two stable accurate data-driven forecasting beyond fast accurate extrapolator surrogate model achieving three four order magnitude speedup traditional numerical solver case governing equation known stencil-net repurposed learn closure correction computational fluid dynamic active material model along line stencil-net also repurposed learn correction coarse-grained discretizations using existing numerical method e.g. spectral method finite-volume method hybrid machine-learning/simulation workflow finally since stencil-net able directly operate noisy data shown also used decompose spatiotemporal dynamic propagator-free noise application area biology neuroscience finance physical model true dynamic may available future work includes extension stencil-net architecture problem time delayed coordinate inferring latent variable addition combined learning local global stencil could explored interest reproducibility publish gpu multi-core cpu python implementation stencil-net also make trained model raw training data available user available http