introduction gaussian process prominent member larger family stochastic process provides powerful flexible framework stochastic function approximation form gaussian process regression gpr characterized gaussian probability distribution function space =\sum _i^n~\alpha ~\forall kernel function set hyperparameters mean covariance gaussian probability distribution learned constrained function optimization data y_i\ conditioned observation y_i\ yield posterior probability density function throughout paper refer optimization often training learning emphasize link machine learning gps assume model form ~=~ +\epsilon unknown latent function noisy function evaluation measurement noise term element input space index set learning hyperparameters subsequent conditioning lead stochastic representation model function used decision-making visualization interpretation paper deal example regression gpr proposed methodology easily applied gp-related task therefore simply use more-general acronym throughout paper comparison neural network gps scale better dimensionality input space—since number weight parameter doe depend it—and provide exact function approximation additionally gps provide highly-coveted bayesian uncertainty quantification top function approximation neural-network-based method estimate error often result rigorous bayesian uncertainty quantification even gps come one difficult-to-circumvent problem due unfavorable scaling n^3 computation n^2 storage applicability gps largely limited small moderate dataset size prevents method used many field large datasets common major disadvantage compared method e.g. neural network field include many machine learning application earth environmental climate material science engineering numerical complexity gps stem need store invert typically-dense covariance matrix direct inversion replaced iterative linear system solves speedup rather modest dense covariance matrix method alleviate scaling issue exist largely based approximation workarounds fall broad category set local expert dataset divided subset serf input separate gps resulting posterior combined also interpreted one large sparse block-diagonal covariance matrix common divide dataset locality leading name local expert inducing-points method instead inducing sparse covariance matrix picking subset dataset inducing-points method place new point inside domain inducing favorable data structure translates sparsity function value point calculated via standard interpolation technique popular example approach include kiss-gp predictive process fixed-rank kriging generally inducing-points method agnostic kernel definition therefore limit kernel used limitation major drawback given recent application increasingly using flexible non-stationary kernel function instance generally incompatible inducing-points method structure-exploiting method method special kind inducing-points method place pseudo-points grid covariance matrix toeplitz algebra lead fast linear algebra needed train condition success method agnostic kernel definition vecchia approximation instead calculating full conditional probability density function prior vecchia approximation used pick subset data condition method also kernel-dependent largely applied using stationary kernel statistic literature contains variety related approach see recent summary traditional state-of-the-art approach direct comparison method common dataset another outstanding review existing method introduced one thing common sparsity exploitable structure covariance matrix introduced operating data points—either considering subset full dataset utilizing representative pseudo-points allow favorable structure e.g toeplitz covariance sparsity commonality lead one major issue existing method approximation exact gps lead poor prediction performance highly non-linear functions—i.e function exhibiting large first second-order derivative frequently changing sign high-fidelity approximation number sub-selected data point pseudo point must approach size original dataset eliminates method advantage fundamentally sparsity structure covariance matrix dictated nature problem data computational constraint lead consider kernel take advantage naturally occurring—problem data dictated—sparsity instead operating input points—by selecting subset pseudo-points—an alternative approach let kernel find expressive sparse structure covariance matrix principle flexible kernel could discover—not induce—naturally occurring sparse structure covariance matrix without acting data point case approximation taking place compared inducing-points local-experts vecchia method ad-hoc point selection required additionally shall see restriction used problem-specific kernel function long combined proposed sparsity-enabling therefore sparsity-discovering kernel added advantage kernel-discovered sparsity entirely independent spatial relationship data point meaning distant data point discovered high covariance point close proximity might independent ad-hoc dependency covariance euclidean point distance —in contrast local expert instance outline creating exact learns utilizes naturally-occurring sparsity shall require three main building block ultra-flexible non-stationary compactly-supported kernel function specially customized learn encode zero-covariances high-performing implementation compute sub-matrices covariance matrix distributed parallel fashion constrained augmented optimization routine ensure learned covariance matrix sparse least enforce preference sparsity last point important large problem protect computing system over-utilization extreme case naturally-occurring sparsity insufficient non-existent sparsity-inducing optimization routine would seamlessly result optimal approximate contribution paper summarized follows show combining tailored kernel design hpc implementation constrained optimization exact gps scaled datasets size assumption naturally-occurring sparsity core idea allows scaling sparsity-discovering kernel design optimization learns data point correlated independent respective locality input space sparse structure artificially induced state-of-the-art method instead allow discover natural sparsity dataset principle visualized figure reference included table comparing contrasting different existing method proposed method see figure in-depth quantitative comparison proposed method versus existing state-of-the-art approach would illuminating argue exercise beyond scope manuscript due fact performance depends variety subjective choice data application kernel function computing architecture prior mean function among thing figure table comparing existing approximate method large-scale gaussian process proposed method proposed method one restriction dimensionality input space kernel design data-point geometry full size image figure figure illustrating premise proposed algorithm panel show test data measured daily maximum temperature ^\circ\ april 10th across united state n=4718\ problem size still well within capability standard whose posterior mean shown employ flexible non-stationary compactly-supported kernel learn optimization marginal log-likelihood covariance essence prediction sparse result shown panel show covariance matrix dense sparse respectively sparse covariance 1.5 non-zero entry full dense matrix sparsity problem discovered induced leading exact principle combination hpc truly large covariance matrix constrained function optimization enables gps scaled ten million data point full size image contribution paper glance propose new non-stationary flexible compactly-supported kernel design allows gaussian process discover sparsity show use new kernel design concert distributed computing scale gps million data point draw attention hyperparameter optimization process solution allow sparsity preferred method basic gaussian process characterized gaussian probability density function function value aligned =\frac 2\pi dim -\frac -\textbf -\textbf aligned gaussian likelihood aligned =\frac 2\pi dim -\frac -\textbf -\textbf aligned observation-noise matrix often diagonal covariance matrix defined kernel function ~=~k prior-mean vector training done maximizing marginal log-likelihood ignoring additive constant aligned =-\frac -\textbf -\textbf aligned respect hyperparameters hyperparameters found posterior defined aligned _0|\textbf =~\int r^n _0|\textbf ~d\textbf ~\propto +\pmb ^t~ +\textbf ~\left -\textbf _0\right ^t~\left +\textbf ~\pmb aligned basic framework extended ever-more flexible mean noise kernel function proposed method entirely agnostic even symbiotic—in sense mutual support—to extension therefore omit dependency thereof bottleneck training gps optimizing respect n^3 numerical complexity calculating —or equivalently solving linear system—and storage scale n^2 however sparse problem would avoided goal approximate method work synthetically inducing sparsity contrast approximate technique propose achieve sparsity purely flexible kernel design approximation leading sparse exact sparsity case discovered induced however problem doe natural sparsity constrained optimization described used optimize shall guarantee minimal approximation needed satisfy system-dictated minimum-sparsity constraint consider simple example squared exponential kernel aligned _s^2~\exp -0.5~||\textbf _2||^2/l^2\right aligned used approximately application even data point naturally uncorrelated squared exponential kernel would able learn independence global support always return covariance true commonly used stationary kernel non-stationary kernel instead formulating kernel learn well point dependent propose consider kernel tailored capable learning independence non-stationary flexible compactly-supported kernel first building block proposed framework even kernel defined covariance matrix still computed stored time-consuming often prohibitive due storage requirement distributed computing hpc compute architecture—as second building block—can help splitting computational storage burden third building block augmented constrained optimization guarantee sparse solution given preference even requirement building block non-stationary ultra-flexible compactly-supported kernel function natural sparsity discovered kernel function designed flexibly encode correlation data point including instance correlation exist kernel meet three requirement compact support obvious necessary property since attempting discover zero covariance kernel compactly supported non-stationarity compactly-supported kernel used mostly stationary case however stationary case sparsity taken advantage entirely local way—i.e point happens far away point covariance zero kernel able learn complicated distance-unrelated sparsity-exploiting dependency flexibility pick sparsity across geometry distance kernel flexible recognize neighboring point may correlated point distance vice versa combining compact support non-stationarity flexibility yield kernel tailored learn existing non-existing covariance examine example solidify idea kernel aligned k_s\left _2\right =\tilde _2\right _1\right _2\right aligned rather well-known example non-stationary kernel subscript stand sparsity since kernel allow later discover sparsity kernel assumed compactly-supported stationary instance non-stationarity produced term _1\right _2\right flexibility kernel depends entirely parameterization flexible case could sum kronecker- function centered subset data point _i\right\ i=1 ^n\ i.e. aligned h_i _i\right aligned binary coefficient h_i 1\right\ hyperparameters may optimized training allowed include data point would obtain learned point safely ignored h_i without impacting marginal log-likelihood kernel flexible two issue first explicitly depends potentially million binary hyperparameters second unable encode varying covariance data point point either turned even flexible kernel fact turn selected covariance instead point defined aligned k_s\left _2\right =\tilde _2\right ~\left g_1 _1\right g_1 _2\right g_2 _1\right g_2 _2\right aligned aligned g_1 g_1 _i\right aligned aligned g_2 g_2 _i\right aligned g_1 _i\ g_2 1\right\ kernel effectively discover certain covariance perhaps zero include data point kernel hyperparameters optimize overwhelming optimization large alleviate challenge large number hyperparameters trade flexibility therefore sparsity parameterization fewer hyperparameters purpose propose kernel function aligned k_s\left _2\right =\tilde _2\right _i^ n_1 _i\left _1\right _i\left _2\right aligned aligned j=1 n_2 -\beta 1-\frac -\textbf _0^ -\textbf _0^ aligned equation sum so-called bump function indicator function -\textbf _0^ otherwise _0^ bump function location radius shape parameter bump function ~c^ compactly supported precisely property need create sparsity-discovering kernel function kernel function allows seamlessly choose flexibility directly impact ability discover sparsity number hyperparameters compare n_1=2\ n_2=1\ see fig visualization kernel test combine kernel compactly-supported stationary kernel given aligned _2\right array ^2\right 1+\sqrt ^2+1\right 1-\left else array aligned d=||\textbf _1-\textbf _2||_ radius support kernel function rather well-known compactly-supported stationary kernel since kernel multiplied combine sparsity-discovering kernel k_s\ kernel k_c~\cdot ~k_s\ leading restriction core kernel k_c\ bump function k_s\ normalized shaped order equal one within support zero otherwise understood mask leaf core kernel k_c\ untouched area support since bump function appears kernel shape lead positive semi-definiteness kernel kernel k_s\ also give opportunity estimate sparsity covariance matrix limit adding infinitely many uniformly distributed data point inside fixed domain discrete covariance matrix becomes covariance operator kernel number non-zero entry becomes integral case sparsity covariance matrix bounded aligned s=\frac number non-zero covariance n^2 s_k ~d\textbf d\textbf ~d\textbf d\textbf aligned use formulate objective function allow give preference sparse solution formulate sparsity constraint note small mean high sparsity s_k\ set support kernel i.e. s_k case cartesian product two ball —the volume cartesian-product set two ball embedded ^n\ _2\ product respective volume therefore kernel sparsity bounded above—assuming entirely disjoint support since overlap increase sparsity lower —so aligned s_k ~d\textbf d\textbf =\sum _i^ n_1 _j^ n_2 _k^ n_2 vol_s\left dim vol_s\left dim aligned vol_s dim volume dim -dimensional sphere radius defined aligned vol_s dim =\frac dim/2 dim 1\right dim aligned gamma function dim dimensionality kernel effect k_c\ region support vanishes see fig example therefore seen shape parameter bump function become delta function kernel obtained figure figure showing flexible non-stationary compactly-supported kernel function panel k_s\ sparsified squared-exponential core kernel k_c\ panel one-dimensional input domain visualize kernel function kernel set compactly-supported bump function naturally discover sparsity optimization bump function position height radius shape since multiplication kernel valid kernel sparsity-discovering kernel k_s\ panel combined kernel therefore compared approximate method doe limit user ability design employ arbitrary kernel function panel show concept kernel k_s\ support covariance function becomes squared-exponential kernel full size image building block high-performance computing take advantage sparse kernel flexible non-stationary compactly-supported kernel core building block algorithm extreme-scale gps covariance matrix computed dense format first take full advantage multi-threading however could violate ram restriction large datasets computing covariance matrix sparse format place would prohibitively inefficient avoid slow computation going beyond ram limit define host covariance matrix one host machine sparse first place compute dense sub-matrices distributed way cast sparse format communicate sparse sub-matrices back host machine inserted host covariance matrix strategy address ram limitation distributing covariance matrix across many computing resources—and could even exploit out-of-core methodology utilizing disk storage needed additionally computation time sped leveraging heterogeneous architecture gpus efficient data-parallel operation threading-task-parallel cpu operation combination distributing memory exploiting parallelism across core allows algorithm operate datasets practically-unlimited size—given enough distributed worker sufficient natural sparsity procedure illustrated fig shown pseudo-code split dataset length batch size large way covariance matrix computed distributing computational burden dividing host covariance matrix block sub-matrices representative unique data-batch pair see fig batch pair transmitted different worker often per node via python library dask denote number parallel-executed task one task per worker task exact batch-covariance computed specifically-designed kernel many element sub-matrix zero way theoretically any-size covariance matrix computed stored distributed way sub-matrices transferred back translated sparse representation injected sparse host covariance matrix host machine matrix size sparsity avoids problem storing computation computation batch covariance matrix accelerated taking advantage many parallel thread gpu cpu offer future work compare compute performance different implementation architecture proposed algorithm given resource able compute solution faster exhibiting strong scaling property inherent design see fig furthermore problem size increase algorithm match set resource also highlighting weak scaling summary formulation speed computation reduces memory burden provides ability exploit heterogeneous architecture cpus/gpus/tpus providing future compatibility proposed framework since future architecture leveraged theoretical computing time covariance matrix calculated aligned t_c=\frac 2nb +1\right ~t_b aligned t_b\ compute time one sub-matrix whose scaling depends exact implementation availability number parallel cpu gpu thread equation suggests number task number parallel worker approach scaling becomes linear i.e complexity extension number worker approach total batch number scaling becomes constant linear-system solution accomplished conjugate gradient method numerical complexity o\left m\sqrt number non-zero entry covariance matrix condition number log-determinant computation done via cholesky factorization whose scaling depends exact structure matrix furthermore since intent purpose approximate aligned t_c t_b 2nb^2 aligned help estimate optimal batch size given particular architecture sequential computation t_b\ scale o\left b^2\right batch size drop equation extreme perfect parallelization t_b\ scale therefore want maximize batch size point linear scaling stop number depends particular architecture figure figure illustrating computational building block proposed algorithm dataset divided batch panel pair batch sent compute node associated sub-matrices covariance matrix calculated using presented sparse kernel panel sub-matrices cast sparse format compute node sent back host get assembled obtain full sparse master covariance matrix panel subsequent mathematical operation needed namely calculating log-determinant solving linear system performed efficiently sparse covariance matrix full size image figure figure illustrating theoretical measured strong weak scaling distributed covariance computation computation time problem fixed size function number worker computation time function number worker problem dataset size increased left 2e5 4e5 8e5 16e5 also see label figure suggest strong case made favorable scalability exact gaussian process exact number worker run indicated number adjacent dot full size image building block augmented constrained optimization proposed kernel definition paired constrained gathered arg 1mu max subject sparsity requirement gathered augmented optimization argmax estimated sparsity lagrange multiplier constraint mean formulation exact ram restriction hit turn approximate without need user make decision point considered augmented biased optimization always prefer sparse covariance matrix however caution exercised ensure optimization dominated need sparsity formulation give priority likelihood since s\in 0,1 mean objective function bounded ,2\ln note solving linear system log-determinants optimization strategy computing sparse covariance matrix distributed fashion left enable training optimize marginal log-likelihood need solve aligned =\textbf aligned compute aligned aligned common approach dense sparse case use cholesky factorization given factorization linear-system solution log-determinant computation trivial however even sparse input matrix cholesky might large memory requirement depending fill-in pivoting option addition decomposition method successful matrix extremely sparse handful non-diagonal non-zero entry level sparsity might able guarantee matrix originating experience better use iterative method e.g conjugate gradient solve linear system leaf problem estimating log-determinant accurately work employed random linear algebra rla specifically implemented method presented since training via markov-chain monte-carlo random noise induced rla affect training move deterministic optimizers especially derivative-based optimizers discussion revisited moderately-sized example verify error convergence demonstrate functionality method investigate error convergence gp-predicted model proposed method viable ground truth recovered data use united state topography point choose point training dataset point test dataset test test _i\right\ dataset moderately large outside capability exact-gp algorithm chose smaller dataset able calculate root-mean-square error rmse iteration training somewhat efficiently example used kernel n_1 n_2 employing markov-chain-monte-carlo mcmc algorithm training rmse defined aligned rmse i=1 test f_i\right aligned test _i\ test measurement elevation f_i\ calculated posterior mean result presented fig result conclude error converges toward zero hyperparameter search progress figure verify functionality proposed methodology present error convergence posterior-mean prediction test data panel show full data set topography united state meter sea level evaluated point set selected data point training test point randomly calculated rmse hyperparameter search via mcmc progressed panel show error convergence confirm proposed methodology lead error convergence approach final hyperparameters full size image climate example million data point demonstrate proposed methodology temporal extension dataset shown fig dataset contains daily maximum temperature circa gauge-based weather station across continental united state accounting missing daily measurement station yield 51.6 million data point across approximately 10000-day 30-year period due computing constraint writing paper randomly extracted dataset point use example reproducibility purpose dataset found online ftp test used gp2scale library part fvgp python package available github http pypi pip install fvgp gaussian process needed analyze data variety reason first situ measurement daily weather variable provide realistic data source understanding historical climate user data often require geospatially-interpolated datasets account irregular sampling density provide complete picture temperature vary space daily maximum temperature furthermore exhibit strong spatial autocorrelations due driving physical mechanism ocean atmosphere gps particularly well-suited model statistically temporal domain autocorrelations also generally quite strong daily temperature due e.g. seasonality imposed solar cycle gps needed appropriately impute missing measurement gauge location example defined kernel aligned k\left _2\right =\tilde _2\right g_1 _1\right g_1 _2\right g_2 _1\right g_2 _2\right aligned defined g_1 g_2 defined n_2 giving rise hyperparameters deliver proof-of-concept proposed strategy employing markov-chain-monte-carlo mcmc training function evaluation since total compute time scale linearly number function evaluation straightforward estimate compute time many training strategy test chose two different architecture namely nersc cori haswell node http perlmutter gpu node perlmutter phase http due challenge allocating dask worker cori result shown calculated perlmutter a100 nvidia gpus computing batch size accomplished circa 0.6 second gpu node see fig visualization result due early-access constraint split run separate run storing hyperparameters therefore state training therefore total run time contains initialization iteration mcmc took circa sec leading total estimated run time sec also included information error convergence figure using smaller subset full dataset figure result gaussian process trained million data point paper best understood proof-of-concept want ensure show reader resulting model reasonable end training panel distribution climate station temperature first day dataset jan 1st normalized panel interpolation subdomain northeast time slice june noise measurement estimated ad-hoc explains somewhat rough appearance posterior-mean function trained via mcmc iteration doe reach convergence enough demonstrate feasibility extreme-scale panel show marginal log-likelihood function training time trained gpus opening door much larger gps verify error convergence also extracted smaller dataset point full climate dataset rmse respect test point function mcmc iteration number visualized panel full size image summary discussion conclusion paper proposed new methodology algorithm extreme-scale exact gaussian process gps based flexible non-stationary compactly-supported kernel distributed computing method another approximate designed discover—not induce—naturally occurring sparsity use alleviate challenge numerical complexity compute time storage strong belief natural sparsity common many modern datasets fundamental assumption work gps often give rise sparse covariance matrix naturally given enough flexibility non-stationary kernel design discover sparsity achieved kernel flexible non-stationary compactly supported efficiency reason covariance still computed dense format first accomplished distributing workload many cpu gpu node constrained augmented optimization used give sparse solution priority constrain sparsity constraint take effect ram computing restriction system exceeded would turn exact optimal sparse work proof-of-concept stage therefore several challenge current form addressed future work sparsity-discovering kernel example relatively simple shown much flexible bump-function-based kernel formulated hyperparameters found robustly however trade-off consider flexible kernel lead better detection sparsity costly optimization hyperparameters hyperparameters also mean possible ill-posed optimization problem used mcmc training mean evaluate marginal log-likelihood proposed method extended gradient-based optimization hyperparameters covariance matrix computed distributed manner linear-system solution log-determinant computation serialized even though worker idle used task however observed sparsity found substantial computation bottleneck despite shortcoming method shown strength training gaussian process five million data point knowledge largest exact ever trained given strong weak scaling shown fig predicted confident exact gps million data point currently possible code available part open-source python package fvgp gpcam