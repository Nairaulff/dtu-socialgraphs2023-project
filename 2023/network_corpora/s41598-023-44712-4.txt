introduction popularity microsoft kinect intel realsense modern smartphones e.g iphone samsung galaxy s20 depth image obtained easily conveniently result salient object detection sod using rgb image depth image rgb-d image become hot research topic benefiting stable geometry additional contrast cue depth image provide important complementary information sod especially emergence fully convolutional neural network fcns make possible capture multi-level multi-scale feature thereby boosting performance rgb-d sod fcns predict salient object assembling multi-level feature however object-part dilemma mechanism fcns demonstrated fig four representative example shown fig part predicted salient object fcns immersed background disturbed background may easily mislabeled non-salient region result incomplete segmentation word relationship object part taken consideration existing fcns ideally salient object complete entity composed several associated part large proportion object predicted salient region complete object would determined salient object shown fig salient object segmented whole high probability object-part relationship taken consideration capsule network capsnet figure example object-part relationship sod first row tail wing aircraft recognized salient object fcns proposed method predicts aircraft whole second row incoherence right arm cartoon figure predicted fcns proposed method regard right arm cartoon figure integral whole third line salient object predicted fcns miss stem plant flower stem predicted ccnet last row flame satellite misidentified salient object fcns however proposed method suppresses interference identifying satellite complete object note reproduced permission reference copyright ©2017 ieee reference copyright ©2016 ieee reference copyright ©2018 ieee reference copyright ©2015 ieee full size image recently capsnet proposed preserve vector quantity rather scalar quantity replacing max-pooling operation convolutional stride dynamic routing vector quantity capable preserving object-part relationship basic element capsule capsule encapsulates group neuron whose output vector quantity representing different property entity including position size sharp orientation preserve enough information explore object-part relationship furthermore associated part object represented child capsule child capsule clustered dynamic routing algorithm generate parent capsule unfortunately despite great performance capsnet known high computational demand term memory run-time even simple image classification task especially child capsule store intermediate representation parent capsule clustered dynamic routing algorithm determines coefficient every child capsule every parent capsule large amount gpu memory required dynamic routing algorithm occurs therefore impractical capsnet deal sod inspired observation introduce object-part relationship rgb-d sod paper provides solution incomplete salient object segmentation convolutional capsule network based feature extraction integration ccnet rgb-d sod proposed explore object-part relationship low computation demand system consists two key part one proposed extract integrate feature based vgg global context module gcm attention mechenism fdm feature depth module one feature-integrated convolutional capsule network ficaps whose structure composed convolutional part deconvolutional part similar segcaps specifically proposed ficaps child capsule routed parent capsule within defined local kernel besides transformation matrix shared member grid within capsule contribution summarized follows introduce object-part relationship rgb-d sod using ccnet best knowledge earlier attempt apply capsnet explore object-part relationship rgb-d sod novel ficaps proposed integrate external multi-level feature internal capsule demonstrated fig proposed method recognize segment salient object whole high probability compared method based fcns compare approach state-of-the-art rgb-d sod experimental result demonstrate ccnet outperform state-of-the-art algorithm related work utilization rgb-d image sod based fcns extensively explored year based goal paper review rgb-d sod method well capsnet illustrate difference proposed method related work rgb-d salient object detection pioneering work produced niu based traditional method various hand-crafted feature originally applied rgb sod extended rgb-d sod paper pay much attention rgb-d sod based deep learning algorithm example propose lightweight sod real-time localization composed lightweight feature extraction network based multi-scale attention jump connection residual refinement module chen propose alternate refinement strategy combine guided residual block predict refined feature salient map simultaneously lei proposes su2ge-net firstly cnn-based backbone replaced transformer-based swin-transformerv2 besides edge-based loss training-only augmentation loss introduced enhance spatial stability zhao build real single-stream network combining rgb-d image starting point taking advantage potential contrast information provided depth image compared algorithm similarity difference similarity design idea proposed algorithm su2ge-net related work try replace cnn-based backbone novel backbone swin-transformerv2 capsnet furthermore structure salient object detector well including encoders decoder difference ccnet predicts salient object mainly based capsnet whose basic element vector quantity however salient detector scalar quantity capsule network recently novel deep learning network named capsnet developed hinton capsule essentially group neuron represent specific type entity position size orientation deformation texture etc capsnet totally different fcns two aspect one hand neuron fcns scalar capsnet vector hand fcns extract integrate multi-level feature encoder decoder capsnet match associated active child capsule parent capsule dynamic routing algorithm sabour proposes vector capsnet iterative dynamic routing algorithm proposed assign child capsule corresponding parent capsule via transformation weight spatial relationship part object encoded learned dynamic routing algorithm transformation weight one year later hinton consolidated vector capsnet proposing matrix capsnet whose capsule composed pose matrix activation probability coefficient child capsule parent capsule calculated iterative expectation–maximization algorithm finding tightest cluster capsule compared vector capsnet transformation matrix matrix capsnet much parameter furthermore matrix capsnet use iterative measure similarity capsule vector capsnet cosine similarity view advance attempt made apply capsnet several computer vision task including object segmentation sod reduce high computational demand lalonde bagci design segcap based vector capsule solve object segmentation extends idea convolutional capsule locally-connected routing concept deconvolutional capsule liu colleague propose two-stream part-object relational network tsportnet implement matrix capsnet sod whose activation map final salient map method try reduce computation demand make possible used large-scale image task paper structure proposed method similar segcaps different segcaps proposed method encoder excavate capsule concatenates corresponding multi-level feature however encoder decoder segcaps enclosed environment furthermore tsportnet prefers explore object-part relationship based matrix capsule use activation map salient map predicted salient map coarse need refined contrary proposed ficaps extracted feature fcns input subsequently refined salient map predicted directly ficaps without post-processing methodology paper begin demonstrating overall architecture ccnet depicted fig introduce principle detail information module figure framework ccnet feature extracted vgg backbone denoted feature refer output gcm depth image downsampled directly labeled feature gcm depth image integrated fdm whose output fd_ fd_ fd_ fd_ fd_ next step output fdm aggregated feature fusion module ffm progressively denoted ff_ ff_ ff_ ff_ .in ficaps conv mean traditional convolution kernel size convcaps mean convolution capsule layer whose stride padding equal deconvcaps refers deconvolution capsule layer implemented transposed convolution stride padding concatenation indicates series operation including concatenation reshape convolution integrating internal capsule external feature full size image overall architecture figure show overall architecture ccnet system begin vgg backbone extracting multi-level feature feature input gcm exploit depth image downsampled max pooling shrink corresponding multiply including respectively next step depth image integrated feature gcm fdm directly based attention mechanism output fdm integrated ffm progressively whose output input ficaps fuse external multi-level feature capsule ulteriorly structure ficaps similar u-net encoder capsule processed convolutional capsule layer map child capsule parent capsule dynamic routing algorithm defined local connection besides concatenation module encoder proposed integrate external feature internal capsule come decoder capsule processed deconvolutional capsule layer mainly composed transposed convolution stride finally output ficaps predicted salient map feature depth module fdm used reweight feature gcm based depth image structure gcm introduced detail fig multiply feature depth image downsampled corresponding size product processed two convolution batch normalization relu operation consequently feature concatenated downsampled depth image processed two convolution next facilitate attention mechanism including channel attention spacial attention generate reweighted map multiply input feature following convolution procedure formulated follows gd_ conv_ conv_ figure structure fdm conv refers convolution batch normalization relu operation conv mean convolution symbol indicates multiplication concatenation operation pixel level pool refers pooling operation whose multiple full size image cd_ conv\left conv\left cat\left gd_ fd_ conv\left conv_ ca\left cd_ sa\left cd_ refers feature gcm depth image time downsampling conv_ conv\ indicates convolution without batch normalization relu operation mean channel attention spacial attention parameter range symbol mean multiplication operation pixel level feature fusion module ffm integrates adjacent two feature high-level low-level generating feature map showed two input feature first undergo convolution layer respectively relative high-level feature upsampled concatenated low-level feature processed two convolution layer ff_ conv conv cat conv_ fd_ conv_ fd_ fd_ fd_ refers relative high-level feature low-level feature respectively conv_ conv_ conv\ indicate convolution batch normalization relu operation up\ indicates 2-times upsample operation cat\ mean concatenation operation range feature-integrated convolutional capsule network figure show detail ficaps first introduce structure ficaps elaborate detail including convolutional capsule layer deconvolutional capsule layer concatenation layer ficaps share architecture u-net encoder contains two basic module convolutional capsule layer concatenation layer decoder composed convolutional capsule layer deconvolutional capsule layer first feature map ffm transformed capsule capsule downsampled convolutional capsule layer stride put convolutional capsule layer stride mapping child capsule parent capsule using dynamic routing algorithm subsequently concatenation layer first transforms capsule back feature map via reshape operation transformed feature map concatenated corresponding external feature processed convolution reshaped capsule procedure executed three time capsule obtained decoder capsule first upsampled deconvolution capsule layer stride upsampled capsule corresponding capsule encoder concatenated bridge connection generate capsule processed convolutional capsule layer well procedure repeated three time predict final salient map figure structure ficaps conv0 represents traditional convolution convcaps indicates convolutional capsule layer stride convcaps mean convolutional capsule layer stride deconvcaps represents deconvolutional capsule layer based transposed convolution red dash arrow refers bridge connection capsule encoder corresponding capsule decoder black arrow data flow whose data size described text near concatenate layer mean concatenation operation integrating internal capsule external feature mean corresponding external feature full size image convolutional deconvolutional capsule layer convolutional deconvolutional capsule layer contain two part one transformation module capsule one dynamic routing algorithm seven parameter capsule layer described capsule layer\left inv onv .the inv mean number input capsule number vector input capsule onv mean number output capsule number vector output capsule respectively two option including conv deconv conv mean convolution capsule layer deconv mean deconvolutional capsule layer refers number stride convolution cooperating accomplish different operation conv mean convolution time downsampling furthermore deconv mean convolution time upsampling mean iteration time dynamic routing algorithm set paper convolutional capsule layer decides assign active child capsule parent capsule similar process clustering relatively parent capsule corresponds cluster center relatively child capsule corresponds data point solved algorithm mapping measured transformation matrix called voting routing defined refers child capsule parent capsule mean voting result capsule layer capsule layer transformation matrix next gaussian mixture model introduced supposing gaussian distribution n\left diagonal covariance matrix diag posterior probability belonging gaussian defined n\left diag\left n\left diag\left activation capsule mixture coefficient gaussian mixture model treated dimensional vector result child capsule vote parent capsule contribution coefficient capsule calculating cluster center capsule consider activation value follows finally procedure convolutional deconvolutional capsule layer discussed follows first foremost transform capsule feature map reshaping capsule inv feature map inv following convolution layer transform feature map back capsule reshaping feature map onv capsule onv finally capsule execute dynamic routing using algorithm time concatenation layer concatenation layer includes two reshape operation concatenation operation convolution supposing size capsule convolutional capsule layer size external feature map procedure concatenation layer discussed follow first stage transform capsule feature map reshaping capsule feature map therefore shape capsule feature map furthermore concatenate transformed feature map external feature following convolutional layer lastly concatenated result transforms back capsule reshaping feature map back capsule loss function parameter proposed method supervised cross-entropy loss margin loss described loss log\left pred log\left pred max\left pred max\left pred represents cross entropy loss margin loss respectively pred indicates ground truth prediction salient object refers constant parameter paper set 0.9 0.1 respectively set experiment analyze section numerous experiment conducted verify effectiveness superiority ccnet module evaluating four evaluation metric benchmark datasets evaluation metric evaluate performance model four public rgb-d benchmark datasets nju2k sample nlpr sample stere sample sip sample choose sample nlpr sample nju2k train algorithm remaining sample used testing four widely-used metric used evaluate performance including mean absolute error mae f-measure max s-measure e-measure implementation detail proposed ccnet implemented pytorch trained epoch single nvidia tesla gpu adam optimizer used default value initial learning rate set 1e−4 adam optimizer batch size poly learning rate policy used power set 0.9 data augment every input data batch training session resized random flipping rotation color enhance random pepper training session rgb image depth image combined together data batch inference session rgb-d image put trained model predict salient map without post-processing comparison state-of-the-art method section compare proposed network state-of-the-art method including pcf mmci cpfp drma d3net ucnet ssf s2ma conet cmms danet a2dele pagr dfm dsa2f hainet ssl disenfuse icnet cmwnet bbsnet cdnet dcf2 quantitative visual comparison taken account fair comparison quantitative comparison table show quantitative comparison salient detector three perspective first foremost evaluation score method four benchmark datasets present column obviously model achieve top-3 performance nlpr stere sip four evaluation metric importantly proposed method posse least mae nlpr stere approximately 8.7 2.7 promotion respectively secondly count top-3 number every method statistical result demonstrated column named top remarkable proposed method occupy largest number 11/16 finally calculate average value evaluated metric four datasets listed row named average-metric model reach top-3 performance datasets rank 1st average mae table quantitative comparison full size table visual comparison figure show visual comparison example reflect various scenario including complex scene 1st 2nd row multi-objective salient object 3rd 4th row small object 5th 6th row low contrast salient object background 7th 8th row image come downloading experimental result github directly training source code github predicting salient object complex scene compared approach mostly predict blurry salient object recognize non-salient part around salient object salient part multi-objective detection several method miss salient object predict salient object noise come small object compared method predict clear complete salient object whose size small image lastly scenario low contrast existing salient detector mostly get poor object smoothness poor detail salient object besides compared method miss important part salient object sum proposed method consistently produce accurate complete salient map sharp edge various case figure visual comparison different method 1st 2nd row indicate complex scene multi-objective object included 3rd 4th row 5th 6th mean scene small target low contrast background object displayed 7th 8th row note reproduced permission reference copyright ©2017 ieee reference copyright ©2016 ieee reference copyright ©2018 ieee reference copyright ©2015 ieee full size image ablation study section validate effectiveness proposed structure first foremost evaluate performance proposed ficaps comparing u-net furthermore testify strategy integrating external feature internal capsule ficaps next proposed fdm evaluated finally performance gcm verified replacing traditional convolution experimenal result demonstrated table table ablation study full size table effectiveness ficaps evaluate performance ficaps two aspect one hand use u-net compared structure evaluate performance ficaps ficaps replaced u-net module parameter remain unchanged experimental result table row row show ficaps outperforms u-net addition evaluate effectiveness integrating internal capsule external feature ficaps verify train method without integrating external feature obviously row row table integration external feature effective way improve performance approximately 0.1–9.8 promotion effectiveness integration way depth image evaluate performance integrating depth image directly section try integrate depth feature extracted depth image vgg mobilenet resnet18 instead integrating depth image directly section another independent vgg backbone used extract depth feature depth image predict salient map based depth image extracted depth feature integrated feature rgb image experimental result table demonstrates proposed method effective way 15.2–50 promotion mae approximately 0.8–2.2 improvement evaluation metric effectiveness gcm evaluate contribution gcm replace gcm traditional convolution batch normalization relu operation experimental result table demonstrates gcm effective way integrate feature conclusion paper pay much attention solving object-part relationship dilemma sod therefore propose novel ccnet based capsnet computation demand make explore object-part relationship available applicable proposed method includes two main step first step rgb-d feature extracted integrated second step object-part relationship explored fully using ficaps subsequently final salient map predicted ficaps extensive experiment four datasets demonstrate proposed method outperforms sota method importantly ficaps transferable rgb-d sod ficaps used complementary branch architecture area sod explore object-part relationship feature map input ficaps attention map considering object-part relationship predicted attention map integrated feature predict final map future may focus two aspect improve performance ccnet one hand ficaps convolutional capsule network extent pure capsule network therefore discussed related work vector capsnet matrix capsnet may introduced explore object-part relationship true sense hand reducing computational demand capsnet mutual learning knowledge distillation may introduced