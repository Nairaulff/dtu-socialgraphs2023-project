introduction kumaraswamy kum distribution similar beta distribution notable advantage invertible cumulative distribution function expressed closed-form kum distribution flexible distribution two shape parameter applied finite probability ranging covid-19 data monthly water capacity shasta reservoir see gui kum showed commonly used probability distribution function like normal log-normal beta distribution adequately model hydrological data daily rainfall stream flow result kum introduced new probability density function known sine power probability density function kum introduced kum distribution versatile probability density function double-bounded random process distribution suitable modeling various natural phenomenon lower upper bound individual height test score atmospheric temperature hydrological data additionally kum distribution used scientist need model data finite bound even using probability distribution infinite bound analysis kum distribution probability density function pdf described f\left x\right =\alpha two positive shape parameter one obtain uniform distribution special case kum distribution cumulative distribution function cdf kum distribution given f\left x\right =1- figure showed behavior pdf cdf kum distribution different value parameter fig notice different pattern distribution according value parameter notice presence u-shaped shape =0.5 =0.5 gamma distribution find form approximates exponential distribution rest value figure behavior kum distribution full size image industrial life testing medical survival analysis common object interest lost withdrawn failure object lifetime known within interval result sample incomplete often referred censored sample various reason removal experimental unit saving future use reducing total test time lowering associated cost right censoring technique used life-testing experiment handle censored sample conventional type-i type-ii censoring scheme common method right censoring allow removal unit point terminal point experiment limiting flexibility address limitation general censoring scheme called progressive type-ii censoring scheme pcs-ii proposed follows suppose unite placed test time zero failure observed first failure say remaining unit randomly selected removed time second failure remaining unit selected removed finally time failure rest unit removed =n- -\dots m-1 -m\ thus possible witness data gradual censorship plan even though encompassed section data value previously known joint probability density function pcs-ii order statistic balakrishnan aggarwala l\left =\mathrm i=1 i\right 1-f i\right c=n\left -1\right -\dots -m+1\right =\dots m-1 =0\ =n-m\ corresponds type-ii censoring =\dots =0\ n=m\ corresponds complete sample theory estimation consists method make inference generalization population parameter today trend distinguish non-bayesian bayesian estimate method statistical computational strategy doe rely bayesian inference referred non-bayesian method bayesian method utilizes prior subjective knowledge probability distribution unknown parameter conjunction information provided sample data non-bayesian method bayesian method estimation introduced next section gholizadeh examined performance bayesian non-bayesian estimator estimating shape parameter reliability failure rate function kumaraswamy distribution progressively type-ii censored sample used various loss function squared error loss precautionary loss function linear exponential linex loss function obtain maximum likelihood bayes estimate reliability failure rate function symmetric asymmetric feroze el-batal focused estimating two parameter kumaraswamy distribution using progressive type-ii censoring random removal derived maximum likelihood estimate unknown parameter also determined asymptotic variance–covariance matrix eldin studied parameter estimation kumaraswamy distribution using progressive type-ii censoring obtained estimate maximum likelihood bayesian method bayesian approach two parameter considered random variable estimator parameter obtained employing squared error loss function erick focused estimating parameter test unit kumaraswamy distribution using progressive type-ii censoring scheme employed algorithm derive maximum likelihood estimate parameter additionally calculated expected fisher information matrix based missing value principle el-sagheer study various method used estimate unknown parameter lifetime parameter reliability hazard function two-parameter kumaraswamy distribution based progressively type-ii right-censored sample method included maximum likelihood bayes parametric bootstrap classical bayes estimate obtained utilizing markov chain monte carlo technique sultana investigated estimation problem unknown parameter kumaraswamy distribution type-i progressive hybrid censoring derived maximum likelihood estimate parameter using algorithm bayes estimate obtained different loss function using lindley method importance sampling procedure gui discussed considered estimation unknown parameter featured kumaraswamy distribution condition generalized progressive hybrid censoring scheme derived maximum likelihood estimator bayesian estimator symmetric loss function asymmetric loss function like general entropy squared error well linex loss function since bayesian estimate fail explicit computation lindley approximation well tierney kadane method employed obtain bayesian estimate ghafouri rastogi considered estimation parameter reliability characteristic kumaraswamy distribution using progressive first failure censored sample derived maximum likelihood estimate using algorithm compute observed information parameter used constructing asymptotic confidence interval also computed bayes estimate parameter using lindley approximation well metropolis–hastings algorithm kohansal bakouch conducted study estimation unknown parameter kumaraswamy distribution using adaptive type-ii hybrid progressive censored sample firstly obtained maximum likelihood estimation parameter using different algorithm newton–raphson method expectation maximization stochastic sem also derived asymptotic distribution parameter calculated asymptotic confidence interval ass uncertainty associated estimate moreover two bootstrap confidence interval achieved second bayesian estimation parameter approximated using markov chain monte carlo algorithm lindley method derived highest posterior density credible interval parameter paper organized follows maximum likelihood estimator maximum product spacing estimator unknown parameter kum distribution used create non-bayesian estimation method next section part look existence distinctiveness mles addition introduce asymptotic distribution unknown parameter generate asymptotic confidence interval bayesian estimation method section focus obtaining bayes estimate parameter lindley approximation markov chain monte carlo method utilized assuming independent gamma prior parameter additionally section includes construction highest posterior density credible interval unknown parameter simulation result data analysis presented simulation study real data analysis section providing examination performance estimation method various simulation real data analysis optimal progressive type-ii censoring scheme section introduces optimal progressive censoring scheme comparing different competing censoring scheme finally conclude paper conclusion section non-bayesian estimation method section examine task estimating kum parameter pcs-ii sample employing two estimation technique known maximum likelihood estimator mles maximum product spacing estimator mps maximum likelihood estimation suppose 1\right 2\right m\right pcs-ii sample drawn kum population whose pdf cdf given censoring scheme likelihood function given l\left i=1 taking log-likelihood function l\left =\mathrm log l\left one obtaining l\left =c+m\mathrm log +m\mathrm log +\left -1\right i=1 log i\right +\sum_ i=1 log i\right estimate parameter kum distribution obtained finding first partial derivative concerning parameter follows aligned l\left log log l\left log aligned let partial derivative respectively aligned log log log log aligned maximum likelihood estimator mles mle mle solution two nonlinear equation system need solved numerically obtain parameter estimation value maximum product spacing introduced maximum product spacing method based pcs-ii sample method technique selects parameter value minimize deviation observed data predetermined quantitative measure uniformity g\left =\prod_ i=1 m+1 i\right i-1\right i=1 1-f i\right one get g\left =\prod_ i=1 m+1 i-1 i=1 natural logarithm product spacing function s\left =\sum_ i=1 m+1 log i-1 +\sum_ i=1 log s\left =\mathrm log g\left estimator denoted respectively obtained solving following normal equation simultaneously s\left =\sum_ i=1 log +\sum_ i=1 m+1 log -\beta i-1 i-1 log i-1 i-1 s\left =\sum_ i=1 log +\sum_ i=1 m+1 i-1 log i-1 log i-1 obtain estimate parameter system requires solving two nonlinear equation numerically lead solution asymptotic variance–covariance asymptotic variance–covariance matrix mles two parameter inverse observed fisher information matrix follows array l\left l\left l\left l\left array =\widehat =\widehat variance–covariance matrix parameter given =\left array var cov cov var array aligned l\left l\left aligned l\left -\sum_ i=1 i\right log i\right i\right using asymptotic 1-\gamma confidence interval parameter easily obtained var var respectively var var element main diagonal variance–covariance uppe percentile standard normal distribution mle bayesian estimation method section bayesian estimation shape parameter denoted respectively obtained assumption independent random variable prior distribution gamma gamma respectively pdfs gathered gathered hyper parameter chosen reflect prior knowledge joint prior given applying bayes theorem combining likelihood function joint prior obtain posterior distribution parameter denoted proportional likelihood prior expressed =\frac l\left l\left d\alpha d\beta likelihood function follows l\left i=1 thus likelihood function rewritten follows aligned l\left c\left c\left c\left aligned hence taken exp log function aligned exp l\left exp c\left l\left c\left log c\left aligned join posterior density function written m+a m+a i=1 exp -\beta -\sum_ i=1 log m+a m+a i=1 exp -\beta -\sum_ i=1 log d\alpha d\beta thus posterior density function rewritten m+a m+a i=1 exp -\beta -\sum_ i=1 log conditional posterior density follows x\right =\frac i=1 exp i=1 log d\alpha hence conditional posterior density x\right i=1 exp i=1 log x\right =\frac i=1 exp -\beta -\sum_ i=1 log d\beta hence conditional posterior density x\right exp -\beta -\sum_ i=1 log clear x\right density function gamma -\sum_ i=1 log random variable obtained based three different type loss function namely squared error loss function symmetric loss function linear exponential linex general entropy loss function asymmetric loss function loss function loss function symmetric loss function take form -\theta -\theta estimate parameter loss function given stand posterior expectation parameter kum distribution loss function posterior mean d\alpha d\beta d\alpha d\beta linex loss function zellner represent linex asymmetric loss function defined linex -\theta c\left -\theta -c\left -\theta estimate parameter linex loss function linex given linex =-\frac log -c\theta stand posterior expectation parameter represents deviation direction degree deviation reflected magnitude underestimation greater overestimation opposite case approach zero linex loss function transformed loss function parameter kum distribution linex loss function may defined linex =-\frac log -c\alpha -c\alpha -c\alpha d\alpha d\beta linex =-\frac log -c\beta -c\beta -c\beta d\alpha d\beta loss function asymmetric loss function another valuable tool detecting overestimation underestimation extension entropy loss function calabria pulcini proposed general entropy loss function parameter given -\theta -q\mathrm log estimate parameter loss function given stand posterior expectation proper choice challenging task analyst reflects asymmetry loss function parameter kum distribution loss function may defined d\alpha d\beta d\alpha d\beta examining equation presented becomes apparent ratio two integral obtaining explicit expression integral difficult consequently necessary use appropriate technique approximate integral thus provide approximate lindley approximation markov chain monte carlo mcmc lindley approximation lindley suggested estimate calculating ratio integral given using three distinct loss function based prior distribution consider ratio integral i\left x\right l\left +\rho d\alpha d\beta l\left +\rho d\alpha d\beta function l\left log-likelihood =\mathrm log using approach developed lindley ratio integral written aligned i\left 2\hat 2\hat 2\hat 2\hat aligned =\widehat =\widehat i\partial =\widehat =\widehat i\partial =\widehat =\widehat ijk i\partial j\partial =\widehat =\widehat =\widehat =\widehat =-\frac mle partial derivative log-likelihood function ijk one obtained follows =\frac l\left =\frac +\sum_ i=1 log -\sum_ i=1 i\right log i\right i\right =\frac l\left -\sum_ i=1 i\right log i\right i\right =\frac l\left 2\mathrm -\sum_ i=1 log log =\frac l\left +\sum_ i=1 log =\frac l\left =\frac l\left =\frac l\left =-\sum_ i=1 log =\frac l\left -\sum_ i=1 i\right log i\right i\right =\frac l\left partial derivative log-prior function follows =\mathrm log +\left -1\right log +\left -1\right log thus =\frac =\frac lindley approximation loss function g\left observe =0\ hence obtained +\frac g\left derive hence obtained +\frac lindley approximation linex loss function g\left -c\alpha observe =-c -c\alpha -c\alpha hence obtained linex =-\frac log -c\widehat +\frac +\frac g\left -c\beta observe =-c -c\beta -c\beta hence obtained linex =-\frac log -c\widehat +\frac +\frac lindley approximation loss function g\left observe =-q -q-1 =q\left q+1\right -q-2 hence obtained +\frac +\frac g\left observe =-q -q-1 =q\left q+1\right -q-2 hence obtained +\frac +\frac hard obtain third derivative log-likelihood function metropolis–hastings algorithm used computing desired two advantage considering algorithm lindley method firstly need calculate third derivative log-likelihood function secondly sample obtained algorithm used obtain highest posterior density hpd interval distribution unknown parameter possible lindley method detail see dey markov chain monte-carlo case apply mcmc technique produce sample posterior distribution sample calculate unknown parameter construct corresponding credible interval conditional posterior density gamma density shape parameter scale parameter -\sum_ i=1 log thus sample easily generated using gamma generating routine posterior given doe present standard form plot show similar normal distribution mean standard deviation represents variance–covariance matrix therefore generate random number distribution use metropolis–hastings algorithm normal proposal distribution generate sample see tierney el-sagheer therefore mcmc given start initial value 0\right set i=1\ generate gamma -\sum_ i=1 log using following metropolis–hastings generate x\right normal proposal distribution n\left 4.1 generate proposal n\left standard deviation 4.2 calculate acceptance probability =\mathrm min x\right i-1 x\right 4.3 generate uniform 0,1 distribution 4.4 accept proposal set else set i-1 set i=i+1\ repeat step 3–6 time drawing random sample size posterior density possible discard initial sample burn-in use remaining sample calculate possible derive approximate bayes point estimate linex loss function follows =\frac i\right linex =-\frac log i\right represents number burn-in sample computation 1-\gamma hpd interval order construct 1-\gamma confidence interval n\left 1-\gamma n\left 1-\gamma n\gamma finally hpd confidence interval interval shortest length elicitation hyper-parameters section discus elicitation hyper-parameter value informative prior considered noted hyper-parameter value chosen depending informative prior suppose sample available k\left distribution associated maximum likelihood estimate j=\mathrm 1,2 hyper-parameter value obtained equating mean variance j=\mathrm 1,2 mean variance examined prior present work considered gamma prior respectively mean =\frac mean =\frac variance =\frac variance =\frac therefore equating mean variance j=\mathrm 1,2 mean variance gamma prior get j=1 =\frac k-1 j=1 -\frac j=1 =\frac j=1 =\frac k-1 j=1 -\frac j=1 =\frac find estimator solving follows j=1 =\frac j=1 k-1 j=1 -\frac j=1 =\frac k-1 j=1 -\frac j=1 solving equation yield estimator hyper-parameters =\frac j=1 k-1 j=1 -\frac j=1 =\frac j=1 k-1 j=1 -\frac j=1 prior distribution similarly estimator hyper-parameters prior distribution found =\frac j=1 k-1 j=1 -\frac j=1 =\frac j=1 k-1 j=1 -\frac j=1 one may also refer work dey singh tripathi regard simulation study real data analysis objective section evaluate effectiveness different estimation method discussed preceding section real dataset used illustrative purpose simulation study conducted observe proposed method perform pcs-ii simulation study subsection conduct simulation study compare performance different estimate confidence interval non-be method unknown parameter kum distribution pcs-ii compute average mean square error mse confidence interval average interval length ail coverage probability compare performance different method using number replication performance non-be compared based following assumption value =\left 0.5 0.5\right 0.5 1\right 1\right sample size simulation algorithm proposed balakrishnan sandhu used generate progressively type-ii censored sample removed item assumed different sample size number stage shown table table numerous pattern removing item life test different number stage full size table based generated data compute mles mps corresponding asymptotic confidence interval asy-ci deriving mles aware initial assume value considered true parameter value gamma prior distribution used compute parameter using symmetric asymmetric loss function estimate obtained lindley approximation mcmc method determine value hyper-parameters complete sample size constructed kum distribution various value using historical data obtained informative prior value used evaluate desired estimate mles employed initial value using algorithm along corresponding variance–covariance matrix end posterior density removed burn-in sample total 10,000 generated sample produced three different loss function linex -0.5 1.5\ loss function 0.1 additionally hpd interval estimate calculated using approach developed chen shao table display non-be obtained using mle different value respectively first column represents average estimate avg second column represents mean square error confidence interval asymptotic confidence interval asy average interval length ail coverage probability using mle table avg. mse practice asy-ci ail cps non-bes kum distribution different pcs-ii =0.5\ =0.5\ different value full size table table avg. mse practice asy-ci ail cps non-bes kum distribution different pcs-ii =0.5\ =1\ different value full size table table avg. mse practice asy-ci ail cps non-bes kum distribution different pcs-ii =1\ =1\ different value full size table table avg. mse practice asy-ci ail cps non-bes kum distribution different pcs-ii =1\ =2\ different value full size table table show obtained using lindley approximation mcmc method different loss function various value first column table indicates average estimate avg second column represents mean square error table present confidence interval include highest posterior density hpd interval average interval length ail coverage probability using mcmc method table avg mse practice lindely kum distribution different pcs-ii =0.5\ =0.5\ different value full size table table avg mse practice lindely kum distribution different pcs-ii =0.5\ =1\ different value full size table table avg mse practice lindely kum distribution different pcs-ii =1\ =1\ different value full size table table avg mse practice lindely kum distribution different pcs-ii =1\ =2\ different value full size table table avg mse practice mcmc kum distribution different pcs-ii =0.5\ =0.5\ different value full size table table avg mse practice mcmc kum distribution different pcs-ii =0.5\ =1\ different value full size table table avg mse practice mcmc kum distribution different pcs-ii =1\ =1\ different value full size table table avg mse practice mcmc kum distribution different pcs-ii =1\ =2\ different value full size table table hpd-ci ail cps mcmc kum distribution different pcs-ii =0.5\ different value full size table table hpd-ci ail cps mcmc kum distribution different pcs-ii =1\ different value full size table based table increase value specifically increase value lead decrease addition avg denotes true value two parameter estimation method comparing performance non-be method note estimate mle efficient estimate comparison performance method relative lindley approximation find value decrease linex loss function -0.5\ followed loss function mcmc algorithm find value decrease loss function general note estimate mcmc efficient estimate lindley approximation table observed hpd asy smallest largest average length respectively general result see increase case ail decrease corresponding percentage increase graph mcmc estimate using algorithm plotting estimate histogram estimate convergence estimate graph showed fig fig plot display random distribution value observed scattered around mean also histogram sequence observe choosing normal distribution proposal distribution quite appropriate figure convergence mcmc estimate full size image real data application subsection examine actual data pertains shasta reservoir monthly water capacity california usa data cover month february 2010. see sultana sultana data point listed follows aligned 0.338936 0.431915 0.759932 0.724626 0.757583 0.811556 0.785339 0.783660 0.815627 0.847413 0.768007 0.843485 0.787408 0.849868 0.695970 0.842316 0.828689 0.580194 0.430681 0.742563 aligned determine considered dataset appropriately analyzed using kum distribution goodness fit test conducted addition kum distribution also fit generalized exponential gen.exp burr xii burr beta distribution data set judge goodness fit using various criterion example negative log-likelihood criterion nlc akaike information criterion aic introduced akaike corrected aic aicc introduced hurvich tsai bayesian information criterion bic introduced schwarz smaller value criterion better model fit data result shown table fitting given data set graphically empirical cdf plotted corresponding fitted cdfs kum. gen.exp burr beta distribution also histogram plotted corresponding fitted pdf line distribution figure showed fitted line cdfs pdfs given data set corresponding distribution figure also indicate kum distribution provide better fit distribution least data set table goodness fit test different distribution real data full size table figure goodness fit test real data full size image referring value reported table conclude kum distribution fit data set good compared model thus various point interval estimate real data pcs-ii following table table point interval estimate non-bes kum distribution pcs-ii full size table table point interval estimate lindely kum distribution pcs-ii full size table table point interval estimate mcmc kum distribution pcs-ii full size table table display non-be obtained using mle m=10\ computed average estimate avg standard deviation asymptotic confidence interval asy using mle table display obtained using lindley approximation mcmc method different loss function m=10\ different four scheme defined =\left =\left 5\right =\left 5,5 computed average estimate avg standard deviation highest posterior density hpd interval using mcmc table observed avg different estimate close together worst performance bayesian estimate best performance figure showed profile-likelihood estimate mle pcs-ii =\left =10\ parameter form fig conclude existence uniqueness estimate mle maximum value likelihood function mle given estimate achieved figure profile-likelihood estimate mle pcs-ii full size image optimal progressive type-ii censoring scheme preceding section discussed non-be method estimating unknown parameter kum distribution using pcs-ii obtain sample conduct life-testing experiment using pcs-ii necessary advance knowledge however many reliability life testing study practical consideration require selecting optimal pcs-ii set possible scheme balakrishnan aggarwala extensively discussed problem determining best censoring plan using different setup comparing two different censoring scheme great interest several researcher see example kundu lee lee ashour determine optimum pcs-ii consider information measure following criterion criterion minimizing determinant variance–covariance matrix mles obtained det\left =var\left var\left cov\left criterion minimizing trace variance–covariance matrix mles obtained tr\left =var\left +var\left criterion depends choice tends minimize variance logarithmic mle -th quantile log consider -th quantile kum distribution 1-\left 1-u\right hence logarithmic kum distribution given log =\frac log 1-\left 1-u\right using delta method variance log approximated var\left log log log log log log =\widehat =\widehat gradient log respect unknown parameter .where aligned aligned thus variance log obtained var\left log =\left log log array var cov cov var array array log log array calculated value determinant trace variance–covariance matrix mles =0.5\ n=\left 40,80 20,30,40 presented table based three different quantiles namely u=0.25 0.5\mathrm 0.75\ criterion computed calculated value three criterion reported table table optimal censoring scheme kum distribution different criterion =0.5\ =1\ different value full size table using previous application real data application section considered pcs-ii sample size kum distribution using variance–covariance matrix mles easily compute value trace determinant variance–covariance matrix choice scheme presented table based three different quantiles namely u=0.25 0.5\mathrm 0.75\ criterion computed value indicate optimal censoring scheme one yield smallest determinant trace variance–covariance matrix mles table therefore optimum scheme criterion 10\right criterion value u=0.25 0.5 optimum scheme 5,5 value increased became optimum scheme 10\right table comparison different censoring scheme application full size table conclusion paper deal problem estimating unknown parameter kum distribution pcs-ii non-be perspective obtained mle asymptotic confidence interval estimate unknown parameter kum distribution also computed including lindley approximation mcmc symmetric asymmetric loss function along corresponding hpd interval estimate discussed choose hyper-parameter value based past sample compared method using mse ail result indicate superior non-bayesian estimate identified optimal censoring scheme life testing experiment based three criterion measure important information reliability practitioner future work extended studying neutrosophic statistic kum distribution another work could involve modeling covid-19 data different progressive censoring scheme