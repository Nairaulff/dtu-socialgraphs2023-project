introduction designing convolutional neural network cnn model require lot extensive experiment besides exploring machine learning parameter learning rate momentum also layer different hyperparameters considered example convolution layer one need explore convolution filter size number stride initialization approach etc. dropout layer dropout rate varied pooling layer pooling size pooling operation e.g. max average global average considered exploration parameter time consuming need done systematically alleviate model construction process beside exploration layer design one need find right structure specific recognition task searching possible structure computational-intensive consumes resource specific form architecture allowed searching right connection may possible within acceptable accuracy time research study methodology automatic exploration micro-architecture type given baseline architecture approach enables search model modification automatically obtain good accuracy demonstrate approach using vanilla squeezenet since small architecture still expanded proposed approach enable neural network designer explore model answer typical design question number convolution block needed number feature used merged skip connection applied apply optimization block inserted inserted properly flexible design squeezenet prototype called squeezenetsemauto presented traditional squeezenet flexible length selected block insertion skip connection selected merging operation one input also integrated hyperparameter exploration layer source prototype model available github http structure paper follows next introduce background literature review section design methodology methodology flexible design described section experiment present experimental method result finally section application concludes work explains future work background section introduce convolution neural network previous model next complexity various level model exploration explained last present literature review model exploration convolutional neural network convolutional neural network cnn designed based convolution operation applied many high-impact image processing task medical imaging self-driving car etc convolutional layer connected manner layer utilizes convolution operation using fixed-size filter perform operation matrix tensor operation involved multiplication addition typical cnn model consists following layer convolution pooling dropout activation fully-connected etc layer connected certain manner convolution layer important parameter filter stride padding size filter stride size imply output feature map size number consecutive convolution layer implies size receptive field pooling layer used reduce feature size also help select outstanding feature typical parameter pooling stride size pooling operation e.g max average global average pooling various combination convolution pooling cause different feature map output value size dropout layer used purpose overfitting elimination typical parameter dropout rate ranging dropout rate need examined well activation function applied change output value activation function selected depending task e.g. relu leakyrelu sigmoid tanh many others fully connected layer usually attached last classification output number fully connected layer one depending design number classification output besides above-mentioned aspect called hyperparameters cnn learning rate optimization function also affect high accuracy common learning rate value 0.1 0.01 0.001 0.0001 small learning rate lead slow convergence may get stuck local minimum using large learning rate skip global optimal value sometimes learning rate also described function optimization function adam rmsprop stochastic gradient descent sgd adagrad etc different optimization yield different gradient direction leading optimal weight state-of-the-arts architecture various designed module cnn proposed lengthen network increase accuracy module reduce computation model size maintaining accuracy one popular pre-trained model alexnet developed krizhevsky et.al trained imagenet dataset ilsvrc-2010 ilsvrc-2012 1.2 million image category architecture contains eight learnable layer five convolutional three fully connected called fc6 fc7 fc8 first five layer convolutional remaining three fully connected reference caffenet similar version alexnet except max pooling layer precedes local response normalization lrn reduce memory usage googlenet deep convolutional neural network structure used classification detection tool ilsvrc14 goal work small data set use small computing power memory employ inception module simultaneously computes 1×1 3×3 5×5 convolution enabling selection proper filter size automatically trained ilsvrc challenge classify image one leaf-node category imagenet dataset consists 1.2 million training image 50,000 validation 100,000 testing image squeezenet aim improve alexnet efficiency holding level accuracy minimized cnn advantage saving communication time server client over-the-air update feasibility embedded-device deployment squeezenet utilizes method reducing filter size reducing number input channel delaying downsampling squeezenet trained ilsvrc2012 imagenet design focus achieving smaller model size keeping accuracy alexnet vgg net improved accuracy adding convolutional layer removing lrn layer trained imagenet ilsvrc-2014 model various number layer layer making model parameter vary million trained ilsvrc2012 imagenet 1.3 million image 50k validation image 100k testing image resnet one first model contain many layer particularly consists many convolutional block consecutively convolutional block form residual block designed solve gradient vanishing exploding problem network ilsvrc competition variation resnet50 resnet101 resnet152 may combined module googlenet known googleresnet etc senet proposed based two subsequence module squeeze excitation squeeze excite operation squeeze operation performs combining feature map across dimension h\times obtain channel descriptor excitation operation capture channel dependency learns relationship channel performs activation excitation channel utilize above-pre-trained model transfer learning common approach transfer knowledge source model target model image application image feature edge shape learned early layer used later fully connected layer supposed fine-tuned specific task useful target data set size smaller source data set nature target image similar source image closer fewer tune layer pre-trained model small learning rate used pre-trained model skip unlearned feature cnn architecture design typical cnn architecture need adjusted applied new dataset small scale one need fine-tune hyperparameters layer medium scale architecture need adjusted example adding connection merge feature different scale convolutional block may added reduce feature map size large scale whole architecture changed example changing use transformer sequence-to-sequence paper change small scale called hyperparameter tuning medium scale connection structure called micro-architecture previous work using transfer learning fine-tune new task adopting conventional different architecture doe lead significant change model accuracy thus interested change micro-architecture level adjustment level lead accuracy improvement optimal parameter suitable specific classification task hyperparameter possible explore micro-architecture search sometimes called cnn optimization micro-architecture level choice using different kind layer e.g convolutional layer pooling layer classification layer explored layer may combined module certain purpose module convolutional layer certainly play dominant role hierarchically extracting meaningful feature result effective optimization micro-architecture primarily relates utilizing different type module contribute accuracy maintaining network size inception module based googlenet purpose module increase model depth make network wider allow parallel computation increase accuracy module factorizes large convolution smaller one reduce total number computation model size example inception-v1 contains 1x1 3x3 5x5 convolution 3x3 max pooling computing simultaneously concatenates result together figure example first version inception module simultaneous computation convolution speed training time significantly although network deep use various filter size enables selection proper filter size automatically inception-v2 module made wider several small filter 3x3 1x3 3x1 etc filter 1x1 used reshape feature map change dimensionality variation module depends factor convolution increase depth accuracy googlenet training time faster previous network figure inception module example full size image residual block used resnet proposed shown fig make network deeper particular mapping +x\ called identity mapping created feature output previous layer transferred called residual learning operation may convolution layer learning +x\ approximate well idea solve problem accuracy degrading increasing depth network depicted shortcut edge shown fig making deeper way help prevent overfitting expose opportunity improve accuracy gradually tweaking model underlying function instead skipping unneeded layer depth model maintained exact identity mapping following layer adjusting weight residual function zero hand residual function alleviates little remaining error prediction finding optimal function closer identity mapping figure residual block full size image skip connection highway network enables flow intermediate data previous layer using skip connection across sequence following layer instead layer-by-layer forwarding research motivated recurrent neural network learned gating unit control rate information flow give benefit individually better responding different input data fire module squeezenet introduces fire module shown fig split regular convolution layer two sub-layers called squeeze expand layer squeeze layer demonstrates usage 1x1 convolution filter decrease several input channel convolutional layer expand layer minimizes number model parameter preserving level accuracy via given ratio using 1x1 3x3 convolution filter instead using whole 3x3 filter order extract feature input transformed squeeze layer result concatenated output module according empirical result squeezenet reveals capability prediction level accuracy alexnet citealex whereas using 50x fewer parameter figure fire module full size image block squeeze-and-excitation network senet proposed block attached convolutional layer shown fig block used investigate importance relationship feature output channel applies global average pooling feature map derive channel descriptor squeeze operation fed two fully-connected layer learn feature importance excitation operation thus block role rescale original feature map strengthening significant one suppressing impact one figure squeeze excitation operation block full size image using attached block requires additional parameter compared original model work expose worthiness enhancing considerably level accuracy minimal additional cost memory size computation word block attachment rather adding convolution layer help improve accuracy deep model small increment parameter related work concept neural architecture search established year ago goal automatically discover optimal architecture specific task area recently become active deep learning research various technique applied demonstrated great success variety deep learning task image recognition natural language processing etc earlier year reinforcement learning -based method used search architecture controller generate potential neural network trained acquire performance reward evolving controller reinforcement learning algorithm earlier example whole network searched nasnet proposed design search space discover architectural building block instead entire network architecture proxy dataset learned block i.e scaling number learned block transferred targeted dataset save computational time resource searching enable transferability related task enas improved efficiency searching sharing weight possible architecture search space via creating large computational graph gathering possible model every subgraph i.e sampled architecture utilizes corresponding weight together instead retraining new sampled architecture scratch algorithm style performs automatic search whole architecture searching micro-architecture type also possible classification task need adjust portion network lead better performance exhaustively searching possible architecture consumes time resource effort rise automl framework becomes possible implement micro-architecture search easily autokeras one tool automl automate model finding tool relies algorithm three step update generate observation update phase train gaussian process model existing architecture generation phase creates training model based acquisition function ucb also several optimization considered limit search space editing distance tree optimization observation step generated model trained accuracy observed kera tunner hyperparameter optimization framework hyperparamter search contains built-in search algorithm bayesian optimization algorithm behavior based randomness search time result acceptable furthermore framework also provides recent search algorithm hyperband traditional random search framework also allows new implementation search algorithm recent work autopytorch framework utilized multi-fidelity approach optimize hyperparameters approach utilizes cost meta-learning approach squeeze-and-excite block block kind attention mechanism used previously seq2seq network concept concentrate useful feature channel useful one figure focus channel attention mechanism increasingly popular among many researcher study micro-architecture level learn channel importance given immediate result module cnn block used senet learns performs feature re-calibration highlight informative feature channel aggregate spatial information channel using global average pooling gap operation channel representation learned importance bottleneck two fully connected layer senet gap operation embedding information channel variation cbam additional information via max-pooling operation global information embedding utilized shared two layer compute combine result i.e channel importance using element-wise summation channel attention module also cbam present spatial attention module refine feature map along spatial dimension above-mentioned channel attention module moderately increase model complexity although bottleneck two layer proposed reduce fewer number parameter compared non-bottleneck version cause channel dimensionality reduction learning eca-net selected convolution operation instead two layer perform local cross-channel interaction calculating channel importance local cross-channel interaction strategy help avoid problem channel dimensionality reduction still preserve performance significantly decreasing model complexity paper demonstrate proper use block exploring block attachment position variation idea adapted explore option like cbam eca-net design methodology section explain add flexibility model facilitate exploration process variability divided two level first machine learning parameter hyper-parameters layer handled directly autokeras hyperparameter package secondly micro-architecture level user may explore possible use e.g residual block number inserted block location insertion etc figure squeezenet squeezenet v1.1 baseline full size image baseline architecture fig present baseline architecture used methodology squeezenet architecture consists fire module fire2 fire9 fire module depicted fig input size implementation adopted fig original squeezenet contains fire module followed max pooling layer fire module max pooling layer two final layer 9th fire module fire9 10th convolution conv10 classification instead fully connected layer fig max-pooling layer inserted differently particular inserted every two module fire2 fire9 convolution layer reality number fire module varied demonstrated original work thus add first flexibility introducing create network flexible length figure squeezenet auto-bypass squeezenet auto-length hyperparameters full size image adding variable length bypass original paper squeezenet author proposed bypass connection bypass connection skip odd fire module due compatibility dimension size combined layer figure show bypass configuration first modification propose put flag bypass connection fig bypass connection shown dashed line i.e. connection either inserted inserted last fire module dropout layer added dropout rate hyperparameter whose possible value 0.5 0.8 since fire module used pair propose use flexible number fire module pair figure show two dashed rectangle highlight group two fire module group second fire module still coupled skip connection first group flexible length either since required least one fire module second group possible number fire module lead total possibility group equivalently fire module choice pooling operation either average max pooling well figure squeezenetse full size image figure squeezenetseauto full size image module insertion adding block may yield accuracy improvement channel-based attention module easily attached baseline network similar way residual operation however many possible insertion point deep network considering one one time-consuming figure present squeezenet block insertion seen many possible point insertion fire module fig attach block various position squeezenet adding flexibility take network fig add possible connection block fire module group dashed rectangle contains insertion hyperparameter block squeeze ratio valued also variable skip connection added perhaps bypass block figure squeezenetse1auto full size image figure squeezenetautosem full size image multiple input merging last due concept neural network domain knowledge added network yield higher accuracy thus adding input feature improve accuracy however adding many feature lead over-fitting high computation without improvement accuracy figure show typical approach merge two input using addition operation network siamese network use two model proposed input fed model merging output done last stage methodology consider addition input different layer thus implies combining multiple feature flexible merging layer figure improved fig merging first layer finally fig show variable merging point note exists one merge point two path merged one path remains fig selected architecture contains first block two fire module two output merged pooling layer two consecutive pair fire module layer hyperparameters selected concept expanded merge number input figure squeezenetsemauto example full size image proposed methodology present concept adding variety architecture exploration three aspect next section conduct experiment using methodology find model architecture high accuracy framework facilitate model architecture exploration process experiment experiment compare result type micro-architecture exploration based previous section variable number fire module addition skip connection insertion block various place multiple-input merge point two data set cifar-10 tsinghua facial expression datasets used experiment three search strategy executed perform model searching validation accuracy reported measure top-10 model size best solution found found solution yield better accuracy baseline smaller model size divide section follows first report result cifar-10 benchmark goal find suitable architecture varying micro-architecture connection hyperparameters second result recent facial expression dataset reported data set also extract landmark feature dataset feature combined default input merging location explored main goal find whether adding input feature yield model better accuracy demonstrates need combining input model possible merging layer hypothesis experiment assumption set training table present machine learning hyperparameters table machine learning hyper parameter full size table table variable learning rate 0.001 0.0001 two optimization considered sgd rmsprop fix batch size variable hyperparameters assumed dropout layer dropout rate ranging 0.8 default value 0.4 pooling layer two choice operation max pooling average pooling squeezenet another parameter compression ratio set compression ratio block squeeze ratio parameter choose among value maximum epoch early stopped method used accuracy changed iteration three search algorithm tested random hyperband bayesian kera tuner hyperband algorithm take maximum epoch instead set bayesian search algorithm alpha value 0.001 beta 2.6 number initial point number trial random bayesian number trial also search scheme parameter sampled model built parameter input dataset fed batch training training done whole number epoch validation performed resource permitted large scaled dataset used since split batch utilize distributed training kera tuner also support cifar-10 dataset cifar-10 total data set 60,000 image size class image per class data set pre-divided 50,000 training image 10,000 testing image table show accuracy model three search algorithm parameter table used first two model squeezenet squeezenet11 presented section baseline architecture squeezenetauto model section adding variable length bypass squeezenetseauto model section module insertion vary sample size data set training testing see effect accuracy sampling size purpose see sampling size affect maximum accuracy obtained minimizing training time squeezenetseauto-0.1 squeezenetseauto trained training testing dataset similarly squeezenetseauto-0.4 squeezenetseauto-1 utilize whole data set respectively searching strategy doe affect solution much random search performs well enough compared baysian hyperband run little faster two use early stopped approach experiment may train maximum number epoch table accuracy result cifar-10 full size table table using squeezenetauto mostly yield model better accuracy compared baseline adding block accuracy improved model architecture reported table squeezenetauto three search algorithm row fire module block present number repeated block used first dashed block fig fire module block number repeated block second one dashed block contains two fire module row use bypass show whether skip connection red dashed line fig needed result show selected three model similar architecture table fire module block needed part three algorithm thus model use fire2 fire3 fire4 fire5 small difference use bypass section example random bayesian algorithm select bypass connection fire3 hyperband algorithm selects bypass connection fire3 fire5 random bayesian algorithm select bypass connection hyperband algorithm doe pick bypass connection use average pooling fire3 use max pooling fire5 random bayesian algorithm select sgd optimizer rmsprop chosen optimizer thus size three compared baseline fire2 fire9 used achieve similar accuracy table model obtained various search full size table consider solution obtained squeezenetseauto-0.4 table row use mean whether block yellow fig needed note dashed block fire module two block inside block may skip connection inside shown boolean row skip selected model contain number fire module table selected model random hyperband algorithm block needed part bayesian algorithm two block selected first part zero block used second part thus totally fire module fire2 fire3 fire4 fire5 used random selects block fire2 fire3 fire4 fire5 table squeezenetseauto 0.4 three algorithm full size table table show result using data train model random bayesian algorithm selected model contain three fire block hyperband algorithm model contains two fire block number block used inserted fire3–fire7 random algorithm fig fire5 hyperband algorithm fig fire2 fire5 fire7 bayesian algorithm fig respectively thus hyperband algorithm selects better model others since fire module block achieving accuracy model obtained random algorithm result imply achieve complicated model accuracy level table squeezenetseauto 1.0 three algorithm full size table figure model squeezenetauto random full size image figure model squeezenetauto hyperband full size image figure model squeezenetauto bayes full size image tsinghua facial expression data set contains subject subject class expression sad neutral surprise anger disgust happy content image size 2,000 1,500 data set publicly available example image shown fig data set divided 80:20 training testing set respectively figure anger disgust fear happy neutral sad surprise content full size image utilize squeezenet input image resized 224x224 first experiment model searching table report numerical result data set size large cifar-10 image size large reduce size speed time feed input network figure facial landmark extracted full size image auxiliary feature used landmark extracted mediapipe face detector based blazeface landmark contain point shown fig point coordinate value normalized 0,1 respect width height image respectively landmark depth reference depth head center treated origin small value mean position close camera scale figure present landmark plotted space figure draw depth map image fig used another input squeezenetsem experiment figure landmark space depth interpolation full size image table accuracy result tsinghua facial expression data set full size table table present validation accuracy baseline model model proposed algorithm table squeezenet squeezenet11 one table hyperparameters selected table mlpauto multi-layer perceptron use landmark point mediapipe create classic mlp model number dense layer varied number hidden node selected among last two row squeezenetse1auto take two input image input depth image input merges using add operation first convolution pooling layer squeezenetsemauto take two input merge point also parameter fig first three network low accuracy achieved varied hyperparameter thus fixed structure obstacle search exploration augmented block accuracy significantly improved benefit block channel normalization next question insert block many block needed explore block insertion algorithm squeezenetseauto presented section module insertion algorithm lead superior architecture found random hyperband bayesian algorithm respectively table show model size byte obtained three algorithm table show model size byte top-5 model squeezenetauto seen model size quite small accuracy low table show model size byte top-5 model squeezenetseauto random algorithm seen model size three time larger squeezenetauto accuracy time higher similar solution hyperband algorithm however interesting solution offered bayesian algorithm highest accuracy 0.674 model size 145,032 little higher squeezenetauto 123,880 model architecture depicted fig table model size accuracy squeezenetauto full size table table present model size byte top model obtained squeezenetse1auto first rank random algorithm yield model higher accuracy 0.691 model size 182,936 compared model squeezenetauto 123,880 table model size accuracy squeezenetseauto full size table table model size accuracy squeezenetse1auto full size table table solution found random algorithm achieving accuracy 0.691 model size 182,936 highest accuracy 0.714 obtained model selected bayesian algorithm model size time larger 884,688 obtained squeezenetauto 123,880 table model size accuracy squeezenetsemauto full size table figure show comparison size accuracy among case table highlight box show best accuracy case small model size case derived squeezenetseauto bayesian squeezenetse1auto random squeezenetsemauto hyperband squeezenetsemauto random figure present selected top three model table figure comparing model size accuracy test full size image figure model obtained squeezenetseauto tsinghua data set bayesian acc=0.67 size=14m full size image figure model obtained squeezenetse1auto tsinghua data set random acc=0.69 size=14m full size image figure model obtained squeezenetsemauto tsinghua data set bayesian acc=0.71 size=88m full size image experiment demonstrate proposed framework yield exploration model effectively help designer explore possible micro-architectures time exploring hyperparameters layer future customize search algorithm suitable specific micro-architecture type application proposed approach applied explore micro-architecture change baseline architecture requires similar tuning section demonstrate application task image segmentation object detection task backbone architecture similar micro-architecture style image segmentation image segmentation task popular architecture unet originally used biomedical image segmentation architecture contains collection convolution block downsampling upsampling layer downsampling corresponding upsampling layer connected skipped connection grey line fig figure unet architecture full size image figure unet-auto template architecture modified full size image original architecture micro-architecture customized follows fig starting depth convolutional layer number convolutional block to/from bottom number purple/green dashed box number layer down-block number layer up-block divided 4.1 number layer up-block part 4.2 number layer up-block part number convolutional layer bottom block hyperparameters explored dropout rate optimization method learning rate etc table present hyperparameters micro-architecture parameter along machine learning parameter integrated template design fig default value chosen manner original architecture table hyperparameters micro-architecture parameter unet-auto full size table segmentation task applied weed dataset http dataset contains image annotation masking fig figure weed dataset full size image table present top rank discovered model algorithm based validation accuracy training done epoch trial compared traditional model fig achieves 0.964 accuracy unet-auto achieve higher accuracy 0.977 0.973 0.974 search algorithm model size vary due number convolutional block number convolutional layer inside block exploration one choose proper model configuration acceptable accuracy show effectiveness finding proper configuration baseline network table model size accuracy unet-auto full size table table present configuration top-3 solution search algorithm among best model achieves parameter baseline however one prefers small model model bayesian algorithm yield equivalent solution accuracy minimum convolutional depth used achieve equivalent better accuracy lengthening number convolutional block result show rmsprop proper optimization task template help designer explore micro-architecture option along hyperparameter choice effectively table model configuration unet-auto various search algorithm full size table object detection code deploys mobilenetv2 single shot detector ssd backbone adopted implement variation micro-architecture original mobilenetv2 architecture composes bottleneck layer b1_1 b7_1 total number parameter 2.2m shown table backbone ssd consists convolutional layer utilizes mobilenetv2 extract feature table mobilenetv2 architecture full size table duplicate convolutional layer bottleneck block mobilenetv2 e.g. b3_3 b4_3 b5_3 b6_3 note bottleneck block based depthwise separable convolution duplicating convolutional layer lead deeper network without enlarging number parameter template architecture shown fig cycle four layer represent replication number possible replication set within default value vary learning rate parameter way previous experiment figure mobilenetv2-auto full size image figure mnist dataset bounding box full size image mnist dataset object detection bounding box generated example fig confidence loss smoothl1 loss used metric original code training data size testing data size table show validation loss found model different search algorithm show validation loss since model size original one original model yield loss value 0.01 case lead smaller loss table validation loss various mobilenetv2-ssd-auto full size table table list replication rep configuration found case top three loss value varying number convolutional layer lead accuracy increasing model size thus utilizing template model enables exploration micro-architecture choice improve model effectiveness table model configuration mobilenetv2-ssd-auto various search algorithm full size table conclusion future work paper propose framework exploring model choice based automodel baseline model possibly attached module block skip connection etc proper number component selected model hyperparameters searched time better model architecture possible found smaller network size first demonstrate approach based squeezenet model attaching block skip connection variable position model recognized template model variable length attachment point explored standard search algorithm random hyperband bayesian model higher accuracy obtained attaching small number block skip connection automatically also present alternative applying micro-architecture variation model different task image segmentation object detection segmentation task unet-auto template vary many part number convolutional block number convolution layer depth etc object detection mobilenetv2-ssd-auto template vary number convolutional layer bottleneck template model structure best accuracy size obtained conveniently future work include mechanism considering customized search scheme