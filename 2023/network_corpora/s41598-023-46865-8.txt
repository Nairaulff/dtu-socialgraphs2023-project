introduction artificial neural network built collection connected unit node known artificial neuron roughly modelled neuron biological brain warren mcculloch walter pitt donald hebb introduced concept artificial neural network ann 1940s beginning mean simulating intelligent behavior modelling interconnected circuit neuron brain node ann include input layer hidden layer output layer connection node artificial neuron one threshold weight associated node whose output exceeds predefined threshold value activated begin providing data subsequent layer network condition met data sent next network layer training neural network standard approach involves randomly selecting starting point following gradient till reaching top hill method known gradient decent algorithm objective function single peak cost function linear regression method highly effective efficient however real-world scenario tremendously complicated problem described landscape made many peak valley algorithm fail due intrinsic tendency become stuck local optimum another flaw inefficiency differential operation hidden layer multilayer network typically employ sigmoid transfer function compress infinite input range finite output range function commonly referred squashing function defining feature sigmoid function slope tends zero input big example using steepest descent train multilayer network sigmoid function might run trouble magnitude gradient change input size grows would result increasingly minor adjustment weight bias even though still well optimal value contrast genetic algorithm gas potent optimization technique provide fast search strategy large problem space replacing gradient descent might useful avoiding above-mentioned issue also genetic algorithm many advantage global optimization scheme particle swarm optimization pso ant colony optimization aco simulated annealing etc genetic algorithm use population-based approach allowing diverse solution explored converging towards promising region characteristic make well-suited tackling complex optimization problem explores search space extensively ensuring diverse set potential solution considered also parallel nature allows simultaneous evaluation evolution multiple solution enabling faster convergence improved scalability posse inherent robustness local optimum due population-based nature mechanism selection crossover mutation characteristic allows escape suboptimal solution continue searching better solution globally genetic algorithm successfully applied wide range optimization problem including feature selection parameter tuning classification task literature study intends employ train neural network widely used many subfields management science operational research industrial engineering since one well-known metaheuristic algorithm iterative process required genetic operator implemented parallel distributed computer architecture gas would effective solving large-scale optimization issue order harness power efficiency evolution training anns research suggests incorporating popular widely-used apache spark distributed computing framework major contribution work lie integration neural network model distributed evolutionary approach specifically tailored big data classification neural network evolutionary algorithm extensively studied independently research present novel approach combining two domain enhance accuracy scalability classification task involving large datasets using apache spark framework literature review neural network powerful tool solving complex system due ability learn recognize pattern data training large datasets neural network approximate complex function make prediction optimize process detect anomaly versatility capacity learning make valuable wide range application including pattern recognition forecasting optimization control complex system numerous existing method available effectively model complex system iqbal conducted analysis computer virus epidemic model using fractional order differential equation iqbal also conducted analysis stochastic form newell–whitehead–segel equation kazeem utilized exponential matrix algorithm differential transformation algorithm runge–kutta method simulate temperature distribution heating tank liaqat introduces conformable shehu transform decomposition method cstdm novel algorithm solving quantum mechanical model high accuracy efficiency shahzad investigated fluid flow double disk considering various boundary condition incorporating effect microorganism thermal parameter another study present mathematical model non-integer order fractal fractional caputo operator analyze development ebola virus infection also numerous existing work neural network utilized analyze understand complex system basma introduces numerical performance fractional kind food supply fkfs model using fractional derivative stochastic scaled conjugate gradient neural network sabir introduced stochastic solver based levenberg-marquardt backpropagation neural network lmbnns nonlinear host-vector-predator model covid-19 spreading model investigated using artificial neural network levenberg-marquardt backpropagation training umar another work umar introduces numerical computing technique using artificial neural network optimized particle swarm optimization active-set algorithm solve nonlinear corneal shape model neuroevolution term used refer process applying evolutionary algorithm optimization neural network recent year garnered attention researcher possibility integrating search power evolutionary computation learning capability artificial neural network ann variety application traditionally backpropagation common algorithm used train multilayer feed-forward neural network mean reducing network error employ gradient descent rule however certain constraint since excels mainly exploiting existing solution tends converge local optimum might lead subpar classification accuracy also issue convergence speed scalability researcher increasingly adopting metaheuristic algorithm global search capability build optimum weight bias anns since surpass limitation conventional approach form search heuristic take cue darwin theory evolution natural selection strongest adaptable survive pas gene future generation liu suggested hybrid approach employing improve mlp neural network mlpnn parameter singh developed mlp-ga-based approach incoming traffic year intention detecting application layer ddos attack strategy optimizing hyper-parameters mlpnn proposed itano sousa hernandez ecer put forward approach predict stock price movement direction integrated mlp methodology multilayer perceptron-genetic algorithm mlp-ga multilayer-particle swarm optimization mlp-pso training mlp comparing finding obtained via carried training ann algorithm computationally expensive task thus limiting usage processing big complex problem therefore desirable implement anns parallel distributed platform boost performance calvert proposed technique anticipate evaluate performance distributed ann algorithm looking well handle relatively straightforward mathematical operation required build network improve accuracy identifying student learning disability suggested parallelizing optimizing genetic-based ann classifier casas suggested method parallelizing backpropagation algorithm used train network prediction index parallelizing backpropagation technique operate four processor concurrently reduced training time gonzalez proposed multi-step ahead tsf time series forecasting using fully automatic evolutionary ann eann system using two parallel programming standard message passing interface mpi open multi-processing openmp study distributed processing framework ga-evolved neural network classifier explored well effectiveness framework big data classification problem comprehensive review recent advance ann machine-learning algorithm found telikani distributed evolutionary neural network overall architecture study proposes metaheuristic method train neural network fig used determine suboptimal value weight coefficient bias artificial neural network input layer two hidden layer output layer ann model total four layer distributed architecture employed train neural network improve model effectiveness handling large datasets figure overall architecture proposed distributed evolutionary neural network full size image training neural network weight training ann using entail three stage first step determine chromosome representation connection weight population second step evaluate fitness connection weight testing efficacy weight ann computing mean square error third step apply evolutionary process includes selection crossover mutation evolution cease fitness exceeds specified value population converges genetic learning ann weight described using following stage chromosome population represents weight matrix neural network population initialized way chromosome initialized random set integer gene population size size chromosome initialized hyper-parameters evaluate set connection weight passing feed-forward ann model make prediction compute total mean square error predicted target output total mse equation used estimate fitness individual chromosome aligned mse i=1 -\hat aligned iii next phase based fitness value best chromosome selected parent reproduction new population aid model converge move optimal solution selection method based roulette wheel adopted population current generation represented roulette wheel chromosome occupying slot proportion fitness crossover and/or mutation operator applied parent chromosome generate offspring increase genetic variability forming next generation study single-point crossover random combination point chosen parent chromosome chromosomal segment combination location exchanged producing two new offspring applying crossover mutation applied randomly selected chromosome based mutation rate stopping condition met fittest individual population returned otherwise evolution continues distributing genetic operator using map reduce section describes distribution genetic operator weight training ann depicted fig data split training testing purpose proposed apache spark utilized distribute distributed generates initial population solution random distributes rdd across multiple partition using parallelize method initialized population parallelized populationrdd divided segment segment assigned separate node distributed environment processing mapping parallel population segment fitness function segment population evaluated parallel fitness particle population calculated using mean-squared error fitness value chromosome evaluated different worker chuck population particular chromosome resides following driver program collect result performs genetic operator context spark driver execution parallelize method spark used convert initial population populationrdd comprises pair chromosome identifier corresponding chromosome chromosome_id chromosome training data partitioned across node using parallelize method map transformation spark executed map evaluatefitness applied populationrdd training data compute fitness score chromosome population turn populationrdd fitnessvaluerdd contains pair fitness chromosome entry training data distributed across node different partition driver program executes evaluatefitness function cluster parallel different worker node compute fitness value partition training data evaluation phase concludes collect operation initiate collection pair driver next fitnessvaluerdd parallelized using parallelize function spark divided subpopulation mappartitions geneticevolution transformation invoked geneticevolution function performed parallel worker node contains different partition fitnessvaluerdd function geneticevolution three genetic operator applied selection crossover mutation function evolutionrdd produced end contains evolved chromosome chromosome_id chromosome pair selection roulette wheel selection mechanism used select best individual reproduction next generation crossover selected best individual selection reproduction produce better offspring mutation performed produced offspring model converges solution faster perform crossover selected chromosome sampled stored two even list rdds parallelized using parallelize function key-value pair two random chromosome two list formed map function used perform single-point crossover key-value pair chromosome chromosome one one crossoverrdd produced crossover complete gene locus randomly selected chromosome traversed using map mutation factor specified hyperparameter negation operation done gene locus produce new chromosome result mutation evolvedrdd generated saved system memory genetic operator applied evolvedrdd produced map .best_chromo transformation applied convert evolvedrdd bestchromosomerdd key value pair key chromosome_id value fitness value fittest chromosome partition last collect method compiles top performer worker determine optimal solution used train feed-forward neural network coordinated learning process generates numerous potential solution issue hand refines course several generation every solution contains parameter might contribute producing better result applied ann weight layer contribute achieving high accuracy result single solution obtained using contain weight used ann ann model consists total four layer comprised input layer two hidden layer output layer implementation tool experiment carried high-performance hadoop cluster consisting one name node server two data node server total 768gb ram core processor apache spark 3.3.0 supported cluster apache spark free open-source distributed data processing engine scalable rapid allows massive data processing established berkeley permit distributed application developer program java python scala pyspark python api doe provide python apis needed develop spark application also give access interactive pyspark shell analyzing data distributed environment result discussion benchmark datasets three datasets hepmass susy higgs used evaluate performance proposed distributed evolutionary neural network datasets obtained uci machine learning repository hepmass data set contains sophisticated physic experiment designed search exotic particle well bi-classification task susy data discriminate signal process generates super-symmetric particle background activity doe correspond occurrence data referred higgs sample signal used ass consistent emission higgs boson data set property displayed table table detail datasets full size table performance evaluation proposed model existing model performance model evaluated using various performance metric performance measure considered accuracy area roc accuracy degree projected value closely match actual value outcome data point might true positive label prediction positive false positive label negative prediction positive true negative label prediction negative false negative label positive prediction negative accuracy metric described follows aligned tp+tn tp+tn+fp+fn aligned table depicts comparison accuracy value obtained ga-based ann work normal mode proposed distributed evolutionary neural network table accuracy comparison full size table ga-ann denn give comparable result term accuracy value denn producing slightly better performance observed effect distributed environment doe negative impact performance accuracy rather lead marginal improvement accuracy value addition measuring accuracy area roc receiver operating characteristic curve assessed well receiver operating characteristic roc plot compare true positive rate tpr false positive rate fpr various threshold classifying data area receiver operating characteristic curve roc metric depends effectively classifier differentiate two binary class area value varies greater roc area accurate prediction figure depicts result roc curve area roc curve value algorithm consideration figure comparison roc curve susy dataset alogrithms consideration full size image figure comparison roc curve hepmass dataset alogrithms consideration full size image figure comparison roc curve higgs dataset alogrithms consideration full size image case accuracy observed marginal improvement case area roc curve metric value depicts distribution doe considerably impact accuracy auc measure area roc value usually greater 0.65 indicates classification confidence impact training time taking consideration execution time observed use distributed environment denn accelerates learning performance classifier datasets considered compared ga-ann executed normal mode table result obtained whole dataset considered classification table time taken comparison second full size table similar result also seen table distributed environment also improves speed around every scenario partial training data except number row smaller number instance overhead distributing dataset higher hence significant improvement execution time observed table training time comparison partial training data second full size table speedup trend another metric used evaluate performance proposed model speedup increase speed parallel algorithm relative serial equivalent known speedup essential method determining efficiency parallel processing impact parallelization assuming duration serial algorithm single node duration parallel algorithm many node speedup represented using aligned =\frac aligned higher speed greater parallel efficiency performance figure depicts speedup trend observed datasets taken consideration susy hepmass higgs increasing number core figure speedup trend three datasets considered increasing number core full size image observed four core speedup trend similar three datasets four core hepmass dataset highest speedup factor followed higgs susy datasets scalability analysis scalability also used evaluate performance proposed model scalability ability system improve performance number slave rise employing parallel technique consumption rate cluster displayed aligned j=\frac aligned equation scalability speedup denoted number slave positive integer usually equal one could greater sometimes one scalability improves approach one parallel program scalability curve exhibit diminishing trend number slave rise beyond ideal limit table represents performance analysis proposed model increasing number core table performance analysis proposed model increasing number core full size table figure depicts roc curve obtained susy hepmass higgs dataset respect increasing number core roc receiver operating characteristic curve evaluation metric commonly used classification task ass performance model distinguishing different class metric measure model ability correctly classify instance across various threshold value figure roc curve susy dataset increasing number core full size image figure roc curve hepmass dataset increasing number core full size image figure roc curve higgs dataset increasing number core full size image figure illustrates scalability proposed model showcasing improves number core increase graph clearly indicates scalability continues increase six core utilized however beyond point scalability trend begin decline decline occurs overhead distributing model outweighs benefit gained increased efficiency distributed processing essence graph demonstrates optimal point increase number core may yield significant scalability improvement due associated distribution overhead figure scalability analysis model full size image conclusion in-depth discussion benefit significant drawback ann evolved using provided paper distributed model adopted train neural network accuracy auc roc time taken speedup scalability taken measure evaluate performance model discovered accuracy auc roc considerably improve algorithm executed distributed mode still par traditional method happens due similarity implementation proposed method traditional method increasing number node affect computation time required change algorithm performed distributed used prominent improvement execution time execution time improved almost case datasets speedup scalability trend tend increase number core used distribute model increase optimum value speedup scalability doe show considerable improvement due distribution overhead optimum number node identified proposed model node proposed method proved utilization distributed paradigm significantly improved speedup scalability also adapted many learning algorithm bigdata