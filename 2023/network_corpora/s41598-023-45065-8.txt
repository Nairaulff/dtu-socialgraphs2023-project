introduction low-light image enhancement studied many year important application area night-time video surveillance autonomous vehicle therefore using low-light image enhancement algorithm restore low-light image normal-light image provides solid foundation subsequent high-level vision task object detection object tracking semantic segmentation time low-light image enhancement technology also indispensable field military security deep-sea exploration traditional method low-light image enhancement typically based histogram equalization retinex-based approach method effect increasing brightness low-light image often suffer over-enhancement detail loss well excessive noise color distortion due reduction grayscale level scene complexity unstable prior knowledge extraction improvement computer hardware technology speed data processing greatly increased many deep learning-based method shown good performance field low-light image enhancement currently low-light image enhancement method based convolutional neural network cnns learn mapping relationship low-light image normal-light image large amount data carefully designed cnn structure however limited receptive field convolution operation cnns fully consider long-distance pixel relationship input image affect image enhancement effect self-attention mechanism transformer solve problem self-attention mechanism model long-range dependency better preserve image detail reduce impact noise thereby improving quality image transformer-based method made important progress low-level vision task image super-resolution image denoising image dehazing currently related transformer method also applied low-light image enhancement achieved good performance better model non-local information achieve high-quality image reconstruction however method enhance local feature image well cnns excel therefore recent researchr attempted combine cnn transformer network combine advantage improve performance corresponding task low-light enhancement task network architecture design need adapted characteristic low-light image low-light feature high-light feature time low-light enhancement task real scene zero-shot learning method needed better solve high-level vision task real scene paired datasets lacking specifically zero-shot learning mean paired unpaired data needed training substantial contribution study meticulously designed combat issue uneven illumination transformer armed global attention mechanism comprehensively process long-range pixel relation input image however traditional self-attention mechanism demand high quantity computational resource multitude parameter could lead overfitting hand cnn network well-regarded enhancing local feature maintaining robustness still struggle capturing global context information integration two network without thoughtful design cnn network could lead ineffective learning global information feature generated transformer network aiming unite advantage cnn local feature extraction transformer global modeling network introduced study come specific improvement complexity transformer module increase linearly quadratically rise image resolution facilitating efficient acquisition contextual information cnn module class transformer structure designed concentrate better feature extracted transformer making difficulty global information acquisition thus enhancing model efficiency ablation experiment conducted development process multiple combination tested finalizing network architecture presented paper particularly channel attention mechanism auxiliary module multi-dconv head sparse attention mdsa module designed research address extent issue high time space complexity inherent traditional transformer introduction sparse attention mechanism provides deeper understanding handling local feature image low-light enhancement task overly bright local feature may hinder model ability capture critical low-light feature mitigate problem mdsa module adopted precise depiction local feature boost enhancement ability marking first application improved sparse attention mechanism low-light enhancement task figure illustrates unevenly lit low-light environment conventional self-attention mechanism ordinary sparse self-attention mechanism tend place primary focus weight highlight feature ideal low-light enhancement task sparse self-attention mechanism applied study properly bias main weight towards low-light feature effectively reducing weight highlight feature significantly improving model performance low-light enhancement task method unexplored original methodology represents innovative thinking figure depicts handling strategy different attention mechanism condition unevenly lit low-light environment traditional self-attention mechanism generally prefers place main focus highlight feature furthermore conventional sparse self-attention mechanism tends concentrate significant portion weight highlight feature approach ideal low-light enhancement task result tendency overexposure highlight area inhibiting sufficient enhancement detail low-light area however proposed sparse self-attention mechanism break away norm capable appropriately shifting majority weight towards low-light feature simultaneously effectively reducing weight highlight feature facilitates balanced extraction processing feature full size image among two input cross gating feedforward network cgfn module one processed mdsa module bypass mdsa module implement sparse attention mechanism channel dimension therefore proposed cgfn calculates weight spatial dimension addressing lack spatial information feature pass mdsa module additionally presence gating mechanism better suppress propagation information feature unfavorable model convergence low-light enhancement task feature information highlight area severely hamper enhancement quality cgfn module alleviate problem introducing method previously seen methodology therefore considering characteristic low-light image uneven lighting article proposes effective zero-shot learning low-light enhancement network structure main contribution summarized follows zero-shot learning low-light enhancement network named cui-net designed entire network comprises enhancement module auxiliary module enhancement module merges global attention mechanism transformer ability cnn network process local feature efficient computing efficiency powerful modeling capability unique structure enables better handling problem uneven lighting richer feature information extraction achievement image enhancement low-light environment cnn network auxiliary module augments convergence ability enhancement module indirectly rectifies influence lighting multi-dconv head sparse attention mdsa module designed mdsa module constrains highlight feature channel level increase weight important local feature design help quell interference overly bright feature allowing model focus extract low-light feature better thereby enhancing model performance low-light enhancement task novel cross gating feedforward network cgfn proposed cgfn effectively suppress spread information feature conducive model convergence also supplement information loss spatial dimension information exchange thereby boosting efficiency effect model low-light enhancement task feature information highlight area seriously disrupt enhancement quality low-light enhancement task existence cgfn module mitigate problem multitude experiment conducted nine challenging datasets experimental result indicate cui-net surpasses current state-of-the-art method term image quality enhancement effect various evaluation indicator importantly cui-net superior performance high-level visual task object detection face detection semantic segmentation real-world low-light scenario validates practical value effectiveness related work traditional enhancement method traditional low-light enhancement method primarily divided two type method based histogram equalization method based retinex model method based redistribute pixel value based cumulative distribution function input image expand dynamic range however method also prone color fidelity loss generation noise resulting image distortion retinex theory decomposes low-light image reflectance part illumination part based prior knowledge regularization single scale retinex model ssr multi-scale retinex model msr msr considered weighted sum several different ssr output output method may cause change relative proportion enhanced three color channel affected compared original image lead color distortion proposed fusion method combine advantage sigmoid function histogram equalization improved performance compared guo initialized illumination map image finding maximum value rgb channel optimized initial illumination map adding structural prior achieve image enhancement method effect increasing brightness low-light image however algorithm ignore correlation bright dark part resulting color distortion image significant brightness difference deep learning-based method network-based low-light image enhancement algorithm based cnn method cnn-based method method based retinex usually enhance illumination reflection component separately dedicated sub-networks wei introduced retinex-net model aim enhance low-light image model comprises two part decom-net decomposing image illumination reflection component enhance-net adjusting illumination despite purpose retinex-net unfortunately result significant color distortion leading natural-looking enhanced image enlightengan generative adversarial network gan used u-net based attention mechanism generator global-local discriminator obtain enhancement result zerodce train lightweight network dce-net fit brightness mapping curve used adjust brightness distribution image retinex- inspired unrolling architecture search ruas unfolding architecture search handle low-light image enhancement self-calibrated illumination sci proposes simplified network fit physical principle achieve low-light enhancement introduces calibration process training stage improve low-light enhancement model ability thereby improving enhancement effect method combining cnn transformer cnn operation provide efficiency universality receptive field limited fully consider long-range pixel relationship input image affect image enhancement performance contrast transformer self-attention mechanism focus modeling long-range dependency enabling capture global information well however lack attention relevant information complexity grows exponentially spatial resolution leading poor performance task thus combining two effectively improve image enhancement quality focus paper conformer cnn branch transformer branch combine feature coupling unit fuse local convolution block self-attention module mlp unit adjust feature resolution channel number continually eliminating semantic difference cnn transformer branch hnct integrates cnn transformer using local non-local prior extract feature beneficial super-resolution enhanced spatial attention module improve performance ecfan proposes new hybrid super-resolution method called act combine cnn vision transformer effectively aggregate local non-local feature introduces cross-scale token attention module effectively utilize multi-scale token representation careful consideration experimental comparison found method three transformerblocks encoder preserve useful self-attention value avoiding propagation aggregated highlight feature allowing useful global feature fully utilized transmitting useful local feature ensure enhanced low-light image sufficient detail two cnn block serve decoder utilize feature information obtained transformer block better enhance detail texture information low-light image leveraging advantage cnn network sparse attention image captured real-world scenario often suffer uneven illumination example image taken night may contain dark bright area overexposed region area around light source existing method often enhance dark bright region image simultaneously affect visual quality enhancement result however current low-light image enhancement method fully addressed open problem zhao proposed sparse transformer select attention degree model proposed target focus network sparse transformer technique visual object tracking target focus network focus target interest search region highlight feature relevant information better estimating state target inspired sparsett adapt sparse transformer low-light enhancement task low-light image uneven illumination transformer susceptible influence high-light feature computing self-attention resulting higher attention value naturally lead bias towards enhancing high-light feature rather low-light feature low attention value modeling global feature dependency therefore propose sparse attention operation differs usual one choosing set high-light feature lower value effectively suppress high-light information focus relevant information low-light enhancement task proposed method section framework cui-net two main module enhancement module introduced auxiliary module finally explain unsupervised training loss used neural network model overall procedure proposed cui-net cascaded two-stage image enhancement network fig first stage transformer network introduced obtain global information better enhance detail low-light image second stage auxiliary network based multiple convolutional network block constructed original input image used constraint control output detail feature first stage unlike traditional method training part cui-net requires multiple enhancement module auxiliary module testing part contains enhancement module figure overall framework cui-net one enhancement module used obtain result testing phase full size image assume low-light input image i\in h\times w\times height width number channel rgb image equal according retinex theory low-light image obtained performing following operation clear image illumination image aligned r\otimes aligned therefore enhanced image obtained input image illumination map training process entire framework divided two part namely enhancement module auxiliary module aligned t-1 t-1 aligned aligned i\oslash aligned em_ -th image enhancement module network learnable parameter am_t\ -th auxiliary module network learnable parameter t=1\ i.e. em_1\ original low-light image used input i.e em_1 original low-light image added input unlike training part auxiliary module needed testing part one enhancement module used obtain clear image aligned i\oslash aligned image enhancement module image enhancement module consists efficient transformer block cnn block serving encoder decoder respectively transformer model enhances low-light image filtering information uneven lighting channel local detail transferring useful feature next part network core transformer block lie multi-dimensional sparse attention mdsa mechanism cross-gated feed-forward network cgfn mdsa effectively reduce redundant feature improve weight important feature thus enhancing network robustness generalization ability cross-gated mechanism compensate lack information spatial dimension allowing useful information propagate enhance integrity entire feature representation cnn block replaces attention block traditional transformer network deep convolution feed-forward layer simplified cnn structure ensuring lightness meanwhile structure similar transformer network process feature information generality efficiency advantage convolutional neural network summary channel-wise sparse attention cross-gated transformer used encoder image enhancement module increase layer number extracted feature become increasingly abstract semantically rich cnn block used decoder extract enhance feature higher level making suitable image enhancement task uneven lighting condition realizing pixel-level information transfer context association convolution calculation improve performance efficiency model specific process image enhancement module shown fig network structure diagram transformer cnn module enhancement module shown fig first input low-light image undergoes 3\times convolutional operation extract low-level feature increase number channel pass three transformer encoders two cnn decoder residual connection upsampling downsampling operation utilized extract sufficient detail feature finally 3\times convolutional operation used restore original number channel resulting image added input low-light image produce final output image stand concatenation operation figure network architecture enhancement module full size image figure network structure diagram transformer module used enhancement module cnn module used enhancement auxiliary module full size image multi-dconv head sparse attention traditional transformer module multi-head self-attention mechanism compute global information self-attention mechanism spatial dimension resulting quadratic growth complexity increasing resolution main purpose sparse attention mechanism reduce time space complexity traditional transformer paper channel attention mechanism used mdsa module reduces model complexity improves efficiency also help model better understand local feature image low-light enhancement task appearance many high-brightness local feature may interfere model ability capture low-light feature therefore paper sparse attention mechanism assist model better representing local feature improving enhancement ability specific structure mdsa shown fig input tensor denoted i\in represent query key value 1\times point-wise convolution applied aggregate pixel-level cross-channel context followed 3\times depth-wise convolution encode channel-level spatial context operation figure stand reshape ap\ used filter weight attention map matrix weight topk matrix set corresponding weight attention map 0.01 figure network structure diagram mdsa module full size image different vision transformer model mdsa self-attention mechanism calculate similarity channel i.e. attention calculation performed channel dimension rather spatial dimension enables mdsa better capture relationship feature channel thereby improving model representation ability robustness specifically topk operation performed attention map select top attention value followed operation noted unlike general sparse attention calculation low-light task uneven illumination channel information high-light area attention map likely receive higher attention score attention need set 0.01 allow low-light channel feature sent cgfn obtaining required local information aligned w_pspattention aligned aligned spattention softmax topk aligned obtained reshaping original scale meaning spattention sparse attention w_p\ represents 1\times point-wise convolution learnable scaling parameter used control magnitude dot product cross-gated feed-forward network two input cross-gated feed-forward network cgfn input output obtained mdsa cross-gating part equivalent calculating weight spatial dimension weighting specific position order compensate lack spatial dimension information image passed mdsa specific structure cgfn shown fig single path cgfn module two branch one branch gating unit used obtain activation state pixel 1\times convolutional layer used expand channel number followed 3\times depthwise convolutional layer starrelu generate gate map branch doe need pas starrelu activation function two branch dot-multiplied cross-gating cross-calculated two path compensate lack spatial information input cgfn mdsa x\in y\in input previous module without msda cgfn represented follows figure network structure diagram cgfn module full size image aligned w_o^ w_m^ w_o^ w_m^ aligned aligned w_p^0g aligned aligned w_p^0g aligned aligned w_d^1 w_p^1 w_d^2 w_p^2 aligned denotes element-wise multiplication represents starrelu non-linear activation function stand layer normalization w_m\ performs softmax operation w_o\ performs dropout operation serve input next module auxiliary module auxiliary module necessary unsupervised image enhancement method may limitation over-enhancement color bias therefore cnn network high efficiency generalization ability chosen auxiliary module converge output multiple enhancement module one enhancement effect enabling use one enhancement module testing phase achieve enhancement effect multiple enhancement module training part shown fig formula purpose auxiliary module correct input enhancement module indirectly affecting output enhancement module input auxiliary module obtained element-wise addition output previous enhancement module output auxiliary module followed division original low-light image thus auxiliary module obtain feature enhancement module correct uneven illumination original low-light image auxiliary module depth-wise convolution multiple time effectively reduce number parameter computation cost shown fig firstly input image passed 3\times convolution layer increase channel number three cnn block finally 3\times convolution layer used reduce channel dimension shown fig cnn block enhances local detail passing input feature depth-wise convolution 3\times 5\times followed starrelu activation function multiple 1\times convolution minimize number parameter corrected illumination information inputted enhancement module improving enhancement effect enhancement module figure overall architecture diagram auxiliary module full size image training loss order consider color preservation artifact removal gradient backpropagation loss function need optimized loss function used cui-net follows aligned aligned represents total loss _c\ _c\ represent correction loss smoothness loss respectively two positive balancing parameter experiment balancing parameter set =1.5\ =1\ correction loss _c\ ensure consistency estimated illumination adjusted result aligned em_x am_ x-1 aligned em_x\ x-th enhancement module am_x\ x-th auxiliary module am_0\ original input unsupervised loss loss function constrains output auxiliary module smoothness loss used aligned j\in weight_ x_i^t x_j^t\mid aligned total number pixel -th pixel represents neighboring pixel window weight_ represents weight specified equation represents image channel yuv color space =0.1\ standard deviation gaussian kernel aligned weight_ exp -\frac +s_ t-1 +s_ t-1 2\sigma aligned experiment test effectiveness algorithm paper verifies multiple datasets task firstly experimental setting given test conducted public datasets demonstrate effectiveness algorithm quantitative comparison qualitative analysis existing method high-level task including low-light object detection dark face detection nighttime semantic segmentation tested compared existing algorithm validate effectiveness algorithm finally ablation experiment conducted verify effectiveness module experimental setting experiment based pytorch conducted computer intel i9-10940x cpu two rtx gpus 32gb memory training testing main parameter batch size initial learning rate 10^ weight decay =10^ training epoch enhancement module number transformer block set first layer fourth layer number attention head mdta set number channel set starrelu adan optimizer introduced cui-net starrelu variant squared relu designed eliminate distribution shift starrelu performs well algorithm performance computational efficiency due reducing computational cost activation function adan complete training vit half computational cost compared popular optimizer adam adan additional hyperparameter _2\ adjustment _2\ set 0.08 experiment em_x\ represents x-th enhancement module am_x\ represents x-th auxiliary module am_0\ represents original input unsupervised loss loss function constrains output auxiliary module verify effectiveness superiority proposed algorithm cui-net compared state-of-the-art sota method including enlightengan kind zerodce zerodce++ ruas sci uretinex-net additionally comparison made high-level vision task face detection object detection semantic segmentation benchmark description evaluation metric image enhancement testing random image mit dataset random image lsrw dataset used testing quantitatively measure algorithm performance three full-reference metric including psnr ssim lpips four no-reference metric including niqe ilniqe nima musiq used evaluation metric dark face detection task dark face dataset consisting challenging test image used random image selected training set image used testing average precision used evaluation metric low-light object detection task exdark dataset specifically designed low-light object detection used image selected training set image used testing evaluation metric including map_ 0.5:0.95 map_ 0.5 nighttime semantic segmentation task acdc dataset used acdc dataset self-driving dataset released iccv dark condition image used training remaining image used test set evaluation metric include iou miou quantitative qualitative metric quantitative result mit dataset shown table cui-net achieved best performance ssim psnr lpips ilniqe among seven evaluation metric specifically cui-net achieved psnr 193.328db 1.0259db higher best existing best algorithm score 18.3201db ilniqe evaluation metric score 31.9151 1.5756 lower score best existing algorithm table quantitative result three supervised metric psnr ssim lpips four no-reference metric niqe nima musiq ilniqe mit dataset full size table enhancement result mit dataset shown fig compared ground truth fig input low-light original image fig enlightengan fig kind fig zerodce fig sci fig uretinex fig method show inadequate enhancement zerodce++ fig show over-enhancement ruas fig enhances white petal upper part image pinkish color overall saturation high contrast cui-net fig show better color restoration maintaining realistic lighting condition figure enhancement result mit dataset enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net input low-light original image ground truth sequence number full size image quantitative result lsrw dataset shown table among seven evaluation metric cui-net achieved best result nima third-best result psnr niqe musiq uretinex achieved good result lsrw dataset may data augmentation method lsrw dataset similar lol dataset used supervised training uretinex however unsupervised method may sensitive artificially augmented datasets table quantitative result three supervised metric psnr ssim lpips four non-reference metric niqe nima musiq ilniqe lsrw dataset full size table enhancement result lsrw dataset shown fig except zerodce++ fig show over-enhancement overall enhancement effect enlightengan fig kind fig ruas fig zerodce fig zerodce++ fig sci fig uretinex fig cui-net fig method similar enlarging selected local area detailed comparison observed two part scene outdoor indoor scene observed separately ruas fig zerodce++ fig sci fig showed over-exposure outdoor scene uretinex fig achieved better quantitative result also showed over-exposure worth noting even ground truth fig show over-enhancement outdoor scene compared low-light original image fig since cui-net fig suppress highlight area uneven lighting condition better enhancement outdoor scene may always contribute evaluation metric indoor scene enlightengan fig kind fig zerodce fig resulted blurred text realistic surface reflection cui-net enhance detail contour low-light area also restore realistic lighting condition scene addition cui-net enhance text white paper paper box desk clearly may practical application low-light image text extraction task figure enhanced image lsrw dataset enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net input low-light image ground truth full size image figure detail corresponding enlarged area fig lsrw dataset enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net input low-light image ground truth full size image although cui-net shortcoming quantitative metric lsrw dataset qualitative analysis enhancement result show discrepancy relevant metric subjective observation practical application conducted training testing unpaired low-light enhancement datasets mef dicm lime qualitative result illustrated fig respectively observed method effectively prevents overexposure across four datasets achieves satisfactory enhancement detail restores realistic shadow lighting observed instance detail tabletop facial feature flower cluster door number cliff building figure test result display mef dataset enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net full size image figure test result display dataset enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net full size image figure test result display dicm dataset enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net full size image figure test result display dicm dataset enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net full size image quantitative result shown table table quantitative test result mef dataset full size table table quantitative test result dataset full size table table quantitative test result dicm dataset full size table table quantitative test result lime dataset full size table table observed method outperforms others term quantitative result unpaired low-light datasets demonstrating robustness approach dark face detection dsfd face detection framework utilized experiment adopts ssd network structure trained wider face dataset face detection experiment result different low-light enhancement method used input dsfd finally compared average precision different iou threshold test result shown table cui-net achieved highest value iou threshold 0.5 0.6 second-highest value iou threshold 0.7 table average precision iou 0.5 0.6 0.7 threshold full size table figure show detection result different method add low-light input image fig face detection result fig comparison lower right corner method result image corresponding magnified detail image seen iou threshold 0.5 ruas fig cui-net fig detect face area pointed arrow enlightengan fig kind fig zerodce fig zerodce++ fig sci fig uretinex fig failed detect face area pointed arrow however ruas serious overexposure detail ground seen clearly cui-net detect face also produce realistic enhancement effect better quantitative indicator sota method figure result dark face detection enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net unenhanced low-light image input result face detection directly unenhanced low-light input image full size image low-light object detection trained yolov3 model exdark object detection dataset tested exdark validation dataset yolov3 series object detection framework model pre-trained coco dataset unlike face detection experiment fine-tuned yolov3 pre-trained model object detection i.e. retrained object detection model evaluate enhancement effect method table show quantitative result among different method cui-net achieved best map value map_ 0.5:0.95 map_ 0.5 table quantitative result object detection exdark dataset full size table experimental result obtained performing object detection low-light image enhanced various sota algorithm baseline object detection directly unenhanced low-light image specific detection result object detection low-light image fig shown fig ruas fig zerodce++ fig uretinex fig cui-net fig recognize target enlightengan fig kind fig zerodce fig sci fig baseline fig detect target completely overall average confidence value ruas zerodce++ uretinex lower cui-net addition main reason ruas zerodce++ lower map value table due overexposure problem however cui-net found good balance able avoid overall lower map score caused overexposure figure experimental result object detection exdark dataset enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net unenhanced low-light image input result object detection directly unenhanced low-light input image full size image low-light semantic segmentation evaluated performance segmentation method acdc low-light semantic segmentation dataset using deeplab-v3+ model pre-training fine-tuning mode pre-trained model trained cityscape dataset table show miou value multiple category overall average among different low-light enhancement method cui-net achieved best miou score among six segmentation target second-best method among seven segmentation target outperformed second-best method 4.5 wall category 1.9 traffic light category 6.6 motorcycle category overall average miou value 2.8 higher second-best method table show miou value multiple category average among different low-light enhancement method full size table table show macc value multiple category average among different low-light enhancement method cui-net achieved highest macc value five segmentation target 12.7 higher second-best method motor category 22.9 higher rider category cui-net also obtained second-highest macc value four segmentation target overall macc value higher second-best method table show macc value multiple category average among different low-light enhancement method full size table figure show overlaid result semantic segmentation mask enhanced image acdc dataset overall ruas fig sci fig exhibited overexposure enlightengan fig kind fig zerodce fig zerodce++ fig uretinex fig cui-net fig method showed significant difference nighttime semantic segmentation application attention detail particularly important timely segmentation pedestrian traffic sign road avoid serious accident nighttime autonomous driving figure segmentation result acdc dataset enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net full size image local detailed semantic segmentation result method corresponding red box fig shown fig comparing ground truth fig first red box region contains two traffic sign enlightengan fig kind fig ruas fig zerodce++ fig uretinex fig failed segment traffic sign zerodce fig sci fig recognized left traffic sign however cui-net fig able recognize traffic sign middle red box region contains two pedestrian two traffic sign zerodce++ fig uretinex fig recognized traffic sign cui-net fig recognized additional pedestrian right red box region contains two pedestrian kind fig sci fig cui-net fig able segment pedestrian well addition pedestrian crossing category doe exist acdc dataset seen fig cui-net obvious enhancement effect may play role nighttime safety autonomous driving task clearly cui-net potential nighttime semantic segmentation task figure enlarged detail red box fig enlightengan kind ruas zerodce zerodce++ sci uretinex cui-net full size image figure left ground truth right image zoomed-in detail corresponding red area ground truth image full size image ablation study verify whether network structure enhancement module cui-net improve model enhancement ability conducted four ablation experiment lsrw dataset training testing evaluated quality enhanced image using ssim psnr lpips firstly verify whether adan starrelu accelerate convergence model choose train epoch result obtained shown table observed replacing gelu starrelu adam adan lead better result smaller number epoch table ablation experiment replacing gelu adam starrelu adan full size table secondly verify whether network structure design enhancement module effective replaced five module overall network full cnn module full transformer module three transformer module two cnn module cui-net experimental analysis result obtained shown table network structure cui-net achieve better performance table replacing five module used original cui-net different one full size table thirdly verify whether mdsa cgfn improve model enhancement ability selected mdta gdfn restormer ablation study result shown table mdsa cgfn improve performance model table ablation experiment conducted compare network module used transformer block cui-net mdta gdfn full size table finally ablation study conducted sparse attention operation channel mdsa module cui-net result shown table topk\_normal\ operation usual sparse attention operation attention weight except topk set zero contrast top\_cui\ operation used cui-net reduces attention weight channel obtained topk low value result ablation study indicate sparse attention channel used cui-net contributes achieving better enhancement result table perform ablation experiment comparing usual sparse attention mechanism sparse attention mechanism used cui-net network full size table conclusion paper propose cui-net framework consisting enhancement module auxiliary module achieve differential enhancement low-light highlight region low-light environment enhancement module efficient low-light enhancement transformer cnn network introduced enhance low-light image acquiring global pixel information auxiliary module lightweight cnn network designed assist enhancement module converge better correct lighting effect quantitative analysis qualitative comparison cui-net state-of-the-art low-light image enhancement method conducted two public low-light datasets demonstrating effectiveness proposed method furthermore practicality method verified high-level vision task namely low-light object detection dark face detection nighttime semantic segmentation