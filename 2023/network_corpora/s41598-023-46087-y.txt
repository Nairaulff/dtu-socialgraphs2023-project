introduction rapid development internet industry big data technology recommender system playing increasingly important role social network academic education e-commerce nowadays recommender system become indispensable part daily life online shopping next point-of-interest recommendation music recommendation video push according user historical behavioral data recommender system predict user rating item perform personalized recommendation help user quickly discover item interested improve user satisfaction therefore order provide better personalized recommendation service accurately predict user rating item boost recommendation becomes challenge problem solve issue researcher proposed variety item rating prediction method among rating prediction method based collaborative filtering one widely used method method based matrix factorization learning latent feature user item matrix model recommendation considering user rating item reflect interaction behavior explicit feature zhang obtained user item feature user–item rating information based deep matrix factorization however rapid growth number user item problem sparsity rating data unfortunately information extracted rating data limited consequently restricting recommendation performance compared rating data review information contains rich semantics reflect user satisfaction item quality function also indirectly express user preference item feature thus review-based item rating prediction attracted extensive attention researcher convmf deepconn d-attn narre daml etc method alleviate sparsity problem caused rating data review information thus obtain relatively accurate prediction rating recommendation however two major limitation follows expression ability user/item feature insufficient research d-attn daml etc. leverage word vector statically encoded word2vec glove resulting sparse feature representation insufficient semantics polysemy affect ability model extract user item feature moreover model convmf deepconn narre use convolutional neural network cnn extract user item feature review effectively extract long sequence text feature review thus accurately express user item feature limiting model performance influence feature interaction user item recommendation performance ignored example model deepconn d-attn narre daml etc. obtain prediction rating dot product factorization machine splicing user item feature feature interaction modelling method ignore different effect different feature interaction recommendation result furthermore useless feature interaction introduce noise thus reducing recommendation performance address issue paper proposed attentional factorization machine review-based user–item interaction recommendation specifically order better capture review-based user feature item feature first obtain embedding feature review pre-trained model roberta alleviates problem static word vector adapt polysemy combine bidirectional gate recurrent unit bigru attention network highlight important information review obtain user review embedding item review embedding furthermore obtained review embedding user item concatenated together input attentional factorization machine afm perform accurately rating prediction make recommendation main contribution paper summarized follows build enhanced framework user/item feature representation leverage roberta obtain embedding feature user/item review alleviate problem polysemy bigru attention network measure contribution embedding feature review obtain better expression ability user/item feature use afm learn user–item feature interaction distinguish importance different feature interaction alleviate effect noise may introduced useless feature interaction conduct comprehensive experiment five real-world datasets demonstrate proposed afmrui model outperforms state-of-the-art model remainder paper organized follows related work provide overview related work section proposed approach elaborates proposed afmrui model next evaluate effectiveness model analyze experimental result experiment finally conclusion present conclusion sketch direction future work related work embedding representation method review-based recommendation task word embedding representation method usually used express user item review embedding feature model convmf deepconn d-attn narre daml etc. use glove word2vec belonging static word vector model however obtained user/item review embedding feature change contextual semantics problem polysemy produced result dynamic word vector used solve problem example google proposed bidirectional encoder representation transformer bert dynamic word vector pre-trained model achieve excellent result natural language processing task recent research sifn u-bert use bert obtain review embedding representation large performance improvement rating prediction compared method using static word vector model based bert improved model roberta introduced inherits advantage bert also simplifies next sentence prediction task bert roberta retrained using new hyperparameters large new dataset allows model fully trained significant improvement performance end adopt roberta model mitigate problem polysemy user/item review encoding obtained word-level embedding representation review review-based recommendation method increase interactive information generated user various field various interactive information related user item e.g. review introduced recommender system improve performance next outline two review-based recommendation method review-based topic modeling recommendation method topic modeling approach first apply review recommender system mainly obtaining latent topic distribution review latent dirichlet allocation lda non-negative matrix factorization demonstrated usefulness review example proposed topic model-based model mainly obtained review-based feature lda-based extended model huang similarly obtained potential feature user yelp restaurant review dataset lda algorithm help restaurant operator understand customer preference since topic model based lda preserve word order information context information review ignored aiming problem lda algorithm bao proposed topicmf model used latent factor user item obtained matrix factorization correlate improve accuracy rating prediction ganu learned preference feature user review information used method based latent factor model lfm rating prediction however lfm model learn linear low-level feature conducive interactive learning among feature fusion layer method mentioned use bag-of-words-based topic model review processing preserve word order information well local context information contained review ignored shallow semantic information extracted however rich semantic information user/item review accurately captured research use roberta bigru model user review item review effectively obtain user item review embedding feature rich semantics review-based deep learning recommendation method recent year cnn widely used task review-based recommendation example kim first introduced cnn recommender system proposed convmf model however convmf model item review user rating training ignoring user review information problem zheng introduced deep parallel network framework deepconn alleviated problem convmf using two parallel cnn network model user review document item review document respectively considering different word different importance modeling user item seo introduced cnn dual local global attention learn review embedding user item perform rating prediction chen introduced neural attentional regression model review-level explanation used review-level attention mechanism assign different weight review making recommendation interpretable method use cnn encode review cnn-based method fail effectively extract feature review different length address problem tay learned feature representation user item using pointer word-level review-level based review information obtain important information review improve prediction result chen modeled dynamic preference user well item attribute gated recurrent unit gru sentence-level cnn improved interpretability proposed model according analysis review-based deep learning recommendation method superior performance compared topic-based modeling recommendation method model leverage bigru incorporate attention network measure importance review improve user/item feature representation feature interaction method feature interaction user item research traditional feature interaction method dot product fully connected factorization machine etc supervised learning method augment linear regression model incorporating feature interaction example multi-pointer co-attention network show obtain better result interaction model good interaction ability however traditional method model feature interaction fail distinguish importance different feature interaction therefore zhang proposed combination model deep neural network based factorization machine neural network model generated higher-order feature combination strengthened learning ability model feature however different sample weight different feature interaction also different word unimportant feature interaction reduce weight high-importance feature interaction increase weight end xiao improved recognizing importance different feature interaction introduced afm learn importance feature interaction attention mechanism alleviate problem reduced feature representation performance caused useless feature interaction inspired reference afmrui model adopt afm learn feature interaction user item obtain better feature representation distinguishing importance different feature interaction alleviate effect noise may introduced useless feature interaction proposed approach section first present problem definition recommendation task list key notation used work table elaborate model framework afmrui problem definition assume dataset contains user item well plentiful review corresponding rating sample dataset defined userid-itemid-review-rating quadruplet meaning user make review give corresponding rating item sample dataset obtain review set user review set item retrieving userid itemid work focus predicting user rating item based obtained corresponding review set user item define review-based recommendation task follows table key notation used paper full size table definition review-based recommendation task given review set _u\ user review set _i\ item task review-based recommendation predict user rating item make recommendation afmrui framework architecture proposed afmrui model shown fig afmrui model composed two parallel network similar structure namely user review network _u\ item review network _i\ review set _u\ user review set _i\ item given _u\ _i\ respectively input corresponding rating predicted item produced output make recommendation seen fig afmrui model consists four layer layer outlined follows figure illustration afmrui model full size image review embedding layer mainly used obtain embedding feature review set _\textit _i\ roberta sequence encoding layer mainly leverage bigru encode embedding feature review produced review embedding layer fully mine internal dependency among review embedding feature obtain corresponding hidden feature attention layer utilized obtain review embedding user item adaptively measuring weight hidden feature review model focus useful review improve feature expression ability user item rating prediction layer first concatenates review embedding user item obtained attention layer leverage afm learn user–item feature interaction predict user rating item make recommendation since _u\ _i\ differ input next take _u\ network example illustrate process detail note process described following subsection review embedding layer sequence encoding layer attention layer also applied _i\ network review embedding layer review embedding layer used obtain embedding feature review user review set _u\ roberta according requirement roberta original review _u\ need preprocessed achieve corresponding review embedding feature specifically first remove special character mathematical symbol punctuation mark review _u\ set obtained review unified maximum length combine review processed list get corresponding user review list _u\ furthermore set obtained review list user dataset fixed length represents user maximum number review input roberta length _u\ exceeds truncation operation performed get first review _u\ otherwise use zero vector filling operation roberta mapping get specified length afterwards insert special character beginning end review respectively fixed length processing obtain review list _u\ user denoted u_1 u_2 u_n subsequently review list _u\ need expressed form word-level embedding representation composed token embeddings segment embeddings position embeddings take review love album inspiring fun album user a2b2j5vs139vlm item b004l49k20 digital music dataset example figure show obtain word-level embedding representation review shown fig original review preprocessed input word-level embedding representation extract token embeddings segment embeddings position embeddings preprocessed review respectively add get word-level embedding representation review -th token preprocessed user review u_i word-level embedding representation denoted aligned =e_ token +e_ seg +e_ aligned token token embedding corresponding -th token u_i obtained mapping token 768-dimensional embedding seg represents segment embedding corresponding -th token u_i since preprocessed review considered sentence segment embedding word u_i shown segment embeddings fig segment embedding token review example _a\ position embedding represents result encoding position word u_i figure illustration obtain word-level embedding representation review full size image based processing obtain u_i word-level embedding representation u_i list _u\ represented aligned u_i e_0 e_1 e_j aligned operation preprocessed review _u\ obtain corresponding word-level embedding representation review represented u_1 u_2 u_n represents specified maximum number user review considering multi-head attention mechanism roberta effectively capture semantic information among token review mitigate problem polysemy user/item review therefore leverage roberta semantically encode obtained word-level embedding representation review specifically given word-level review embedding representation u_i input roberta obtain corresponding review embedding feature u_i denoted aligned u_i roberta u_i 1,2 aligned u_i fixed -dimensional semantic feature embedding feature review _u\ output roberta represented review embedding feature list denoted u_1 u_2 u_n sequence encoding layer sequence encoding layer used obtain corresponding hidden feature review order capture relationship among review embedding feature user use bigru proven successful practical application encode embedding feature review list way embedding feature review modeled forward backward direction fully mine internal dependency among review embedding feature obtain corresponding hidden feature specifically take list u_1 u_2 u_n input bigru obtain corresponding forward hidden feature backward hidden feature represented aligned u_i gru u_i aligned aligned u_i gru u_i aligned u_i represents forward hidden feature corresponding u_i gru represents forward processing u_1 u_n represents forward hidden feature corresponding correspondingly u_i represents backward hidden feature corresponding u_i gru represents backward processing u_n u_1 represents backward hidden feature corresponding concatenate u_i u_i review obtain corresponding hidden feature u_i represents hidden dimension gru u_i denoted aligned u_i u_i u_i aligned similarly obtain whole hidden feature corresponding list sequence coding layer denoted aligned u_1 u_2 u_n aligned attention layer considering review made user different item reflect different user preference introduce attention mechanism adaptively measure weight review hidden feature aggregate useful informative review form user review embedding specifically attention network take whole hidden feature input yield corresponding vector attention weight 1\times represented aligned soft _u^ aligned t_1 represents vector parameter t_1 weight matrix t_1 represents hidden unit number attention network soft used normalize attention weight vector dimension denotes degree user preference reflected review compute weighted sum multiplying attention weight vector whole hidden feature obtain user review vector denoted aligned aligned next used input fully connected layer obtain user review embedding represents latent dimension represented aligned aligned weight matrix fully connected layer bias term similarly _i\ network get item review embedding corresponding item review set _i\ rating prediction layer rating prediction layer goal predict user rating item based user review embedding item review embedding fact predicted user rating item actually kind user–item feature interaction however existing approach dot product effectively learn user–item feature interaction fail distinguish importance different feature interaction afm obtain accurate rating prediction distinguishing importance different feature interaction alleviate influence noise may introduced useless feature interaction therefore adopt afm learn user–item feature interaction obtain specifically concatenate joint vector x_1 x_2 given input afm output predicted rating ensures user–item feature interaction joint vector reflects different importance represented aligned w_0 w_i x_i ^\mathrm x_i x_j b_u b_i aligned w_0 denotes global bias term w_i weight primary term represents feature number joint vector represents weight vector rating prediction layer embedding vector corresponding certain dimension x_i similarly embedding vector corresponding certain dimension x_j size embedding vector b_u represents user bias term b_i represents item bias term represents element-wise product embedding vector represents attention weight calculated aligned aligned represents attention score feature interaction x_i x_j computed aligned x_i x_j aligned represents weight vector fully connected layer softmax output layer represents size hidden layer attention network afm represent weight matrix bias term respectively basis operation item recommendation performed according obtained predicted rating model learning squared loss function widely used rating prediction task recommender system adopt loss function defined aligned aligned represents training sample represents predicted rating sample represents real rating sample experiment section conduct experiment evaluate effectiveness proposed afmrui model five real-world datasets first introduce experimental setup including datasets preprocessing evaluation metric baseline method experimental configuration afterwards conduct performance comparison also demonstrate corresponding ablation study furthermore analyze effect different parameter performance afmrui discus impact different embedding representation method different feature interaction method model performance experimental setup datasets preprocessing evaluate afmrui model five real-world datasets different scale industry among four amazon datasets including digital music baby office product beauty contain real amazon review may july yelp dataset yelp challenge sample dataset includes userid itemid review rating etc moreover user dataset posted least five review corresponding item table show statistic five datasets table statistic five datasets full size table ensure model well trained sample five datasets need preprocessed according sample format described problem definition mainly use value four field mentioned sample dataset use panda tool preprocess original sample dataset extract four attribute including userid itemid user review item user rating item 1–5 point result every sample unified userid-itemid-review-rating quadruplet preprocessing facilitate input model training evaluation metric leverage mean square error mse mean absolute error mae evaluate performance different method two metric utilized measure accuracy rating prediction computing difference predicted actual rating lower mse mae value indicate higher accuracy model prediction formula calculating mse mae aligned mse aligned aligned mae aligned represents test sample represents number sample test set denotes predicted rating test sample real rating sample corresponding test dataset baseline method demonstrate effectiveness afmrui model select traditional recommendation model based matrix factorization nine model based neural network selected representative baseline method described follows atrix actorization method regression algorithm take rating data input obtains user item feature matrix factorization deep operative eural etworks deepconn model utilizes two parallel convolutional layer process review document user item respectively perform rating prediction show review information alleviate sparsity problem rating data ual att ention-based etwork d-attn model obtains review-based feature representation user item combining local global learning finally predicts rating using dot product trans formational neural net work transnets model add transform layer deepconn mainly transforms latent representation review user item feature predict rating eural ttentional egression model eview-level xplanations narre model learns user item feature using cnn attention mechanism lfm rating prediction ulti- ointer o-attention etworks mpcn model pointer network learn user item feature review rating prediction ual ttention utual earning daml model utilizes local mutual attention cnn jointly learn user item feature review neural factorization machine introduced predict rating eural ollaborative mbedding odel ncem model utilizes aspect-level attention layer measure correlation degree review towards different aspect multi-layer neural factorization machine introduced predict rating ross-domain recommendation framework via spect ransfer etwork catn model learns aspect level feature user item corresponding review attention mechanism semantic matching performed aspect level feature predict rating atch yramid ecommender ystem mprs model cnn architecture fed matching matrix corresponding review pair user–item regression layer introduced predict rating configuration experiment code written python 3.8 tensorflow 1.15.5 utilized framework experiment conducted linux server intel xeon gold cpu rtx gpu randomly divided dataset used experiment training set validation set test set according proportion 8:1:1 furthermore selected parameter validation set performed evaluation test set setting parameter described follows method latent dimension user item uniformly set deepconn d-attn transnets narre mpcn daml ncem catn mprs set parameter method based setting strategy corresponding paper specifically learning rate 0.002 dropout set 0.1 0.3 0.5 0.7 0.9 batch size set find best parameter embedding dimension set narre daml model d-attn narre daml ncem catn model dimension attention score vector set deepconn transnets narre catn mprs model cnn used process review size convolution kernel set number convolution kernel set word vector model adopted glove embedding dimension ncem version bert bert-base note used model latent dimension set proposed model afmrui carefully tested batch size looked optimal value learning rate 0.0001 0.0005 0.001 0.005 dataset prevent overfitting turned dropout 0.1 0.3 0.5 0.7 0.9 batch size set learning rate set 0.001 dropout set 0.3 adam used optimizer unified maximum length review set version roberta roberta-base subsequently add fully connected layer compress semantic feature dimension hidden unit number t_1 set attention layer size embedding vector set rating prediction layer parameter determined optimizing mse mae validation set dataset result discussion comparison model performance subsection compare performance eleven method five datasets table show result best-performing one highlighted bold table make following observation table performance comparison five datasets mean std full size table first proposed model afmrui outperforms model term mse mae five datasets notably compared best baseline method mprs afmrui enhances performance digital music dataset approximately 3.7 mse 2.1 mae similarly high performance gain observed four datasets result demonstrate superiority model second method utilizing review information generally work better consider rating data clear deepconn d-attn transnets narre mpcn daml ncem catn mprs afmrui perform better term mse mae five datasets performance improvement method may due leveraging neural network rating prediction using review information effectively capture user/item feature review information reduce effect data sparsity due using rating data therefore method utilizing review information gain pure improvement compared third proposed afmrui model performs better nine baseline model leveraging review information five datasets reason model roberta capture global context mitigate problem polysemy user/item review accurately understanding review information guaranteed moreover model afm rather dot product learn different feature interaction distinguish importance different feature interaction also alleviate effect noise may introduced useless feature interaction afmrui achieves better performance five datasets addition eleven method also provide order magnitude approximate model parameter comparison shown second column table represents million comparison result table show ten deep learning-based method parameter compared mainly due fact deep learning model usually contain multi-layer neural network layer contains large number parameter ncem afmrui much model parameter compared eight deep learning-based method mainly method use pre-trained model encode review pre-trained model need learn lot linguistic knowledge law stronger expression generalization ability compared ncem afmrui model parameter mainly model leverage pre-trained model roberta made improvement model structure optimization algorithm basis bert used ncem thus requiring parameter ncem effectiveness different component subsection performed ablation experiment analyze effect different component model performance order validate benefit brought component construct following variant afmrui based basic model afmrui-base static word vector model glove represent user/item review embedding feature predicts user rating item afmrui-ro model roberta instead glove obtain user/item review embedding feature basis afmrui-base variant model verify roberta better glove extracting review embedding feature afmrui-bi model bigru added basis afmrui-ro encode user/item review embedding feature output roberta variant model verify effectiveness bigru afmrui-att model add attention network basis review-bi variant model verify effectiveness attention network measuring contribution review user/item feature representation table comparison model different component full size table table effectiveness different component five datasets full size table table show model different component take two metric demonstrate effectiveness model table five datasets result shown table seen table model performance afmrui-ro improved compared basic model indicating using roberta obtain context-related user/item review embedding feature alleviate problem polysemy effectively enhance feature representation compared afmrui-ro afmrui-bi performs better mainly bigru suitable dealing sequence problem fully exploit internal dependency among review performance afmrui-bi worse afmrui-att attention network introduced adaptively measure importance review user/item feature representation enabling model focus useful review contrast performance proposed afmrui model better four variant model show afm better learn feature interaction user item obtain accurate prediction rating also demonstrates integrating component help better model review feature user item improve model performance effect parameter section analyzed effect different model parameter performance afmrui focused five critical parameter namely maximum number user review item review semantic feature dimension gru hidden dimension latent dimension next analyzed effect five parameter two metric effect maximum number review proposed afmrui model performs rating prediction based user review item review therefore maximum number user review item review directly affect feature representation user item thereby affecting model performance considering different datasets different number review different user different item make statistic number user review item review five datasets determine range maximum number review shown table table statistic review five datasets full size table take digital music dataset second row table example user review accounting 80.29 total number user item review accounting 81.05 total number item according statistical result considering noise introduced number review large effective information extracted number review small set range maximum number user review range maximum number item review similarly set range maximum number review four datasets keeping hyper-parameters unchanged figure show result five datasets since result mae similar mse take mse example analyze effect parameter model performance shown fig digital music dataset increase mse decrease first increase number review large noise may introduced affect feature representation user item number review small accurately express feature representation user item therefore set maximum number user review set maximum number item review get best performance digital music dataset similarly maximum number user review item review set baby dataset respectively office product dataset beauty dataset yelp according analysis select value corresponding maximum number user review item review five datasets figure effect maximum number user review item review model performance full size image effect semantic feature dimension order investigate sensitive afmrui semantic feature dimension fixed dimension review embedding feature output roberta obtained corresponding review embedding feature different semantic feature dimension fully connected layer compression demonstrated effect five datasets fig shown fig five datasets increase model performance gradually improved model performance reach best begin decline moreover computational cost also increasing therefore set semantic feature dimension get best performance five datasets figure effect semantic feature dimension model performance full size image figure effect gru hidden dimension model performance full size image figure effect latent dimension model performance full size image effect gru hidden dimension illustrate effect gru hidden dimension set value keeping hyper-parameters unchanged figure show result five datasets curve show trend falling first rising five datasets maybe gru hidden dimension small fully mine internal dependency among review embedding feature gru hidden dimension large make model over-fitting therefore similar selection semantic dimension set gru hidden dimension get best performance five datasets effect latent dimension subsection investigate impact latent dimension model performance keeping parameter unchanged result presented fig observe increase mse mae first decrease digital music baby beauty yelp datasets reach best increase thereafter office product dataset mse mae reach best small value may lead model unable capture potential information user item review large value may cause over-fitting increase model complexity therefore set office product dataset four datasets comparison different embedding representation method section discus impact different embedding representation method model performance select classical algorithm deepconn best baseline method mprs different embedding representation shown table mainly discus nine comparison method table effect different embedding representation method model performance full size table experimental result reported table show proposed model afmrui outperforms variant afmrui-glove afmrui-bert-base term mse mae five datasets specifically yelp dataset afmrui improves performance approximately 3.8 mse 3.5 mae compared afmrui-glove relative performance improvement 1.5 mse 1.1 mae compared afmrui-bert-base four datasets show similarly high performance gain result essentially demonstrate competitiveness proposed model using roberta obtain context-related user/item review embedding feature alleviate problem polysemy effectively enhance feature representation addition also compared deepconn mprs variant model experimental result show deepconn-bert-base deepconn-roberta-base outperform deepconn-glove mprs-bert-base mprs-roberta-base outperform mprs-glove mainly traditional word vector model rely before-and-after review information review set efficient representation user item however bert-base roberta-base alleviate problem whereas deepconn-roberta-base outperforms deepconn-bert-base mprs-roberta-base outperforms mprs-bert-base mainly roberta-base inherits advantage bert-base also new hyperparameters new large dataset retraining doe alleviate problem multiple meaning word review also better model global information semantic representation user item review resulting accurate predictive score better model performance comparison different feature interaction method section discus impact different feature interaction method model performance mainly discus following three method table effect different feature interaction method model performance full size table afmrui-dp method conduct dot product operation user review embedding item review embedding predict rating afmrui-fm approach encodes vector formed concatenating user item review embeddings afmrui proposed method afm learn feature interaction user item perform rating prediction table show result five datasets seen table afmrui-dp experience performance decrease compared afmrui-fm afmrui five datasets whereas afmrui best performance dot product operation used afmrui-dp fully explore complex internal structure joint vector user review embedding item review embedding advantage dot product operation capture feature interaction two dimension joint vector user review embedding item review embedding thus performance afmrui-fm better afmrui-dp compared afmrui-fm afmrui model effective afm model add attention mechanism basis distinguish importance different feature interaction alleviate effect noise possibly introduced useless feature interaction obtain accurate prediction rating improve model performance table user–item feature interaction type full size table figure attention score feature interaction different type full size image basis analysis order explore contribution different feature interaction afmrui model intuitively use digital music dataset example demonstrate contribution different feature interaction since afmrui model achieves best result digital music dataset number latent dimension dimension user review embedding item review embedding set dimension vector stitched together i.e. x_1 defined user interaction object defined item interaction object three type feature interaction vector shown table user–item feature interaction e.g. x_1\ formed taking random dimension repeatedly select different user–item feature interaction feature interaction type similarly obtain different feature interaction two type respectively attention score feature interaction shown fig shown fig lighter color lower attention score contributes model performance vice versa specifically feature interaction type adopted model deepconn transnets achieved good result indicating user–item feature interaction beneficial quality rating prediction however according fig seen attention score feature interaction stable 0.2 0.5 indicating user–item feature interaction positive impact rating prediction type higher attention score mainly range 0.5–0.9 indicating although interaction object feature interaction important positive impact model performance resulting accurate prediction user rating item thus provide better recommendation summary seen different feature interaction different attention score different impact model performance afm adopted model distinguish importance different feature interaction obtained attention score thereby alleviating impact useless feature interaction model performance conclusion recent year review-based recommendation one hot research topic recommender system paper proposed afmrui model recommendation specifically afmrui leverage roberta mitigate problem polysemy user/item review learns review embedding user item bigru attention network better model user review embedding item review embedding utilizes afm learn user–item feature interaction obtain accurate prediction rating distinguishing importance different feature interaction extensive experiment five publicly available datasets demonstrated proposed afmrui model outperforms state-of the-art method regarding two metric paper leverage review information extract user item feature recently study shown user–item interaction graph additional useful information promote recommendation therefore future work combine review information user–item interaction graph capture accurate feature user item provide better model performance