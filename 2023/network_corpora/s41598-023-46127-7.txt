introduction customer valuable resource present main reason industry success hand churner customer abandon current company join another competing company service market customer churn common problem telecom business company sector try minimize churn rate study show reducing customer churn rate save money acquiring new customer cost five time satisfying existing one therefore reducing churn rate become particularly important preserving revenue sector significant financial implication correctly predicting customer churn model become vital crm identify customer likely terminate relationship result much focus developing new method improve accuracy using nowadays technique used predict future pattern behavior customer marketing strategy improved according produced result model approach play critical part success different application oil price prediction sentiment analysis energy consumption medical diagnosis application use one type family algorithm called ensemble method inspirited human cognitive system method powerful capability deal high-dimensional data generate several diverse solution given task ensemble method build many base model merge one achieve better prediction result using single base model bagging boosting popular ensemble method bagging method also known bootstrap aggregation based averaging base model boosting method built upon constructive iterative mechanism boosting algorithm several weak learner combined stage-wise obtain strong learner improved prediction accuracy family boosting method depends different constructive strategy ensemble formation gradient-descent-based formulation boosting method called gradient boosting machine gbm derived gbm considered optimization model aiming train series weak-learner model sequentially minimizes pre-defined loss function according several essential choice differentiable weak-learner model loss function customized given task gbm model making model highly flexible applied several application based task requirement paper aim develop new model improving gbm structure effectively predict customer churn telecom sector main contribution paper summarized follows cp-egbm new model high predictive performance may used develop effective strategy contains customer churn risk telecom sector enhance learning ability gbm model structure using svm base learner exponential loss loss function boosting capability pso exploration phase using consumption operator aeo method could effectively find suitable value cp-egbm hyper-parameters performance proposed cp-egbm assessed using seven datasets several evaluation metric cp-egbm model outperformed either gbm svm alone superior several earlier reported model literature making suitable rest paper arranged follows literature review provides literature review proposed cp-egbm model presented proposed cp-egbm experimental result discus experimental result lastly conclusion paper possible future work provided conclusion future work literature review many work applied ensemble model predict customer churn wang investigated capability gbm model used large customer dataset obtained bing-ads platform company identify whether customer would leave stay based analysis historical data record result showed gbm effective efficient model predicting churner customer near future several comparative analysis conducted using model ahmad compared four model including decision tree dts random forest gbm extreme gradient boosting xgboost customer churn prediction result showed xgboost method outperformed model evaluated model using big data provided telecom company syria jain used four model banking telecom sector used logistic regression svm xgboost result showed xgboost performed better others telecom sector another work dhini compared xgboost find best model used private dataset collected different company indonesia evaluate model result showed predictive performance xgboost better model sabbeh author compared set model using publicly available dataset result showed attained best result compared model used work sandhya applied k-nearest neighbor knn svm model publicly available dataset author first preprocessed dataset overcame class imbalance problem using synthetic minority oversampling technique smote obtained result showed performed better model kimura used six model svm catboost xgboost lightgbm data preprocessing author used smote tomek link smote-enn sampling method balance class distribution publicly available dataset result showed catboost smote best model zhu liu conducted comparative study ten model churn prediction using publicly available dataset result indicated xgboost obtained best accuracy compared model kanwal employed hybrid model using pso select informative feature publicly available dataset selected feature used input dts knn gradient boosted tree gbt model finding indicate pso gbt model obtained successful accuracy outcome compared model bilal introduced model based hybrid clustering classification method predict customer churn two publicly available datasets result showed model robust existing model literature stacking model technique i.e. mechanism aim leverage benefit set base model ignoring disadvantage also used karuppaiah gopalan presented stacked customer lifetime value-based heuristic incorporated ensemble model predict customer churn author used publicly available dataset evaluate proposed model obtained accuracy result showed good performance compared existing model literature rabbah proposed new model using deep learning stacked model used publicly available dataset validate model dataset first preprocessed balanced smote method used pre-trained convolutional neural network cnn select essential feature dataset employed stacking model technique i.e. mechanism aim leverage benefit set base model ignoring disadvantage predict customer churn result demonstrated high efficacy developed model dts xgboost naive bayes model karamollaoglu used separate datasets telecommunication industry eight model explored including knn svm adaboost multi-layer perceptron although model reported good performance ensemble-based model showed highest performance akinrotimi used oversampling technique class imbalance problem applied dimensionality reduction technique pick optimal feature strong predictive ability used model classification strategy result showed provided efficient result akbar apriono used xgboost bernoulli model showed xgboost attained best performance compared model based provided research work customer following research gap identified limited exploration ensemble model study applied ensemble model stacking model still need exploration evaluation different ensemble technique effectiveness improving prediction accuracy limited investigation hybrid model hybrid model combine different machine learning algorithm feature selection technique shown promising result however still lack comprehensive study comparing various hybrid model evaluating performance different datasets lack focus industry-specific many study evaluated model publicly available datasets need research focusing specific industry banking telecom different industry may unique characteristic churn pattern requiring customized approach preliminary analysis feature selection technique feature selection play crucial role help identify informative feature accurate prediction however existing literature lack comprehensive analysis comparison different feature selection technique impact performance lack comparison across multiple performance metric many study focus single performance metric accuracy f1-measure evaluating model however comprehensive comparison across multiple metric including precision recall area receiver operating characteristic curve auc-roc essential understand different model overall performance effectiveness addressing research gap would contribute advancing field customer churn prediction providing insight effectiveness different model technique approach various industry context facilitating accurate proactive customer retention strategy although existing model based ensemble method achieved tremendous success application still need effort provide sector efficient accurate model identify churner non-churner customer accurately ass decision-makers sector develop effective strategy order reduce customer churn rate gbm model show excellent potential classification problem typically base learner initialize model sub-optimum svm powerful mathematical model prof ability solve problem choosing effective base learner starting point gbm learning process could produce effective gbm model hence work base- learner gbm replaced svm addition hyper-parameters modified gbm optimized using modified version pso method best knowledge author optimizing gbm never applied far paper present new model improves gbm structure optimizes hyperparameters predict customer churn effectively proposed model ass improving efficiency designing optimal decision policy sector proposed cp-egbm overall process flow depicted fig proposed cp-egbm classification model red following sub-sections provide detail model figure flowchart proposed enhanced gbm egbm model full size image data preprocessing feature selection let dataset consist example -dimension feature vector 1\le n\le 1\le m\le m\right\ target label 1\le y\le c\right\ number class feature dataset normalized range per improve classification capability =\frac min max min min max minimum maximum value feature dimension normalized value feature example performance model degrades class-imbalanced dataset dataset balance checked comparing number example class label balancing dataset minority class example oversampled match number example using heterogeneous euclidean-overlap metric genetic algorithm heomga approach another critical factor affecting performance model input feature dimensional space significant feature classification selected normalized-balanced dataset using ant colony optimization- reptile search algorithm aco-rsa approach aco-rsa recent meta-heuristic approach published feature selection method optimal feature set comprises significant feature classification finally datasets split two exclusive exhaustive set training testing proposed cp-egbm model classification using cp-egbm overview gbm description developed cp-egbm hyper-parameter optimization cp-egbm given section gradient boosting machine gbm gradient boosting machine gbm combine set weak learner focusing resulting error iteration strong learner obtained sum successive weak one let n=1 denote training example goal gradient boosting find optimal estimate f\left x\right approximation function map instance minimize expected value given loss function distribution training example argmin f\left x\right f\left x\right gbm logistic loss function classification task estimate approximation function l\left f\left x\right y-f\left x\right gbm start weak learner f\left x\right usually constant value fit weak learner correct error made previous weak learner strengthen prediction performance minimizing loss function boosting stage stage local minimum proportional take step loss function negative gradient find local minimum gradient direction loss function boosting stage calculated l\left f\left f\left f\left x\right i-1 x\right gbm generalizes calculation range gradient regression tree used parameter weak-learners usually parameterized function input variable characterized parameter indicates partial derivative tree obtained solving following argmin n=1 -\beta h\left a\right parameter obtained iteration weight value i.e. expansion coefficient weak learner optimal length determined model x\right updated iteration number iteration step gbm algorithm gbm detailed algorithm choice base learner loss function derived gbm model facilitate capacity design development model researcher based task requirement work aim develop new classification model application enhancing structure gbm hyper-parameters discussed following subsection develop cp-egbm mentioned earlier gbm model typically base learner boosting stage new weak learner fitted current residual concatenated previous model update residual process continues maximum number boosting stage reached however using base learner might optimally approximate smooth function since extrapolates relationship input/output data point constant value thus using start gbm model training process could result poor predictive performance overfitting gbm model various base-learners derived divided linear smooth dts model optimized gbm using different manner however previous work focused changing base learner gbm improve structure using svm svm model introduced prof ability solve various classification problem classifier svm depends training data build model finding best decision hyperplane separate class label i.e. response variable main goal svm find optimum hyperplane maximizing margin minimizing classification error class addition using kernel function strategy applicability linearly non-separable data extended map input data higher dimensional space hyperplane described follows +\mathrm average vector position relative area coordinate center optimization margin support vector converted constrained programming problem min +c\sum_ i=1 s.t +b\right represents misclassified sample corresponding margin hyperplane cost penalty svm model widely used kernel function linear polynomial sigmoid radial basis function rbf among rbf preferable due reliability implementation adaptability handle complex parameter simplicity research svm rbf kernel svm rbf integrated base learner gbm structure boost learning capability provide accurate approximation target label rbf kernel function given k\left =\mathrm exp -\upgamma +c\right vector feature computing training test data point determines influence training example cost penalty gbm learning performance given task depends greatly loss function therefore essential carefully select loss function function calculate corresponding negative gradient gbm model structure several loss function reported literature classification including logistic regression i.e. deviance bernoulli exponential comparison loss function presented next section pseudo-code developed cp-egbm given algorithm hyper-parameter optimization parameter setting essential enhancing model efficacy performance traditionally hyper-parameters selected using trial-and-error however manually tuning parameter often time-consuming yielding unsatisfactory result without deep expertise method tune model hyper-parameters solving problem two method pso aeo presented following subsection modified pso mpso method introduced particle swarm optimization pso pso method inspired simulate social group behavior animal human insect method set particle initial population traverse given search space randomly iteration position particle velocity particle updated using best position current population let particle -dimensional search space position velocity time expressed t\right t\right t\right t\right 1\le i\le t\right t\right t\right t\right fitness local best position best global best position best time represented best t\right t\right t\right t\right best t\right t\right t\right t\right time t+1\ velocity t+1 particle updated t+1\right t\right besti t\right t\right best t\right t\right inertia weight factor control velocity allows swarm converge cognitive factor social factor control randomness added velocity t+1 next position t+1 two random vector range 0,1 t+1\right t\right t+1\right next position t+1\right particle computed using current position t\right updated velocity t+1\right generated finally vector present solution present momentum particle artificial ecosystem-based optimization aeo aeo another method motivated energy flow natural ecosystem introduced aeo three operator achieve optimal solution described production operator producer represents worst individual population thus must updated concerning best individual considering upper lower boundary given search space guide individual search region operator generates new individual best individual best based fitness randomly produced position individual search space r\mathrm replacing previous one operator given t+1\right 1-\alpha best t\right rand t\right 1-t/\mathrm rand ub-lb\right +lb rand t\right guide individual explore search space subsequent iteration broadly t+1\right lead individual exploitation region around best t\right intensively linear weight coefficient move individual linearly random position position best individual best t\right pre-defined maximum number iteration random number interval ub\ lb\ represent upper lower boundary search space consumption operator start production operator completed may eat randomly chosen low-energy consumer producer obtain food energy levy flight-like random walk called consumption factor employed enhance exploration capability defined follows =\frac n\left 0,1 n\left 0,1 normal distribution zero mean unity standard deviation different type consumer adopt different consumption behavior update position strategy include herbivore behavior herbivore consumer would eat producer formulated t+1\right t\right +cf.\left t\right t\right i\in p\right carnivore behavior carnivore consumer would eat another consumer higher energy modeled t+1\right t\right +cf.\left t\right r\mathrm nd\in 2i-1\right t\right i\in p\right omnivore behavior omnivore consumer eat random producer producer higher energy behavior formulated t+1\right t\right +cf\left t\right t\right t\right r\mathrm nd\in 2i-1\right t\right i\in p\right decomposition final phase ecosystem agent dissolve decomposer break remains dead individual provide required growth nutrient producer decomposition operator expressed t+1\right t\right +de t\right -h. nd\in 2i-1\right t\right i\in p\right de=3u u\in n\left 1\right randi\left 2\right h=2 de\ weight coefficient designed model decomposition behavior modified pso mpso method exploration phase integral algorithm aiming find better solution investigating search space pso suffers premature convergence local minimum make spend time locally optimal solution hence weak exploring new area search space modified pso mpso method aim avoid premature convergence local optimum thus enhance capability tune optimum hyper-parameters cp-egbm model mpso method integrates consumption operator aeo pso method structure discussed previous subsection consumption phase aeo method responsible exploration three leading operator herbivore carnivore omnivore herbivore omnivore based producer solution i.e. equal best solution swarm last operator depends two randomly selected solution help explore new region search space mpso method utilizes strength aeo exploration strength pso exploitation select optimum hyper-parameters cp-egbm model mpso presented pseudo-code mpso described algorithm t+1\right t\right cf- t\right best t\right t\right evaluation measure study cp-egbm model assessed using set evaluation measure including accuracy precision recall f1-measure area roc curve auc computed follows ac= tp+tn tp+tn+fn+fp recall tp+fn f1-measure tp+fn\right tp+fp\right tp\left 2tp+fn+fp\right auc tp+fn fp+tn true positive true negative denote correctly detected sample positive negative respectively similarly false negative false positive represent number misclassified positive negative example experimental result experiment performed ass cp-egbm model comparing performance gbm svm rbf model described experimental setup performance cp-egbm validated conducting experiment publicly available datasets characteristic datasets presented table heomga used data balancing aco-rsa employed datasets possible bias selecting training testing datasets avoided using tenfold cross-validation technique employed experiment implemented using python executed 3.13 ghz ram window operating system table characteristic open-source datasets used evaluating developed cp-egbm full size table base learner behavior gbm model examine effect changing base learner svm rbf gbm model probability density distribution used test dataset classification score number indicating degree much testing example belongs churner/non-churner class generated base learner visualized using violin plot method shown fig classification score raw continuous-valued probabilistic output model binary classification one class assume churner classification score another class score 1-p\ figure comparative analysis testing dataset classification score generated svm rbf base learner based probability density distribution non-churners churners datasets full size image violin plot method similar box plot additional characteristic called probability density typically smoothed kernel density estimator interquartile range calculated distribution compare base learner dispersion non-churner churner class horizontal dotted line class group indicate first 25th percentile data second 50th percentile data median third 75th percentile data quartile corresponding distribution similarity/closeness two distribution directly proportional closeness quartile visualization fig show quartile classification score using svm rbf base learner well-separate churners red non-churners green quartile using using svm rbf base learner better classifies churner non-churner distribution churners non-churners similar base learner also indicated closer quartile class resulting poor classification base learner result confirm prove suitability svm rbf used base learner developed cp-egbm model loss function selection loss function give general picture well model performed prediction predicted result much closer actual value loss minimum result far away original value loss value maximum section experiment conducted using three loss function figure suitable one application include logistic deviance cross-entropy loss negative log-likelihood bernoulli model default loss function gbm defined logi =-y\mathit log 1-y log 1-\widehat bernoulli formulated follows bern =\mathrm log 1+\mathrm exp -2y.\widehat exponential also used adaboost algorithm defined ada =\mathrm exp -y.\widehat binary class indicator either probability class 1-\widehat probability class figure plot behavior loss function defined number iteration using developed cp-egbm seen fig exponential loss function obtains smaller loss value explained exponentially effectively contrasting misclassified data point much enabling cp-egbm capture outlying data point much earlier logistic bernoulli function result experiment confirm exponential loss function suitable two competitor loss function application figure comparative analysis loss function behavior datasets cp-egbm framework full size image hyper-parameter setting better understand behavior introduced mpso convergence curve generated iteration x-axis fitness value y-axis shown fig wide range method introduced literature used hyper-parameters tuning however mpso compared multi-verse optimizer mvo whale optimization algorithm woa gray wolf optimizer gwo pso aeo method population size set maximum iteration equal run time setting selected empirically studying fig convergence speed mpso faster method five seven datasets stabilizes shallow fitness value fewer iteration overall suggested improvement pso lead better convergence attribute computation time making mpso suitable tuning cp-egbm model hyper-parameters figure comparison convergence behavior proposed mpso algorithm datasets optimizing cp-egbm model full size image several hyper-parameters need initialized developed cp-egbm mpso method used optimize hyper-parameter setting optimized information dataset listed table respectively table optimization hyper-parameters different model tuning developed cp-egbm full size table table hyper-parameters different model cp-egbm optimized mpso datasets full size table experimental result discussion result gbm svm rbf developed cp-egbm model using evaluation metric receiver operating characteristic roc statistical test model stability discussed section also comparison cp-egbm used model recent work provided performance result performance assessment gbm alone svm rbf alone developed cp-egbm model datasets carried section applying tenfold-cv fine-tuning model hyper-parameters using mpso average result computed recorded table respectively table performance evaluation gbm alone datasets full size table table performance evaluation svm rbf alone datasets full size table table performance evaluation optimized cp-egbm datasets full size table result table show developed cp-egbm performs better model datasets individual evaluation metric figure show model performance datasets figure reveal cp-egbm accomplished effective outcome compared gbm svm rbf instance dataset cp-egbm obtained accuracy 97.79 recall 90.33 f1-measure 91.52 auc 92.73 result table fig confirm superiority cp-egbm compared model figure comparative accuracy analysis gbm svm rbf cp-egbm datasets full size image figure comparative recall analysis gbm sbm rbf cp-egbm datasets full size image figure comparative f1-measure analysis gbm svm rbf cp-egbm datasets full size image figure comparative auc analysis gbm svm rbf cp-egbm datasets full size image svm relatively good accuracy 0.8799 f1-measure 0.8410 gbm high better accuracy 0.9401 f1-measure 0.8439 developed cp-egbm outperforms highest accuracy 0.9623 f1-measure 0.8698 svm alone moderate performance accuracy 0.8376 f1-measure 0.7394 gbm provides accuracy 0.8677 f1-measure 0.8200 cp-egbm provides relatively high accuracy 0.8649 f1-measure 0.8211 gbm show worst performance accuracy 0.6737 f1-measure 0.6813 time cp-egbm best accuracy 0.6949 f1-measure 0.7044 similarly gbm lowest accuracy 0.5631 f1-measure 0.5902 cp-egbm show high performance accuracy 0.6250 f1-measure 0.6287 hand gbm performs better svm ds5 cp-egbm outperforms high accuracy 0.9482 f1-measure 0.8727 similar observation made outstanding performance cp-egbm providing high accuracy 0.9779 f1-measure 0.9152 gbm cp-egbm provide accuracy later higher f1-measure earlier overall cp-egbm consistently outperforms gbm svm across datasets term accuracy f1-measure auc however gbm svm show competitive performance achieving high accuracy f1-measure datasets lower performance others roc curve roc curve computes model performance changing confidence level model score get distinct value true-positive rate tpr false positive rate fpr illustrated fig figure show cp-egbm curve dominate gbm svm rbf model point considered datasets indicates suitability developed cp-egbm figure roc-based performance comparison gbm svm rbf cp-egbm datasets full size image statistical test model stability developed cp-egbm selected control model friedman rank test shown fig figure cp-egbm get highest accuracy fig fitness value rank fig followed gbm second svmrbf ranked last therefore work concludes cp-egbm significantly better model figure friedman rank test statistical comparison accuracy fitness value different model full size image relative stability result associated standard deviation std developed cp-egbm model also calculated provided table according result table developed cp-egbm model achieved smallest std value compared gbm svm rbf model datasets reflects stability robustness developed cp-egbm applying table comparison stability robustness model using std value datasets full size table performance comparison existing model several study recently used model predict customer churn telecom sector comparison developed cp-egbm study given table see table study utilized evaluate model used work therefore use compare performance cp-egbm per result table developed cp-egbm model great potential predict customer churn term accuracy f1-measure better prediction result existing model table comparison existing model proposed cp-egbm model term accuracy f1-measure full size table proposed framework algorithm feature selection hyper-parameter tuning although algorithm shown effectiveness many domain also certain limitation algorithm may converge prematurely get stuck local optimum fail explore search space adequately algorithm require many iteration evaluation objective function computationally expensive complex problem several control parameter need set appropriately achieve good performance search process becomes challenging high-dimensional space algorithm may struggle explore exploit search space effectively despite limitation algorithm remain valuable tool solving complex optimization problem conclusion future work telecom sector accumulated massive amount customer information development hand widespread data warehouse technology application make possible gain insight historical customer data therefore become clear manager sector customer information used create prediction model contain customer churn risk cp-egbm model developed provide efficient prediction model application cp-egbm model svm base learner dts weak learner gbm structure moreover modified version pso mpso introduced optimize cp-egbm model hyper-parameters injecting aeo consumption operator pso structure cp-egbm assessed using seven datasets experimental result statistical test analysis show higher efficacy cp-egbm model gbm svm reported model literature result confirm ability developed model telecommunication sector future use developed cp-egbm address cp-related problem e-commerce business online shopping application also deploy cp-egbm datasets make result robust also apply suggested mpso method solve feature selection problem different field sentiment analysis internet thing sentimental analysis sentence represented using high dimensional sparse vector due tokenization natural language processing similarly iot application input represented using high-dimensional vector large number sensing node application mpso used reduce feature dimensionality reducing computational complexity system