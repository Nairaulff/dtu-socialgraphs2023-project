introduction 3.9 billion people worldwide used social medium increase january 4.9 billion people accessing social medium globally average social medium user hop platform every month many factor contributing user growth global penetration smartphones evident one instance social medium interaction include comment like share express people opinion enormous amount unstructured data give data scientist information scientist ability look social interaction unprecedented scale level detail never imagined previously analysis evaluation information becoming complicated number people using social networking site grows example facebook instagram e-commerce website blog improve customer satisfaction overall shopping experience customer allowing customer rate comment product purchased planning purchase sentiment analysis also known opinion mining study people attitude sentiment product service attribute sentiment analysis hold paramount importance political discourse particularly within amharic-speaking region ethiopia instance global local political landscape underscore impact sentiment analysis political reform instance election barack obama united state showed role social medium shaping political sentiment galvanizing support mobilizing voter within ethiopia sentiment analysis closely linked political reform ethiopian political landscape undergone significant change recent year social medium helped voice public opinion influencing political decision social medium site facebook twitter youtube used assist country political reform process analyzing amharic political sentiment pose unique challenge due diversity length content social medium comment amharic language encompasses rich vocabulary intricate grammatical structure vary across region context linguistic complexity complicates sentiment analysis necessitating context-aware approach moreover social medium comment often lengthy contextually nuanced making challenging accurately capture intended sentiment previous work explored sentiment analysis amharic application deep learning technique represents novel advancement leveraging power deep learning research beyond traditional method better capture amharic political sentiment uniqueness lie ability automatically learn complex feature data adapt intricate linguistic contextual characteristic amharic discourse general objective study construct deep-learning sentimental analysis model amharic political sentiment related work sentiment analysis involves categorizing sentiment positive negative explored across various domain local context various researcher applied machine learning technique perform sentiment analysis domain entertainment aspect-level sentiment classification social medium deep learning-based amharic sentiment classification hassan mahmood employed deep learning sentiment analysis short text using datasets like stanford large movie review imdb stanford sentiment treebank word2vec utilized word embedding combining convolutional neural network cnn recurrent neural network rnn despite achieving 88.3 47.5 accuracy hybrid model deemed suboptimal suggesting experimentation different rnn model ghorbani introduced integrated architecture cnn bidirectional long short-term memory lstm ass word polarity despite initial setback performance improved 89.02 bidirectional lstm replaced bidirectional gru study underscore model compatibility impact performance mohammed kora tackled sentiment analysis arabic complex resource-scarce language creating dataset 40,000 annotated tweet employed various deep learning model including cnn long short-term memory lstm achieving accuracy rate ranging 72.14 88.71 data augmentation meena demonstrate effectiveness cnn lstm technique analyzing twitter content categorizing emotional sentiment regarding monkeypox positive negative neutral effectiveness combining cnn bidirectional lstm explored multiple language showing superior performance compared individual model noteworthy study include shen imdb movie review zhou chinese product review alharbi arabic datasets ref afaan oromo datasets meena proposes effective sentiment analysis model using deep learning particularly cnn strategy evaluate customer sentiment online product review finding suggest potential using online review inform future product selection study focused laptop phone television room extending approach different product language future research several researcher endeavored build sentiment classification model amharic abraham applied machine learning amharic entertainment text achieving 90.9 accuracy using naïve bayes however challenge remain handling negation exploring n-grams improved feature set aspect-level opinion mining also suggested research mulugeta philemon utilized supervised machine learning naïve bayes bigram sentiment analysis amharic presenting alternative multi-scale approach despite limited training data result encouraging leading proposal research document-level sentiment analysis yeshiwas abebe adopted deep learning approach amharic sentiment analysis annotating comment seven class using cnn various experiment achieved accuracy rate ranging 90.1 finding laid foundation future exploration amharic sentiment analysis turegn evaluated impact data preprocessing amharic sentiment analysis integrating emojis comparing human automatic annotation study found stemming positive impact emojis provided negligible improvement automatic annotation overlapped significantly human annotation study suggested exploration cnn-lstm cnn-bilstm network enhance prediction accuracy mengoni santucci highlight recent stride artificial intelligence particularly natural language processing nlp tackling task machine translation sentiment analysis achievement notable challenge persist including adapting english-based nlp method language study collectively underline evolution amharic sentiment analysis challenge providing valuable insight future research summary related research work depicted table follows table summary related work full size table research methodology overview study implemented experimental research method experimental research design scientific method investigation one independent variable altered applied one dependent variable determine impact latter experimental research experimental setup determining many trial run parameter weight methodology datasets employ data collection preparation data collection total comment acquired study different source prominently discus political environment ethiopia ensure correctness relevance collected sentiment process carried close collaboration linguistic expert keep dataset balanced equal distribution positive negative comment maintained process data acquisition lexicon employed prior researcher used data source study official social medium page affiliated prime minister dr. abiy ahmed fana broadcasting corporation fbc ezema political party official facebook page prosperity party official facebook account dataset preparation dataset collected careful process data organization cleansing followed goal eliminate inconsistency typographical error well duplicate inaccurate information might distort integrity dataset data cleaning stage helped address various form noise within dataset emojis linguistic inconsistency inaccuracy short form word expanded full form stop word removed synonym converted normalized form preprocessing deep learning approach used various deep-learning model exist sentiment classification study selection deep learning model contingent suitability amharic sentiment analysis model selection process criterion noted ref considered criterion encompass aspect feature extraction proficiency preservation long-term dependency mitigation vanishing gradient problem aptitude comprehending diverse linguistic context well model characterized fewer parameter faster convergence time cnn cnn model use convolutional layer pooling layer extract high-level feature research cnn sentiment word treat sentiment one-dimensional collection pixel employed cnn used find hidden connection word nearby region cnn recognized capability extract feature accurately minimizing number input feature built applying different step first embedded word fed convolutional layer selects feature pooling layer performs dimensionality reduction feature extracted previous layer feature combined passed fully connected layer output determined based sigmoid function normalizes two class i.e. positive negative figure present architecture cnn model used text classification figure cnn model architecture text classification full size image bidirectional-lstm long short-term memory network bidirectional incorporate context information past future input long sequence part gradient vector may exponentially expand decline making challenging rnn include long-term dependency lstm design overcomes issue learning long-term dependency presented simple rnn incorporating memory cell hold state long period way bidirectional-lstm combine forward hidden layer backward hidden layer see fig manipulate previous future input figure bidirectional-lstm full size image seen fig bi-lstm learn direction integrate piece knowledge make prediction embedded word used input bidirectional lstm model added bi-lstm layer using kera tensorflow kera new bidirectional class used construct bidirectional-lstm fit model data gated recurrent unit gru gru gating unit influence flow information within unit address vanishing gradient problem regular rnn large text benefit greatly gru gru like lstm gating unit regulate data flow unlike lstm need additional designated memory cell update reset gate two crucial gate gru decide information passed output architecture depicted fig show gru two gate output determination reset gate determines whether part prior hidden state integrated present input formulate new hidden state update gate oversees deciding much prior hidden state kept much proposed new hidden state reset gate included final hidden state whenever update gate multiplied prior hidden state first time gate chooses piece prior hidden state preserve memory dismiss rest result whenever utilizes reverse update gate extract newly proposed hidden state reset gate filling required piece information figure internal structure gru full size image hybrid cnn-bidirectional-lstm strength cnn bi-directional model combined hybrid technique see fig cnn model use convolutional layer pooling layer extract feature whereas bidirectional-lstm model preserve long-term dependency word sequence hence cnn-bidirectional-lstm model suitable sentiment classification figure proposed model architecture full size image input preprocessed embedded passed cnn convolutional layer extract feature different part text pooling layer reduces number feature input feature obtained pooling layer passed bidirectional-lstm extract contextual information finally last state bilstm concatenated passed sigmoid activation function squash final value range proposed architecture design general architecture amharic sentimental analysis using deep learning approach shown fig figure architecture sentiment analysis amharic language using deep learning full size image data preprocessing data preprocessing process removing distortion data make classification task easier case sentiment classification improve performance model result critical apply data preprocessing overcome issue data cleaned accurate deep learning model short-form expansion amharic lot short form need expanded get full-length word researcher using word train data short form used frequently writing comment opinion amharic shown table data cleaning stage preprocessing eliminate special character symbol emojis needed started removing non-amharic character special character shown table normalization amharic different character sound written different form like description algorithm used transforming text single canonical form depicted fig tokenization larger chunk text document tokenized list sentence sentence list word list word identified tokenizer function used training also testing comprehended deep learning system token also transformed vector format stop-words detection removal stop word must removed reduce dimensionality word vector contribution determining emotion sentiment common stop word amharic language etc padding deep learning network expect datasets vector equal dimension however sentence size preprocessing put another way sentence longer shorter term word contain make document uniform size zero added pre sentence post sentence shorter sentence matrix called padding sentence numerically represented word maximum length given sentence used input sentence maximum length post padding applied adding zero end sentence make equal maximum length sentence research table amharic short form writing full size table table removed word number punctuation full size table figure algorithm normalization amharic character variant full size image word-embedding word-embedding feature learning technique word phrase vocabulary mapped n-dimensional real-number vector goal word embedding convert word dictionary lower-dimensional vector build word representation data deep learning model researcher employ word2vec embedding model preprocessing converting datasets format analyzed word sentence must represented vector word2vec calculate similarity analogy embedding layer convert input n\times dimensional vector represents longest sentence dataset represents embedding dimension experimental result four experiment conducted dividing preprocessed dataset three subset sentence training validation another testing experimenting using cnn cnn experimentation began inputting preprocessed data cnn layer facilitate feature extraction cnn layer employed filter kernel utilized relu activation function following feature extraction step data forwarded globalmaxpooling1d layer downed sample representation selecting maximum value across time converting output subsequently value passed fully connected output layer maintain output value binary classification task negative positive sentiment sigmoid activation function applied binary cross-entropy chosen loss function training researcher measured accuracy recall precision performance metric conducted training epoch optimize model model assessed test dataset model fitted result presented shown table table 1st evaluation result cnn model full size table table observed model achieved 99.43 accuracy training dataset achieves 84.96 84.32 validation testing accuracy respectively learning curve depicted fig figure learning curve cnn model full size image training accuracy increase number epoch increase validation accuracy decrease number epoch increase result concluded model over-fitted compared work required combat over-fitting building model executing code easier part researcher used many regularization approach model seeding also known random state reduce model vulnerability over-fitting researcher added one dense layer hidden layer neuron activation function relu added dropout layer convolutional layer feeding pooling layer added dense layer dense layer researcher also added another dropout layer fed fully connected layer dropout discovered incredibly essential since allows model avoid over-fitting dropping neuron random point batch size increased epoch number decreased change made based manual tunning experimental result presented table table model result regularization full size table presented table regularization accuracy model improved result show minimal difference observed among training validation test accuracy show problem over-fitting solved compared previous result achieved regularization figure also show learning curve cnn model regularization figure learning curve cnn model regularization full size image learning curve fig model overfitting problem since gap shown training validation decreased cnn model amharic sentiment dataset finally registered accuracy precision recall 84.79 80.39 73.69 respectively experimenting using bidirectional-lstm bidirectional-lstm layer receives vector representation data input learn feature data preprocessed embedding component constructed bi-directional lstm bi-lstm extract important contextual data past future time sequence bi-lstm contrast lstm contains forward backward layer conducting additional feature extraction suitable amharic language language nature need context information understand sentence bi-lstm one hidden layer direction extract feature one copy hidden layer fit input sequence traditional lstm placed reversed copy input sequence result obtained lstms concatenated default forward backward hidden layer model researcher used bidirectional lstm 64-memory unit add dropout 0.4 0.5 random state embedded size batch size epoch minimize overfitting calculate loss function binary classification used adam optimizer experimental result bi-lstm presented table table bi-lstm model evaluation full size table bi-lstm model result show accuracy 90.76 89.18 85.27 training validation testing respectively hereunder fig present learning curve bi-lstm figure learning curve bi-lstm model full size image learning curve depicted fig difference training validation accuracy nominal indicating overfitted hence capable generalizing previously unknown data real world model result show satisfactory fit dataset get ideal state model researcher employed regularization approach like dropout discussed accuracy precision recall bi-lstm amharic sentiment dataset 85.27 percent 85.24 81.67 respectively result show bi-lstm model performs better cnn model indicates capability bi-lstm improve classification performance considering previous future word learning experimenting using gru gru first researcher creates suitable embedding layer maximum feature provide output shape embedding layer hidden layer input value serve weight gated recurrent unit make hidden layer researcher used gru two layer get representation entire sequence passed input outer layer used sigmoid activation function categorize sentiment positive negative adam optimizer gru unit unit memory used building model test result show model overfitted overcome overfitting researcher added dropout 0.5 0.5 change random state batch size epoch one hyperparameter made difference modifying default value adam learning rate 0.1 0.0001 table show experimental result gru table gru model evaluation result full size table presented table gru model register accuracy 97.73 92.67 88.99 training validation testing close result obtained bi-lstm though number epoch considered gru get accuracy twice bi-lstm gru solves over-fitting challenge compared bi-lstm parameter tuning figure depicts learning curve gru model figure learning curve gru model full size image learning curve gru model gap training validation accuracy minimal model start begin underfit however researcher increase epoch number accuracy increased overcomes underfitting loss high first iteration decrease minimum last epoch end gru model converged solution faster large iteration arrive optimal value summary gru model amharic sentiment dataset achieved 88.99 90.61 89.67 accuracy precision recall respectively experimenting using cnn-bidirectional-lstm researcher combined cnn bi-lstm intention take advantage best feature model develop model could comprehend classify amharic sentiment datasets better accuracy combining two model provide best feature extraction context understanding embedding layer input value passed convolutional layer size 64-filter kernel size well activation function relu convolutional layer max-pooling layer pool size output layer passed bidirectional layer unit output passed fully connected layer sigmoid binary classifier optimizer adam binary cross entropy loss function used result shown table table cnn-bi-lstm model evaluation result full size table table trained model register accuracy precision recall model performs poorly validation testing given unseen datasets show model memorizing training data instead learning resulted over-fitting learning curve depicted fig show behavior model accuracy vs. model loss figure learning curve hybrid cnn bi-lstm model full size image learning curve fig show training loss close loss validation set increasing indicates overfitting overcome overfitting researcher applied different first regularization method like weight decaying adding dropout adjusting learning batch size momentum model reducing iteration model various hyperparameters tuned model optimal value reached shifted overfitting ideal fit dataset table show optimal value cnn-bi-lstm table optimal value tuning cnn-bi-lstm model full size table using aforementioned optimized hyperparameters depicted table experimental result shown table table evaluation result cnn-bi-lstm model hyperparameter tuning full size table shown table 99.73 91.11 percent 91.60 percent accuracy achieved training validation testing respectively hybrid model outperforms previous model looking marginal difference training validation testing difference small showing well model work unknown datasets generalization ability figure depicts learning curve hybrid cnn bi-lstm model figure learning curve hybrid cnn bi-lstm model full size image overall amharic sentiment dataset cnn-bi-lstm model achieved 91.60 90.47 93.91 accuracy precision recall respectively comparison model experiment performed using four distinct deep learning model based promising result amharic sentiment analysis obtained figure present comparison four model figure comparison model full size image figure show performance four model amharic sentiment dataset comparing performance cnn-bi-lstm showed much better accuracy precision recall cnn-bi-lstm capability model classify dataset cnn well recognized feature selection bi-lstm enables model include context providing past future sequence combining two model accuracy 91.60 figure provides confusion matrix cnn-bi-lstm entry confusion matrix denotes number prediction made model classified class correctly incorrectly 500-testing dataset available testing cnn-bi-lstm correctly predicted sentiment sentence misclassification rate also known classification error show fraction prediction incorrect calculated using following equation figure confusion matrix cnn-bi-lstm full size image misclassification\ rate =\frac fp+fn tp+tn+fp+fn misclassification rate cnn-bi-lstm calculated first adding false positive false negative divided total testing dataset false positive model false negative give misclassification rate 8.4 model showed low misclassification rate confusion matrix fig show number false-positive higher false negative table show type one type two error encountered model table example misclassification model full size table table show model get confused found comment sarcasm figurative speech sentiment sentence contain word give positive negative sentiment one comment example first sentence contains positive word like second sentience contain also contains word imply negative sentiment like first sentence second contains sentence belong positive class model predicted negative word contained within sentence caused misclassification word implies positive sentiment overall sentiment comment negative caused model predict sentiment positive cnn-bi-lstm model classification error model struggle understand sarcasm figurative speech mixed sentiment available within dataset discussion result research address gap previous work comprehensive experimental study researcher studied impact datasets preparation word embedding deep learning model focus problem sentiment analysis four deep learning model cnn bi-lstm gru cnn-bi-lstm amharic sentiment analysis compared experiment result showed combining cnn bi-lstm generated model outperformed others model compared model specific optimal point model reached good fit cnn-bi-lstm take advantage strength two model cnn recognized ability extract many feature possible sentence bi-lstm keep chronological order word past future enables model understand context several factor influence performance deep learning model instance data preparation size dataset well number word within sentence impact performance model training model using sentence datasets limited number word within sentence give accuracy 85.00 number word increase greater five word per comment within sentence performance improves 85.00 88.66 3.6 improvement whereas increasing size dataset showed accuracy 91.60 upgrade result see impact size dataset well size word within single comment performance model factor like word embedding filter size kernel size pool size activation function batch size adjusting hyperparameter optimization mechanism also play major role performance model overall tuning factor showed significant amount improvement deep learning model performance factor padding respond differently model model instance applying pre-padding cnn increase model performance model perform poorly using pre-padding kapočiūtė-dzikienė claim deep learning model tend underperform used morphologically rich language hence recommend traditional machine learning approach manual feature engineering despite author conclusion recommendation doe hold true comparing performance amharic sentiment analysis model constructed study using deep learning machine learning model proposed ref finding study show deep learning model bring improvement compared traditional machine learning term work needed feature extraction performance scalability manual feature engineering used work eliminates extra effort needed feature extraction addition model could understand context given sentence considering model performance small significant increase achieved scalability main challenge standard machine learning model deep learning model used research showed accuracy model increase size dataset training testing increase two researcher attempted design deep learning model amharic sentiment analysis cnn model designed alemu getachew overfitted generalize well training data unseen data problem solved research adjusting hyperparameter model shift model overfitted fit generalize well unseen data cnn-bi-lstm model designed study outperforms work fikre lstm model increase performance work major contribution update state-of-the-art amharic sentiment analysis improved performance proposed model achieved 91.60 6.81 6.33 2.61 improvement cnn bi-lstm gru respectively proposed model achieved promising result sentiment analysis mostly research work overfitting encountered different hyperparameters applied control learning process hyperparameters like learning rate dropout momentum random state case shifted model overfitting good fit model achieved high accuracy overfitted useful real world model generalization capacity applicable conclusion ethiopia lot opinion available various social medium site must gathered analyzed ass general public opinion finding monitoring comment well extracting information contained manually tough undertaking due huge range opinion internet matter fact normal human reader trouble finding appropriate website accessing summarizing information contained inside result automated sentiment analysis method necessary different researcher used sentimental analysis amharic sentiment either lexical machine learning approach require interference programmer one point another come deep learning minimizes human involvement make life easier research researcher applied sentimental analysis amharic political sentence using four different deep learning approach cnn bi-lstm gru hybrid cnn bi-lstm researcher knowledge first work applied bi-lstm gru cnn-bi-lstm experimental result show hybrid cnn-bi-lstm model achieved better performance 91.60 compared model 84.79 85.27 88.99 cnn bi-lstm gru respectively researcher conduct hyperparameter search find appropriate value solve overfitting problem model result verify main contribution study still room improvement working research problem like manually collecting annotating dataset tiring task even though promising accuracy achieved model trained limited dataset made model learn limited feature considered binary classification model struggle distinguish sarcasm figurative speech sentiment sentence contain word give positive negative sentiment challenge area need research recommendation research underscore significance adopting multi-class classification approach conventional binary positive–negative scheme multi-class framework offer nuanced insightful breakdown sentiment furthermore establishment standardized corpus emerges crucial endeavor study primary focus revolves around political sentiment analysis applicability extends far beyond political domain insight methodology developed herein readily extended diverse sector agriculture industry tourism sport entertainment area concerning employee customer satisfaction future research notably unexplored avenue pertains analysis sarcastic comment amharic language presenting promising area investigation