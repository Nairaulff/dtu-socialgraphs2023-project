introduction deep learning demonstrated significant progress image classification object detection owing powerful feature learning transfer learning capability series detector fast r-cnn fasterr-cnn yolo ssd cascaded r-cnn retinanet proposed obtain significant performance improvement among fpn framework object detection significantly increase performance object detection model specifically final feature map various scale contain rich semantic information low-level feature map fpn combine semantic information high-level feature map various scale fpn-based method produced outstanding performance object detection combining feature various scale however fpn three pervasive limitation feature channel information lost dimensionality reduction significant semantic gap feature various level feature confusion caused multi-scale fusion although existing method libra r-cnn augfpn cefpn alleviate issue extent still ample room improvement describe issue following figure feature pyramid network fpn structure c_i\ represent original feature layer f_i\ represent feature layer dimensionality reduction using convolution p_i\ represents feature layer used prediction feature fusion full size image advanced feature map channel information lost shown fig fpn-based method typically employ 1\times convolutional layer c_i\ f_i\ perform dimensionality reduction however dimensionality reduction performed advanced feature rich semantic information number channel change channel information may lost lost information could critical detection task retained information address issue existing method bifpn performs multiple weighted feature fusion f_i\ p_i\ enhance feature expression ability prediction layer achieves higher detection accuracy brings lot computation augfpn add residual feature enhancement module c_i\ p_i\ reduce information loss channel reduction however method focus reducing channel loss due dimensionality reduction feature fusion take full advantage rich semantic information c_i\ semantic gap feature different level fpn low-level feature transferred layer layer backbone network obtain high-level feature low-level feature contain rich spatial information useful detecting position object image poor semantic information making challenging determine class detected object high-level feature hand rich semantic information spatial information making difficult pinpoint precise location object therefore low-level high-level feature complement object detection reduce number feature channel feature various level processed simple 1\times convolutional layer prior feature fusion directly fusing feature ignoring significant semantic gap feature weakens expressiveness multi-scale feature resulting dilution semantic information top-down feature fusion solve problem pafpn adopts top-down bottom-up fusion method effectively exchange information high-level feature low-level feature libra r-cnn solves problem unbalanced semantic information training fusing high-level feature low-level feature balanced feature pyramid multi-scale fusion lead feature confusion cross-scale fusion libra r-cnn skip linking bifpn technique successful improving performance fpn-based object detection model however semantic gap feature map different scale immediate fusing following linear interpolation may result feature confusion fpn stacking multiple integrated feature may loss location information resulting localization recognition task confusion acfpn overcome challenge present three novel component paper first inspired self-attention mechanism introduce semantic enhancement module channel-spatial attention resizes feature map different scale uniform size concatenates along channel dimension obtain global context global context modelled using attention mechanism obtain global semantic information second propose semantic injection module reducing semantic gap feature different scale third inspired gating unit introduce gated channel guidance module reduce feature confusion caused feature fusion name whole model multiscale semantics enhance feature pyramid network mse-fpn flexible generalisable fpn-based detector main contribution follows inspired self-attention mechanism propose semantic enhancement module channel-spatial attention generate global semantic information introduce semantic injection module reduce semantic gap feature map different scale reduce confusion feature different scale simple effective gated channel guidance module introduced feature fusion evaluated proposed object detection framework coco showed outperformed fpn-based detector significantly related work model-based method object detection advent deep convolutional network object detection advanced significantly recent year current object detection method follow one-stage two-stage model r-cnn first use selective search generate region suggestion combining convolutional neural network object detection sppnet fast r-cnn use r-cnn extract feature map entire image use spatial pyramidal pool roi pool generate region feature respectively region proposal network rpn proposed faster r-cnn improve detector performance enable end-to-end training detector since many method improved r-cnn different angle example handle problem multi-scale detection fpn realizes prediction different level pyramid structure solves scale change cascade r-cnn classic yet powerful cascaded architecture extends faster r-cnn multi-stage detector mask r-cnn extends fasterr-cnn adding mask branch flexibly adapt multiple detection task one-stage detector localization classification usually achieved directly using unified network achieves higher efficiency loses accuracy certain extent ssd detect object different scale yolo feature map predict object category regression box retinanet relies focal loss overcome problem significant imbalance ratio positive negative sample one-stage object detection improve accuracy semantic gap multi-scale feature different scale feature significant semantic difference direct fusion may result feature misunderstanding top-down path put forward feature pyramid network fpn combine multi-scale feature following idea pafpn add additional bottom-up path aggregation network top fpn libra r-cnn proposes balanced pyramid fuse feature level self-attention mechanism refine balanced semantic feature efficientdet present weighted bidirectional fpn conduct feature fusion augfpn implement consistent supervision close semantic gap feature various size cefpn sub-pixel convolutional downsampling reduce semantic gap different scale ac-fpn adaptively capture semantic localization information using attention-guided module enhance discriminative ability feature representation midf novel remote sensing text-image retrieval rsctir framework based global local information design multi-level information dynamic fusion midf module efficaciously integrate feature different level midf leverage local information correct global information utilizes global information supplement local information dynamic addition two generate prominent visual representation mcrn multi-source cross-modal retrieval network mcrn based contrast learning generative adversarial network designed model establishes shared feature space modal entanglement multimodal shared encoding turn yield common representation multiple information source semantic level contrast propose semantic enhancement module semantic injection module cooperate solve problem inspired attention-guided module propose channel-spatial attention mechanism unlike attention-guided module channel-spatial attention two branch one spatial-attention powerful modeling capability self-attention model global context obtain global semantic information channel-attention branch applies weighting global channel information context effectively reduces channel information lost due convolutional downscaling apply channel-spatial attention original feature layer name semantic enhancement module sem sem focus augmenting original feature retain much semantic channel information possible beneficial detection task prevent rich semantic information thinned top-down fusion structure directly up-sample global semantic information fuse original feature map called semantic injection module feature confusion optimization alleviating cluttered feature fusion key realizing full potential model structure libra r-cnn performs refinement operation feature fusion reduce feature confusion cefpn weight fused feature map weight feature generating different level feature optimize final feature generating different level feature augfpn proposes method adaptive fuse level feature using set learnable parameter cbam applies feature refinement using channel attention module cam spatial attention module sam achieves significant performance improvement keeping overhead small inspired cam cefpn channel attention guided module cag weight fused feature generating different level feature map weight feature optimize final feature generating different level feature coordinate attention separate module aggregate information along spatial channel direction embedding positional information channel attention inheriting benefit channel attention capturing long-range dependency precise positional information gmu strategy learn fusion transformation multimodal source synthetic experiment gmu able learn hidden latent variable real scenario outperformed singlemodality approach unlike aforementioned method inspired gating unit introduced gating unit gated channel guidance module capture internal dependency feature map reduce feature confusion specifically weighted fusion feature different level feature fusion using gating unit selectively receive information feature different scale mitigate feature confusion methodology overall structure mse-fpn shown fig goal use semantic enhancement module semantic injection module narrow semantic gap different feature layer obtain better feature fusion moreover employ gated channel guidance module alleviate confusion effect feature fusion describe three module detail figure overview mse-fpn semantic enhancement module sem used extract integrate global semantic information semantic injection module sim map global semantic information feature corresponding level gated channel guidance module gcg performs feature fusion weighted optimization feature full size image semantic enhancement module fpn residual network usually utilized backbone network construct feature map different resolution 2048\ lower-resolution image c_4 c_5\ contain rich semantic channel information higher-resolution image c_2 c_3\ contain rich spatial information shown fig top-down fusion strategy fpn gradually thins rich semantic information contained high-level feature layer moreover simple 1\times convolution dimensionality reduction obtaining f_i\ c_i\ lead loss channel information make rich channel information c_i\ fully utilized lost channel information retained channel information may importance detection task tackle problem propose semantic enhancement module sem obtain global context concatenating feature different level integrating refining different spatial channel information make full use information original feature layer c_i\ use channel-spatial attention model spatial information global context extract rich semantic information weight channel information global context decrease information loss f_i\ channel input sem global context obtained downsampling original feature layer c_2 c_3 c_4 c_5 backbone network uniform scale concatenating along channel dimension use adaptive averaging pool decrease feature map different level 1/64 input size downsampling effectively reducing computational effort without much loss accuracy shape global context c_n denotes number layer original feature layer c_2 c_3 c_4 c_5\ thus n=5 size c_n\ aligned c_n= i=2 c_i aligned aligned concat downsample c_2 c_3 c_4 c_5 aligned specifically channel-spatial attention two branch first branch channel-attention branch weight channel information global context global average pooling global context used generate feature map use two convolution exchange channel information sigmoid obtain channel weight m_w\ process expressed aligned m_w=\sigma fc_1 fc_2 avgpool aligned represents sigmoid function refers relu function fc_1\ fc_2\ represent fully connected operation second branch spatial-attention branch powerful modelling power attention model global context obtain global semantic information capture semantic dependency difference introduce global attention module based self-attention mechanism different self-attention mechanism global pooling feature map performed obtain global position encoding encoding latter feature map pay attention relationship related feature layer therefore output feature global attention module clear semantics contain context dependency surrounding object shown fig shape global context c_n use convolutional layer w_q\ w_k\ respectively transform potential space transformed feature map aligned w_qm aligned aligned w_km aligned shape c_n^ reshape c_n^ n=h obtain relationship different feature layer compute correlation matrix shape reconstruct normalizing group going sigmoid average pooling build matrix aligned =\sigma avgpool =\sigma avgpool q^tk aligned time utilize convolutional layer w_v\ transform feature map another representation aligned v=w_vm aligned shape c_n finally feature map multiplied dot-product element-wise product performed m_w\ feature map added obtain global semantic express function aligned aligned represent dot-product represent element-wise product semantic enhancement module information channel exchanged channel information weighted modeled global context using channel-space attention enhance semantic information figure architecture semantic enhancement module full size image semantic injection module obtaining global semantic information directly fuse original feature map c_2 c_3 c_4 c_5\ avoid semantic information thinned top-down feature fusion process however significant semantic gap exists original feature layer global semantic simple convolutional layer would effectively fuse global semantic information original feature layer therefore introduce semantic injection module sim alleviate semantic gap shonw fig sim take original feature map c_2 c_3 c_4 c_5\ global semantic input original feature map c_2 c_3 c_4 c_5\ generate feature injected convolutional layer global semantic sent convolutional layer group normalization layer sigmoid layer input generate semantics time global semantic also convolutional layer normalized three output channel size original feature map c_2 c_3 c_4 c_5\ injected global semantics form matrix multiplication global semantics added new feature injection semantic injection original feature level obtain semantic information localization information feature map different scale alleviates semantic gap different feature level figure illustration semantic injection module full size image gated channel guidance module semantic difference mapping feature across scale fusing feature scale lead confounding effect confusing localization recognition proposed sim incorporates cross-scale feature mapping resulting severe aliasing effect original fpn reduce aliasing effect negative impact inspired gating unit propose gated channel guidance module gcg selectively accept semantic information contained feature higher-level feature refining fused feature make discriminative feature map c_i i=2,3,4,5 partitioned two direction width height illustrated fig pooling kernel size perform global average pooling feature map obtain f_h\ f_w\ aligned =avgpool h,1 and\ =avgpool aligned next f_h\ f_w\ passed fully connected layer respectively feature weight obtained sigmoid function finally dot-product f_i\ feature weight get f_gated\ process expressed aligned gated =\sigma fc_ fc_ aligned represents sigmoid activation function represents matrix multiplication represents dot-product operator fc_1\ fc_2\ represent fully connected operation denotes pyramid level index figure illustration gated channel guidance module gcg full size image experiment dataset evaluation metric experiment implemented coco dataset contains category image training train-2017 image validation val-2017 also unlabeled image test-dev describe ablation investigation final result val-2017 test-dev training model train-2017 result reported following typical coco-style average precision measurement table comparison baseline state-of-the-art method developed coco test full size table implementation detail fair comparison experiment implemented based mmdetection resize image input process training 1\times schedule denotes epoch epoch 2\times schedule model trained gpus image per gpu initial learning rate default 0.02 1\times schedule learning rate drop factor 0.1 8th 11th round respectively 2\times schedule drop factor 0.1 16th 22nd round specifically stated hyper-parameters follow mmdetection basic setting main result section evaluation mse-fpn operationalized coco test development set compared state-of-the-art detector since mmdetection upgraded version 2.0 re-implemented corresponding baseline method fpn mmdetection fair comparison table demonstrates single-stage detector retinanet observed significant improvement performance 36.3 37.7 using mse-fpn faster r-cnn using resnet50 backbone employed performance improved 39.4 replacing fpn mse-fpn 2.1 point higher faster r-cnn based resnet50-fpn also evaluated proposed method resnext101-32x4d resnext101-64x4d feature extractor fpn-based method achieved 40.5/41.8 mse-fpn achieved 42.2/43.4 result demonstrate mse-fpn improve performance even stronger backbone network moreover model also brings overall improvement ap_s\ ap_m\ ap_l\ result small medium large object respectively especially ap_s\ demonstrates effectiveness proposed method capturing feature useful detection task improvement show mse-fpn effective furthermore compared mse-fpn state-of-the-art detector however due mmdetection v2.0 performs better version 1.0 re-implemented libra r-cnn augfpn mmdetection v2.0 fair comparison compared data libra r-cnn study final performance re-implemented result similar shown table using resnet-101 backbone network schedule mse-fpn-based faster r-cnn obtained 41.2 libra r-cnn augfpn obtained 40.2 40.6 respectively schued fully trained mse-fpn obtained 41.6 libra r-cnn augfpn obtained 41.0 41.5 respectively libra r-cnn yet reached performance mse-fpn schued augfpn 0.3 mse-fpn schued validates method doe require lot training time achieve satisfactory result summary experimental result show mse-fpn achieve competitive performance state-of-the-art detector libra r-cnn augfpn table effect component mse-fpn tested coco val-2017 full size table ablation experiment conducted ablation experiment evaluate significance component mse-fpn overall result ablation experiment shown table gradually added sem sim gcg resnet50-fpn fasterr-cnn baseline since sem sim cooperative one module used separately simply replaced 1\times convolutional layer training process followed 1\times schedule epoch ablation experiment performed setting fair comparison table ablation experiment accepting feature layer different scale full size table ablation study semantic enhancement module section discus sem two aspect accepting input different scale feature layer residual attention module sem table show result experiment conducted feature layer different scale input sem found best performance achieved using feature map 1/4 1/8 1/16 1/32\ amount computation feature map 1/16 1/32\ computation minimal performance worst experiment chose feature map 1/8 1/16 1/32\ achieve balance accuracy computational cost shown table tested sem using three scheme fairness changed output feature map channel number adding convolutional layer sem output show using spatial attention module alone improves 0.7 show using channel attention module alone improves 0.8 represents combination scheme scheme improves 1.2 scheme achieves compelling performance compared baseline experimental demonstrated proposed sem fully utilize channel spatial information brought original feature layer facilitate object detection task table ablation experiment semantic enhancement module full size table ablation study semantic injection module shown table value sim alone 1.3 higher baseline value using conv table show improved 1.9 sim sem used together result indicate two module closely related complement table ablation experiment semantic injection module 1\times conv full size table section discus effect sim 1\times convolutional layer mse-fpn result presented table demonstrate using 1\times conv alone lead relatively low value indicating traditional 1\times convolutional layer may lose channel information reducing dimension add 1\times convolutional layer sim module value using sim module alone therefore sim+ 1\times conv doe affect network performance increase computational cost network parameter also show method fully utilizes channel information c_i\ feature layer global semantic feature obtained sem small increase parameter computation narrowing semantic gap different feature layer effectively replacing traditional 1\times convolutional layer ablation study gated channel guidance module mitigate effect aliasing gcg weighted fusion feature different level feature fusion using gating unit selectively receive information feature different scale mitigate feature confusion according table proposed gcg method achieves improvement 1.2 performance compared baseline also investigate different effect different attention configuration ablation experiment first replaced gated channel guidance module gcg channel attention guided module cag coordinate attention respectively feature fusion shown table cag computationally intensive simple linear layer refine channel feature performance gain pronounced 0.7 due lack focus spatial information embeds positional information channel performance gain significant 0.9ap improvement large number convolution pooling operation lead slight increase computation gcg pooling operation linear mapping embed location information channel simple gating unit selectively retain information useful detection task ensures performance 1.2 improvement compared baseline reduces computation also see metric table gcg outperforms module table ablation experiment different attention module coco val-2017 full size table conclusion paper analyze fpn intrinsic issue discover large semantic gap different feature layer directly fusing feature result feature confusion original multi-scale feature fully utilized channel information lost dimensionality reduction advanced feature tackle issue propose novel multi-scale semantic enhanced feature pyramid network mse-fpn consists three simple yet effective component specifically use semantic enhancement module sem extract global semantic information feed semantic injection module sim narrow semantic gap different feature layer alleviate loss channel information gated channel guidance module gcg introduced alleviate aliasing effect different feature layer experiment show mse-fpn substantially improve baseline method challenging coco dataset