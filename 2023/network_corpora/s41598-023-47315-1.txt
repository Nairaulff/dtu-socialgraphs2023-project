introduction graph usually contain rich node feature edge feature however recent year majority advanced gnn model primarily focused enhancing learning node feature ignoring synchronous learning edge feature although aggregation function designed based message passing neural network framework mpnn aggregate node feature edge feature achieve good result specific application scenario using predefined aggregation function like manual feature engineering applied case therefore hope achieve method learn multi-dimensional edge feature iteratively synchronize update multi-dimensional edge feature process node information aggregation giving full play role node feature edge feature edge feature conventional graph convolution neural network gcns shown fig represented adjacency matrix represented binary indicator variable one-dimensional real value express rich edge information figure show multi-dimensional edge feature representation proposed edge feature longer represented one-dimensional real value simple adjacency matrix represented learnable feature vector express rich edge information updated across layer figure edge feature representation node feature representation ordinary gcn edge feature representation node feature representation used proposed cen-dgcnn full size image explanation symbol fig assuming graph consisting node represent node correspond edge feature representation edge ij\ ordinary gcn me-dgcnn respectively represent feature vector node ordinary gcn me-dgcnn n\times n\times n\times represent edge feature matrix two-dimensional tensor ordinary gcn multi-dimensional edge feature matrix three-dimensional tensor me-dgcnn respectively use notation indicate selecting entire range slicing along respective dimension therefore scalar ij\cdot feature vector denote feature edge ij\ n\times n\times represent node feature matrix two-dimensional tensor ordinary gcn me-dgcnn respectively i\cdot i\cdot denote feature vector node seen ordinary gcn denote presence absence edge n\times adjacency matrix node information filter cen-dgcnn proposed use -dimensional feature vector represent edge feature n\times n\times edge feature matrix used multi-channel filter node information existing gnn method mainly focus effectively obtain accurate node feature ignoring use edge information although message passing neural network mpnn framework proposed gilmer allows edge information node information participate message passing process advanced model still focus node feature ignore edge feature kipf simplified spectral convolution approximating chebyshev polynomial graph laplace operator proposed graph convolution network gcns based non-spectral method gcn simplifies convolution filter limiting receptive field 1-hop neighbor node process information aggregation doe take account different relationship node 1-hop neighbor node doe consider edge feature many scene edge different category label example social network edge labeled friend relationship family relationship work relationship therefore schlichtkrull proposed relational graph convolutional network r-gcns generalize gcn graph data multiple relationship aggregate information according type edge gcn method use laplace operator adjacency matrix aggregate node information without taking account different connection weight different node pair veličković proposed graph attention network gat give weight different node pair according characteristic train weight coefficient associated neighbor node essence weight gat function node feature attention weight coefficient two node connected edge calculated feature vector two node gat stronger adaptability fusion node feature structural feature achieves better result however edge also contains rich information consequently aspire develop gnn model concurrently learn node feature edge feature model facilitate learning node feature based knowledge acquired edge feature approach enable node acquire precise comprehensive information also learning edge embeddings enhance representation edge feature main reason success existing shallow gcn model application scenario node feature mainly rely short-range information local neighborhood example social network friendship limited small world receptive field extended local neighborhood node several hop stacking several layer gcn stacking layer may even lead over-smoothing over-squeezing instead make performance network drop sharply over-smoothing problem also exists continuous-time gnns field one drawback gnns fixed aggregation distance determines number node considered relevant node determined number layer gnn model scale network becomes larger node feature special application background need consider remote node dependency deeper gcn needed expand receptive field example prediction molecular chemical property may require atomic combination opposite side use 56-layer graph convolution neural network constructed segment point cloud data semantically achieve better performance shallow network larger graph mesh also need deep gcn capture remote dependency node still two problem training deep gcn one phenomenon over-smoothing recursive neighborhood aggregation model node aggregate almost global node information cause characteristic node become indistinguishable second phenomenon excessive squeeze network deep many iteration aggregate information large number neighborhood node over-compressed fixed-size vector may lead information distortion make performance deep gnn network model inferior shallow model rusch proposed alleviating over-smoothing necessary condition construction deep gnns pursuit enabling node effectively aggregate information distant node thus acquiring non-local structural feature sophisticated node feature objective establish deep graph neural network framework fulfills criterion mitigating issue over-smoothing over-squeezing propose co-embedding edge node deep graph convolutional neural network cen-dgcnn addressing problem abandon method using binary indicator variable one-dimensional real value represent edge feature conventional gcns introduce multi-dimensional edge embedding representation make full use edge information new message passing framework proposed integrate multi-dimensional edge feature node feature allowing full use node information edge information time order meet application scenario need capture remote dependency node also construct message passing framework introduces idea residual connection dense connection based framework deep graph convolution neural network designed mine remote dependency relationship node addition also construct new graph convolutional layer layer learn node feature edge feature simultaneously updated iteratively across layer edge learning node learning integrated convolution layer greatly improves efficiency model reduces complexity model experimental result demonstrate proposed method attains state-of-the-art performance node classification link prediction task particularly datasets directed edge contribution paper follows propose new message passing framework enables simultaneous aggregation multi-dimensional edge node feature introducing idea residual connection dense connection construction deep graph convolutional neural network model realized used capture long-range dependency non-local structural feature node eliminate limitation conventional gnns use binary variable one-dimensional real value represent edge feature propose multi-dimensional edge feature representation method approach edge embeddings encode rich edge information updated iteratively across graph convolution layer design new graph convolutional layer process node edge embeddings parallel allowing edge feature updated based node feature attention mechanism additionally use multi-dimensional edge feature matrix construct multi-channel filter filtering node information introducing identity mapping mechanism prevent over-smoothing handle directed graph missing edge feature propose multi-dimensional edge feature encoding method multi-channel filter construction method take account directionality edge experimental result demonstrate effectiveness method rest paper organized follows section related work provides brief overview related work deep graph convolution network edge learning section proposed method cen-dgcnn present detail proposed cen-dgcnn model section discussion give brief discussion section experiment present experimental result finally section conclusion conclude paper summarize contribution related work deep graph neural networks-related work order capture long-range dependency non-local structural feature node hope build deep gcn model model deep usually appears phenomenon over-smoothing over-squeezing node representation become indistinguishable distorted resulting great degradation network performance many method proposed deepen gcn existing research method mainly divided three category architecture modification graph normalization random dropping next introduce method architecture modification architecture modification existing method mainly introduce residual connection convolutional neural network cnn gcn borrowed concept cnn applied method residual connection dense connection dilated convolution gcn architecture successfully trained gcn depth layer proved effectiveness model point cloud semantic segmentation task chen also borrowed concept residual connection introduced initial residual graph neural network established graph convolutional network model depth layer achieved good result proposed jumping knowledge network differ common neighborhood aggregation network aggregate information previous layer layer instead last layer residual connection combine output layer graph normalization similarly many study carried around regularization normalization method try deepen gnn zhao proposed new normalized layer pairnorm applied middle layer training prevent node embedding similar experiment large data set show pairnorm obviously better shallow model zhou pointed stacked multi-layer gcn propagation operation transformation operation performed layer graph convolution previous study focused study propagation operation alleviate performance degradation deep gcn model research transformation operation zhou found contribution performance degradation deep model even greater propagation operation proposed variance control technique called nodenorm also found normalization technology play important role training depth gcn proposed message normalization layer called msgnorm zhou clustered node multiple group applied differentiable group normalization dgn node group separately random dropping machine learning model complex many parameter number training sample small easy produce over-fitting phenomenon dropout effectively alleviates overfitting problem model randomly discarding hidden unit neural network preset probability make possible train deeper network field gnn also inspired part work introduces idea dropout rong proposed dropedge eliminate over-smoothing problem deep graph convolutional neural network randomly deleting certain number edge graph training epoch huang also proposed train model removing node dropnode since node deleted edge connected also deleted dropnode regarded special form dropedge two approach viewed data enhancer message passing reducer edge-related work since much real-world data non-euclidean form graph representation learning made tremendous progress recent year current graph representation learning method roughly divided three category matrix factorization random walk graph neural network method based matrix decomposition computationally expensive method based random walk also difficult apply large-scale graph graph neural network method effectively solve problem widely used recent year although graph representation learning achieved success many field method ignore edge information order utilize edge information following method successively proposed implicit simple edge information utilization standard gcn method aggregate first-order neighbor node information neighboring node judgment based whether edge typically edge represented either connection connection edge feature considered binary indicator variable represent whether edge exists alternatively scalar used represent weighted edge adjacency matrix contains value indicate strength connecting edge aggregate information based different type edge many specific scenario edge labeled different type annotation example social network edge marked friend relationship colleague relationship classmate relationship relative relationship etc common approach aggregate information separately based different edge type schlichtkrull proposed relational graph convolutional network r-gcns order solve disadvantage ordinary gcns consider relationship node information aggregation process specific message passing model follows l+1\right =\sigma r\in j\in l\right l\right l\right l\right l+1\right represents output feature node layer r-gcn l\right l\right represent output node layer denotes activation function expression represents set relation represents set first-order neighbor node connected node relationship category denoted regularization constant value l\right denotes weight parameter matrix employed layer model linear transformation neighbor node relation category matrix used transform feature neighbor node connected edge type l\right represents weight parameter matrix associated node layer contrast typical gcns aggregate message first-order neighbor node uniformly r-gcns aggregate message various type first-order neighbor node differentially however method address edge certain type handle edge multi-dimensional feature multi-dimensional edge feature aggregation previously mentioned method effectively utilize handle multi-dimensional edge feature study exploring use multi-dimensional edge feature common method aggregate multi-dimensional edge feature neighbor node feature together aggregation function transfer target node information aggregation phase corso incorporated edge feature aggregation message passing neural network mpnn framework specific message passing framework outlined t+1\right =u\left t\right array i\right e\end array m\left t\right j\to t\right j\to denotes multi-dimensional feature edge j\to t\right represents feature representation node graph convolution layer represent message function vertex update function respectively represents aggregator aggregate neighbor node information way equation messaging framework mpnn comparing two equation see corso made full use multi-dimensional edge feature basis mpnn framework introduced edge feature information aggregation process t+1\right =u\left t\right array i\right e\end array m\left t\right t\right order make full use edge information mahbub introduced multi-dimensional edge feature process information aggregation also used edge feature calculate attention coefficient node proposed edge aggregated graph attention network egret ordinary gat feature two node calculate attention coefficient two node egret feature two node also combine edge feature two node specific attention coefficient calculated follows =\omega represents attention coefficient node learnable parameter matrix represent feature vector node node respectively represents edge feature directed edge node node symbol indicates concatenation operation represents activation function addition egret also applies edge feature process information aggregation aggregate edge feature neighbor node feature feature representation node final representation processed edge aggregation graph attention layer =\sigma j\in +\sum_ j\in represents neighbor node node represents softmax normalization following bahdanau represent learnable parameter matrix denotes activation function meaning symbol see egret applies edge feature neighbor node feature feature update central node make full use edge information edge embedding learning method use initial edge feature learn edge feature iteratively real-world application edge information may composed complex feature vector multiple factor influence edge feature simple handcrafted edge feature may sufficient accurately capture utilize inter-node relationship within graph consequently model may fail fully exploit information available graph data address limitation following method take multi-dimensional edge feature input iteratively update layer graph neural network model learn edge embedding representation inspired linegraph graph theory jiang proposed convolution edge-node switching graph neural network censnet kind network alternately learn node embedding edge embedding censnet build auxiliary graph changing node original undirected graph edge line graph edge also transformed node censnet alternately train model original undirected graph auxiliary graph update node embedding edge embedding censnet different method learn node embeddings embed node edge latent feature space yang proposed model called nenn incorporates node edge feature gnn take advantage rich edge information nenn adopts hierarchical dual-level attention mechanism node-level attention layer edge-level attention layer alternately stacked learn node embeddings edge embeddings unlike censnet handle directed large graph due approximated spectral graph convolution nenn spatial domain-based graph convolution address limitation nenn extends range adjacent node neighbor edge range adjacent edge neighbor node additionally nenn introduces attention mechanism learn effective embedding representation wang highlighted majority gcns designed using single-dimensional edge feature fully exploit abundant edge information present graph address proposed multi-dimensional edge-enhanced graph convolutional network me-gcn semi-supervised text classification me-gcn edge feature treated multi-stream signal stream performs distinct graph convolution operation effectively integrating rich graph edge information across entire text corpus context skeleton-based motion recognition application several gcn-based model developed capture adaptive correlation constructing multiple edge matrix proposed method cen-dgcnn architecture cen-dgcnn section propose co-embedding edge node deep graph convolutional neural network cen-dgcnn first define notation used chapter let graph node node feature represented n\times matrix edge feature represented n\times n\times tensor use index subscript represent element matrix tensor example i=1 j=1 represents channel -dimensional feature vector node graph ijp j=1 p=1 represents channel -dimensional feature vector edge subscript use indicate selection entire range slice corresponding dimension example i\cdot indicates -dimensional feature vector node graph ij\cdot indicates -dimensional feature vector edge edge doe exist set ij\cdot ... table summarizes symbol used paper table table symbol used paper full size table figure depicts architecture cen-dgcnn model input graph initial node feature n\times edge feature n\times n\times superscript denotes layer output n\times represents shape node feature matrix output layer n\times n\times represents shape -channel edge feature matrix reduce influence edge noise input graph n\times n\times pre-processed double random normalization input cen-dgcnn first cen-dgcnn graph convolution layer generates new edge feature n\times n\times input node feature edge feature n\times n\times used multi-channel filter perform graph convolution operation n\times yielding n\times node feature edge feature output first graph convolutional layer used input second graph convolutional layer edge feature updated generate n\times n\times n\times n\times used multi-channel filter perform graph convolution operation n\times generate n\times process repeated subsequent layer graph convolutional layer nonlinear activation applied node feature matrix resulting corresponding -dimensional node embedding extract refined high-order feature non-local structural feature node use deep graph convolution neural network structure model depth set layer experimental verification avoid over-smoothing over-squeezing caused model deepening node layer aggregate part initial node information previous layer node information addition information neighboring node fig concatenation operation represented symbol used combine node information previous layer output initial input passing layer cen-dgcnn obtain output graph composed n\times n\times n\times shown rightmost box fig output node feature used downstream task node classification graph prediction output edge feature used task edge classification edge prediction figure overall architecture co-embedding edge node deep graph convolutional neural network cen-dgcnn full size image cen-dgcnn several structural difference traditional gcn specifically cen-dgcnn fully utilizes edge feature information associating edge attribute edge feature vector representing edge using multi-dimensional feature vector rather one-dimensional edge feature binary edge indicator used ordinary gcn cen-dgcnn employ multi-dimensional edge feature matrix shown fig multi-channel filter instead using binary adjacency matrix one-dimensional positive matrix single-channel filter like ordinary gcn cen-dgcnn learns edge feature learnable parameter adjusted across layer rather using adjacency matrix layer normal gcn cen-dgcnn distinguishes shallow gcn structure employing deep architecture effectively mitigating over-smoothing over-squeezing issue cen-dgcnn performs node embedding edge embedding parallel layer effectively fusing node edge feature graph convolution message passing framework currently three primary general framework graph neural network message passing neural network mpnn non-local neural network nlnn graph network mpnn framework node representation obtained iteratively propagating message message update function round message function proposed mpnn aim aggregate node information neighboring node edge gnn model based mpnn framework aggregate edge information due lack edge feature nlnn framework general summary graph neural network model based attention mechanism graph attention network gat considered special case proposes comprehensive model proposed cen-dgcnn model need meet application scenario large-scale graph need capture non-local structural feature node also need aggregate multi-dimensional edge feature therefore message passing framework adopted cen-dgcnn need meet following three requirement ability extract non-local structural feature ability prevent over-smoothing ability aggregate multi-dimensional edge feature meet requirement propose new graph neural network message passing framework follows l+1\right =\sigma w\in n\left v\right 0\right l-1\right l\right l\right l\right represents activation function represents aggregation function layer framework applicable scenario building deep graph convolutional network model simultaneously aggregate neighbor node feature edge feature central node seen equation layer node feature l+1 aggregate initial node feature 0\right node feature l-1\right layer node feature l\right layer well neighbor node feature l\right layer edge feature connected node adopt idea residual dense connection aggregate initial previous layer feature effectively alleviating over-smoothing increasing network depth connecting output across layer proposed novel message passing framework shown fig iteratively apply graph convolution aggregate feature remote node obtain non-local structural high-order node feature additionally aggregate multi-dimensional edge feature graph convolution process elaborated following section figure novel graph neural network message passing framework proposed full size image new framework simultaneously aggregate neighbor node information multi-dimensional edge feature central node suitable construction deep graph convolutional neural network model framework draw lesson idea residual connection dense connection layer node feature aggregate part initial node feature previous layer node feature effectively avoids problem over-smoothing framework following three characteristic node non-local structural feature refined high-order feature obtained over-smoothing problem effectively avoided multi-dimensional edge feature aggregated central node graph convolution layer section outline graph convolution layer cen-dgcnn traditional gcn model regard edge binary indicator variable one-dimensional real value completely ignoring rich information contained edge large number application scenario edge contain information attribute type connection strength traditional graph neural network express rich edge information incorporate edge information model layer traditional gcn model original adjacency matrix single-channel filter node feature filtering bring two problem first original adjacency matrix may contain noise optimal filtering second edge feature fully incorporated model although complex graph convolutional model extract fine node feature repeated use simple adjacency matrix may contain noise filter layer node feature limit effectiveness filtering operation address issue introduce following information aggregation operation based new messaging framework proposed previous section l\right =\sigma array p\\ p=1\end array l\right l-1\right l-1\right l\right l-1\right l\right l-2\right l\right 0\right defines output layer graph convolution i.e. node feature matrix output layer represents nonlinear activation function represents aggregation function layer function used generate multi-channel filter shape n\times n\times represents slice channel multi-channel filter moreover l\right represents node feature transformation function layer concatenation node feature slice channel indicated symbol feature transformation function linear mapping shown usually used frequent interaction different dimension feature matrix degrades performance model therefore adopting linear map feature transformation function suitable proposed deep graph convolutional model order ensure frequent interaction different dimension feature matrix deep model degrade model performance adopt identity mapping mechanism shown feature transformation function idea identity mapping add identity matrix weight matrix certain proportion weight identity matrix increase model deepens weight matrix attenuation change number layer parameter l\right x\right l\right l\right x\right =\left l\right =\mathrm log 0.5 +\lambda aggregation function definition represent weight parameter corresponding variable respectively adjusting weight parameter three variable aggregated according different weight =\upzeta +\eta +\theta proposed graph convolution layer effectively learn multi-dimensional edge feature incorporate process information aggregation allowing full utilization edge information model utilizes new multi-channel filter enabling graph convolution operation different channel edge feature additionally filter also reduce noise specific method discussed next section edge feature update based attention mechanism section describe learning multi-dimensional edge feature construction multi-channel filter shown fig edge feature l+1 layer updated according feature two node connects time shown fig node aggregate neighbor node feature edge feature update feature edge feature update node feature update performed simultaneously graph convolutional layer figure edge feature update node feature update cen-dgcnn full size image l\right function used generate multi-channel filter layer model function attention mechanism construct filter attention mechanism adopted existing gnn model improved based gat attention coefficient gat depends feature node end edge order make full use edge information fusion node feature edge feature realized cen-dgcnn edge feature two node feature connected edge learn attention coefficient since multi-dimensional edge feature adopt multiple feature channel conduct separate attention learning channel assuming construct single-channel filter dimension edge feature -dimensional feature channel filter l\right feature channel model function previous layer node feature l-1\right previous layer edge feature l-1\right define filter based attention mechanism follows ijp l\right l-1\right ijp l-1\right =\frac exp relu i\cdot l-1\right j\cdot l-1\right k\in exp relu i\cdot l-1\right k\cdot l-1\right ijp l-1\right equation defines filtering parameter channel filter graph convolutional layer cen-dgcnn i\cdot j\cdot represent feature vector node node respectively learnable parameter matrix adjusts output dimension node feature vector symbol represents concatenation operation used concatenate two vector weight vector project concatenated vector scalar relu represents leakyrelu\ activation function also apply regularization function used compute attention coefficient calculate attention coefficient edge feature channel multiply corresponding edge feature matrix graph convolution layer update attention coefficient edge feature according new node feature update equation edge feature matrix given follows l\right l\right since edge feature generate random noise learning process graph data various application scenario may also noise need denoise edge feature matrix learned layer wang proposed use doubly stochastic matrix network enhancement confirmed network denoising performance doubly stochastic matrix experiment prove network enhancement method proposed remove weak edge enhances real connection make downstream task perform better improve performance cen-dgcnn apply doubly stochastic normalization edge feature matrix layer final edge feature matrix obtained normalization follows ijp =\frac ijp k=1 ikp ijp =\sum_ k=1 ikp jkp v=1 vkp edge feature matrix undergoes doubly stochastic normalization operation sum row column feature channel edge feature matrix p=1 indicating matrix left-stochastic right-stochastic cen-dgcnn proposed deep structure edge feature matrix multiplied multiple time across layer normalized doubly stochastic matrix make cross-layer update process stable discussion previously mentioned objective develop graph neural network model effectively learn multi-dimensional edge feature simultaneously learning node feature edge feature graph convolution layer extract finer non-local structural feature node classify node different category accurately possible aim construct deep graph convolutional network model however network deep over-smoothing over-squeezing occur causing feature node tend consistent node become indistinguishable additionally require gnn model capable handling directed graph meet requirement application scenario involve directed graph address issue propose cen-dgcnn deep feed-forward graph convolutional network model utilizes multi-dimensional edge feature vector instead traditional adjacency matrix node information filter use multi-dimensional edge feature vector construct multi-channel filter better capture node feature graph convolutional layer cen-dgcnn node embedding learning edge embedding learning carried simultaneously respective model architecture used learn node feature edge feature furthermore node feature edge feature learned parallel layer used mutually assist learning edge feature node feature respectively node information aggregation update employ deep gcn structure extract non-local structural feature high-order feature node mining long-range dependency avoid over-smoothing over-squeezing problem associated gcn introduce concept residual dense connection node information aggregation process adopt identity mapping linear transformation multi-dimensional edge feature flexibly utilized design graph convolution filter propose multi-channel filter efficiently handle directed graph data technique cen-dgcnn achieves impressive result research certain limitation inclusion multi-dimensional edge feature increase number parameter making challenging cen-dgcnn handle large-scale network future work focus enhancing model investigating strategy apply cen-dgcnn effectively large-scale network application experiment section conduct node classification link prediction task various datasets compare result multiple baseline method demonstrate cen-dgcnn effectively capture precise node feature multi-channel filter constructed multi-dimensional edge feature matrix moreover conduct ablation experiment validate significance necessity component cen-dgcnn furthermore quantitatively ass smoothness layer model establish cen-dgcnn effectively mitigates over-smoothing adopting deep architecture lastly construct directional multi-channel filter tailored dataset characteristic demonstrate superiority multi-channel filter based multi-dimensional edge feature node classification effectiveness multi-dimensional edge feature confirmed comparing various edge feature encoding method model setting experiment run computer 12-core intel i7-12700kf cpu ram nvidia geforce rtx gpu use pytorch implement method implemented construction 64-layer cen-dgcnn model output dimension input layer intermediate hidden layer set dimension use adam optimizer learning rate 0.005 weight decay parameter 0.0005 model optimization dropout rate 0.2 applied input output feature model training batch size set maximum number epoch set 10,000 early stopping epoch non-decreasing validation loss use leakyrelu activation function slope 0.2 hidden layer parameter weight matrix decay parameter set 0.5 learnable weight coefficient model learn automatically backpropagation process data set partitioning three citation network follows standard split disease spreading network flight network adopt division method described related work report average accuracy run node classification directed graph section encode multi-dimensional edge feature directed graph construct multi-channel filter learning direction-related multi-dimensional edge feature aim obtain accurate node feature subsequently compare node classification result various baseline method dataset section ass performance cen-dgcnn node classification task across five datasets comprise three citation network disease spreading network flight network table provides comprehensive overview specific parameter associated dataset brief introduction provided dataset cora dataset citation network composed machine learning related paper commonly used dataset node classification task dataset consists node node represents paper paper divided seven category case based genetic algorithm neural network rule learning reinforcement learning probabilistic method theory feature paper represented 1433-dimensional word vector dimension represents keyword field machine learning paper cora cite least one paper cited another paper cora digraph total reference relationship citeseer citation network dataset contains paper feature paper represented 3703-dimensional word vector six category paper namely agent hci citeseer total citation relation also digraph pubmed citation dataset consists scientific publication diabetes pubmed database divided three category publication described tf/idf weighted word vector dictionary unique word dataset total edge present representing directed citation relationship publication disease dataset simulates sir disease transmission model consists node representing individual classified either infected uninfected state node characterized 1000-dimensional feature representing individual susceptibility additionally dataset includes edge represent propagation path disease individual airport dataset simulates airport route node represent airport characteristic airport described four dimension latitude longitude height information gdp country/region airport belongs population country airport located serf classification label node airport airport divided four category accordingly dataset includes edge represent directed route connecting different airport table dataset statistic full size table multi-dimensional edge feature encoding data set contain rich node feature data set directed graph edge contain direction information direction edge also contains important information graph data many previous study treated benchmark data set undirected graph therefore order verify effectiveness proposed cen-dgcnn encode multi-dimensional edge feature according direction edge encode directed multi-dimensional edge feature vector ij\cdot ij\cdot =\left accordance encode edge feature three distinct edge feature channel three citation datasets namely cora citeseer pubmed three edge feature channel respectively represent citation relationship citation paper cited others disease dataset three edge feature channel denote transmission relationship transmission others transmission others respectively likewise airport dataset three edge feature channel signify existence route flight route return route respectively three-channel filter constructed based edge feature coding method effectively aggregate three type neighboring node information comparative analysis baseline ablation experiment reveals multi-dimensional edge feature coding method significantly enhances model performance compared commonly used undirected graph processing approach baseline perform extensive comparison cen-dgcnn three category state-of-the-art baseline method namely gnn-based method deep architecture-based method approach involving edge embedding learning gnn-based method consideration encompass gcn gat amc-gcn nigcn deep architecture-based approach evaluate dropedge nodenorm gcnii gdc deepgwc additionally include method employing edge embedding learning comprise censnet nenn egat result present comprehensive performance comparison cen-dgcnn baseline method table ass node classification task evaluate model based score given proposed cen-dgcnn adopts deep graph convolutional network architecture incorporates edge embedding learning method baseline method consist three category gnn model deep gnn model gnn model capable learning edge embeddings fair comparison maintain cen-dgcnn network structure across datasets important emphasize introduce additional edge information model rather utilize directed edge present benchmark datasets encode multi-dimensional edge feature consequently comparison proposed cen-dgcnn baseline method conducted fair manner experimental result presented table demonstrate cen-dgcnn singular structure attains best result across five datasets table comparison node classification accuracy gnn method highest accuracy highlighted bold full size table analysis comparing cen-dgcnn deep gnn baseline method observe superior performance deep gnn baseline method cen-dgcnn introduces learning multi-dimensional edge feature within context deep graph convolutional network architecture utilizing multi-dimensional edge feature matrix constructing node feature filter thus conclude gnns derive substantial benefit integration multi-dimensional edge feature comparison three gnn baseline method employing learnable edge embeddings cen-dgcnn demonstrates enhanced performance cen-dgcnn incorporates deep graph convolutional neural network architecture simultaneously learning multi-dimensional edge embeddings hence deduce deep model architecture significantly contributes performance improvement graph convolutional neural network distinguished prevalent neighborhood message passing method gnn cen-dgcnn adopts new message passing framework realizes non-local message passing dense connection seen result cen-dgcnn outperforming baseline method proposed new non-local message passing framework effective learning node feature representation link prediction section aim validate performance cen-dgcnn link prediction task evaluate cen-dgcnn link prediction performance five datasets including three citation network disease spreading network flight network baseline method select seven model known state-of-the-art performance adopt experimental setup vgae specifically use edge training set validation set test set generate negative sample unconnected false edge validation test set randomly sample edge respectively use area roc curve auc evaluation metric link prediction setting cen-dgcnn remain consistent previous experiment experimental result presented table indicating cen-dgcnn employing non-local message passing framework edge-node co-embedding learning structure outperforms method five datasets previous study kipf suggested deeper graph neural network might underperform compared shallow network research demonstrated using two-layer gcns encoders common approach link prediction task however integration proposed non-local message passing framework cen-dgcnn achieves 64-layer model structure successfully overcoming degradation issue associated excessively deep model surpassing performance shallow layer hence assert incorporating deep model architecture multi-dimensional edge feature contributes enhancement gcns performance table comparison link prediction accuracy gnn method highest accuracy highlighted bold full size table node classification result large-scale datasets section ass performance cen-dgcnn context node classification task conducted large-scale datasets given cen-dgcnn capability encode multi-dimensional feature based edge directionality selected directed graph dataset ogbn-arxiv open graph benchmark ogb evaluation additionally underscore efficacy encoding edge feature respect edge directionality included large-scale undirected graph dataset reddit assessment table provides overview specific parameter two extensive datasets table dataset statistic full size table model setting term model configuration primary objective conduct comprehensive comparison al. revgnn revgat-deep signifies deep version featuring layer channel 'revgat-wide represents wide version layer channel comparison undertaken underscore distinctive advantage cen-dgcnn hence experiment section adopt model architecture comprising layer hidden channel model trained maximum epoch implement early stopping strategy specifically training cease validation set loss fails decrease consecutive span epoch configuration devised demonstrate even limited number hidden channel cen-dgcnn still achieve outstanding result utilization multi-dimensional edge feature encoding thereby showcasing superiority ogbn-arxiv dataset employ identical dataset partition prescribed ogb open benchmark consists training validation testing case reddit dataset utilize standard data division comprising training validation testing implementation multi-dimensional edge feature encoding tailor approach citation direction within ogbn-arxiv dataset comprehensive description provided section multi-dimensional edge feature encoding within encoding process employ three distinct edge feature channel encapsulate citation relationship citation research paper instance cited others context reddit dataset characterized undirected graph transform reddit edge feature standard 1-dimensional adjacency matrix matrix edge channel serf indicator signifying whether two post received comment user thereby reflecting degree correlation remaining model configuration adhere specification outlined section model setting result analysis table provides detailed performance comparison cen-dgcnn various baseline method across two extensive datasets table employ bold formatting highlight best-performing result evident table cen-dgcnn demonstrates outstanding performance sizeable citation dataset ogbn-arxiv also achieving commendable result reddit dataset table result ogbn-arxiv reddit datasets highest accuracy highlighted bold full size table begin conduct comparative analysis revgcn-deep revgat-wide revgcn-deep represents deep version model comprising layer channel conversely revgat-wide represents wide-body variant characterized 5-layer model structure layer accommodating channel experimental finding becomes evident revgat-wide exhibit superior performance compared revgcn-deep observation suggests increasing number channel contributes enhanced performance within revgnn model whereas reducing number layer doe lead performance degradation consequently infer performance revgnn model predominantly influenced configuration channel quantity subsequently embark comprehensive analysis experimental result contrast revgnn treat ogbn-arxiv dataset undirected graph cen-dgcnn encodes edge based citation direction within dataset experimental finding reveal model depth cen-dgcnn outperforms revgat-wide utilizing approximately channel capacity serf evidence effectiveness multi-dimensional edge feature encoding showcasing capacity deliver superior result even reduced channel count given reddit dataset inherently undirected graph employ multi-dimensional edge feature encoding dataset nevertheless exceptional performance multi-dimensional edge feature encoding ogbn-arxiv datasets coupled result ablation experiment section different edge feature encoding method provides compelling evidence significant impact multi-dimensional edge feature encoding overall model performance enhancement conducted comparative analysis cen-dgcnn top-performing baseline model revgat-wide discussed detail section model complexity analysis complexity analysis increasing number channel result exponential increase model parameter complexity time complexity worth noting complexity analysis indicates linear growth number layer employing reduced channel count significantly curtail number parameter theoretically training time substantial reduction parameter training time achieved without compromising model performance therefore comparison revgnn model remarkable advantage cen-dgcnn maintain model accuracy greatly reducing number channel significantly reduce parameter complexity time complexity quantitative qualitative analysis node representation smoothness quantitative analysis evaluation metric metric smoothness mad quantitative metric proposed chen measure smoothness reason node over-smoothing model deep many graph convolution operation node almost aggregate information global node lead consistency feature node i.e. spatial distribution node feature becomes close therefore principle mad measure node smoothness calculating average average distance node node specific equation calculating mad follows mad^ tgt tgt u\left tgt tgt tgt 1\left tgt tgt tgt =1-\frac j\in 1,2 n\right\ mad tgt represents mad value target node pair u\left x\right =1\ otherwise tgt used calculate average non-zero element row tgt tgt represents n\times mask matrix represents information filtering operation mask matrix tgt multiply n\times distance matrix element-by-element element calculation distance matrix shown represents node feature matrix represents feature vector node element value distance matrix obtained calculating cosine value node pair pointed node feature matrix output last layer cen-dgcnn quantitative analysis order enable cen-dgcnn learn multi-dimensional edge embeddings aggregate long-range high-order node feature propose novel message passing framework based framework deep gcn model constructed however common model shallow structure model deep cause serious node over-smoothing problem order eliminate problem node over-smoothing introduce idea residual connection dense connection use identity mapping transform node feature technique problem node over-smoothing caused deep graph neural network model effectively solved section quantitatively measure node smoothness cen-dgcnn depth layer demonstrate effectiveness proposed method figure present mad value proposed 256-layer cen-dgcnn five datasets contrast fig show mad value gcn model deepened layer datasets higher mad value indicates higher degree differentiation node i.e. lower degree over-smoothing visualized quantized mad value heat map darker color indicate smaller degree over-smoothing lighter color mean severe node over-smoothing figure mad value different layer cen-dgcnn datasets full size image figure mad value different layer regular gcn datasets full size image shown fig smoothness node cen-dgcnn decrease slowly increase graph convolution layer fact mad value even increase number layer deepens indicating model avoid over-smoothing instance mad value 160th layer cen-dgcnn cora dataset higher 4th layer mad value 256th layer even higher 2nd layer citeseer dataset remarkably mad value cen-dgcnn remains stable 64th layer thus select 64-layer cen-dgcnn default model study finding demonstrate cen-dgcnn strong ability prevent over-smoothing deep model contrast illustrated fig conventional gcn model exhibit severe over-smoothing depth reach layer number layer increase node feature become indistinguishable making challenging model capture high-order node feature global structural information order quantitatively analyze over-smoothing problem also take node classification accuracy indicator evaluate over-smoothing elimination theoretically model becomes excessively smooth number layer deepens accuracy node classification decline accordingly thus examine variation node classification accuracy cen-dgcnn concerning number layer table present node classification result different layer model three citation datasets specifically compare three model gcn gcnii deep structure egat learning edge embedding cen-dgcnn table summary classification accuracy result various depth highest accuracy highlighted bold full size table table observe performance gcn egat address issue over-smoothing gradually diminishes model depth increase particularly evident citeseer dataset significant performance drop model exceeds layer however gcnii cen-dgcnn constructed deep model architecture suffer performance degradation increasing model depth contrary achieve optimal result number layer deeper outcome demonstrates cen-dgcnn effectively address problem performance degradation associated excessively deep layer gnns additionally cen-dgcnn outperforms gcnii indicating proposed multi-dimensional edge embedding learning method contributes enhancing model performance qualitative analysis objective node classification task learn distinct node feature using gnn model type node similar feature different type node classification performance good node kind clustered together space different kind node highly differentiated space visualize high-order node feature learned cen-dgcnn two-dimensional space utilize t-sne algorithm reduce node feature high-dimensional two-dimensional examine two-dimensional space shown fig five column left right represent node feature distribution cora citeseer pubmed disease airport datasets respectively top bottom three row represent initial node feature 32nd layer output node feature cen-dgcnn 64th layer output node feature cen-dgcnn respectively figure t-sne visualization node representation learned cen-dgcnn full size image result fig demonstrate initial node feature five datasets highly entangled making difficult distinguish different type node causing category node randomly distributed space however initial node feature processed 32-layer cen-dgcnn type node become clustered distinguishable space iteration 64-layer cen-dgcnn clear boundary appears different type node five datasets type node tightly clustered together thus qualitative analysis suggests deep structure cen-dgcnn effectively mitigates node over-smoothing problem achieves remarkable node classification performance ablation experiment section carry ablation experiment two perspective proposed multidimensional edge feature encoding method new message passing framework prove effectiveness multidimensional edge feature encoding method new framework experiment section prove following three point proposed multi-dimensional edge feature encoding method significantly improve node classification accuracy achieving 55.19 improvement five datasets missing edge feature even without use novel messaging framework novel message passing framework robust deep graph convolutional neural network encoded low-dimensional edge feature case low-dimensional edge feature encoding using deep graph convolutional neural network model based new message passing framework improve node classification task 27.04–53.7 accuracy using new message passing framework multi-dimensional edge feature encoding method time achieve better classification result different edge feature encoding method section aim validate effectiveness proposed multi-dimensional edge feature encoding method missing edge feature network conventional gnns learn node feature edge feature whereas proposed cen-dgcnn update multi-dimensional edge feature vector across layer making possible learn various feature edge category attribute direction connection strength automatically however many datasets fewer edge feature edge citation network dataset contain direction feature address issue propose multi-dimensional edge feature encoding method shown edge directionality directed graph datasets verify effectiveness proposed multi-dimensional edge feature encoding method compare performance various low-dimensional edge feature representation construct low-dimensional edge feature representation three specific way single-channel edge feature construction method includes one-way edge namely single-channel edge feature construction method regarded undirected graph namely dual-channel edge feature construction method bi-directional edge namely node classification accuracy proposed multi-dimensional edge feature construction method low-dimensional edge construction method five datasets shown table table accuracy node classification different edge feature encoding method highest accuracy highlighted bold full size table table demonstrates proposed multi-dimensional edge construction method outperforms low-dimensional edge construction method node classification task figure show t-sne visualization result 64-layer cen-dgcnn using four edge construction method classify node cora dataset edge construction method left right t-sne algorithm reduces high-dimensional node feature output cen-dgcnn three-dimensional space seen fig seven type node outputted low-dimensional edge construction method closely clustered confused space however node feature obtained proposed edge construction method clearly classified space resulting best clustering effect similar node demonstrates multi-channel edge feature coding method better classification effect low-channel edge feature coding method figure node feature visualization result cen-dgcnn various edge feature encoding method cora dataset full size image analyze effectiveness multi-dimensional edge feature construction method enhancing model performance attributed utilizing edge feature matrix different channel filter aggregate node feature facilitates node acquiring comprehensive information equivalent aggregating node feature diverse edge dimension ultimately combining node feature aggregated across different dimension taking example three-dimensional edge feature representation method based edge directionality node engage message passing aggregation three edge direction edge feature learned updated layer eventually node feature based multiple edge direction combined node feature obtained approach contain significantly richer information compared conventional method filtering invariant adjacency matrix layer consequently utilization multi-dimensional edge feature empowers model better capture intricate relationship among node leading improved model performance effectiveness novel message passing framework order capture long-range dependency node obtain refined high-order node feature non-local structural feature propose new message passing framework shown demonstrate effectiveness conduct two set model comparison experiment first set model includes cen-dgcnn four different edge construction method using traditional mpnn framework instead novel message passing framework second set model also includes cen-dgcnns four different edge construction method proposed novel message passing framework node classification result eight cen-dgcnns variant model shown table experimental data seen proposed multi-dimensional edge feature construction method combined novel message passing framework achieves best node classification result moreover compared edge feature construction method node classification performance low-dimensional edge feature construction model significantly improved using novel message passing framework result suggest novel message passing framework robust deep graph neural network using low-dimensional edge feature construction method table node classification accuracy different message passing framework highest accuracy highlighted bold full size table figure show t-sne visualization result model table node classification citeseer dataset upper lower line represent message passing framework using mpnn novel messaging framework respectively column left right represents edge feature encoding method comparing first two column seen novel messaging framework performs strongly case low-dimensional edge construction node feature first two column upper row almost indistinguishable three-dimensional space first two column lower row achieve better node classification result seen last column best node classification result achieved using novel message passing framework multi-dimensional edge feature construction method figure node feature visualization result cen-dgcnn different message passing framework edge feature construction method citeseer dataset full size image model analysis section analyze model complexity cen-dgcnn explore effect simultaneously learning node embeddings edge embeddings model complexity additionally investigate sensitivity hyperparameter model performance moreover compare attention distribution several representative model highlight advantage cen-dgcnn model complexity analysis analyze complexity cen-dgcnn highlight advantage performance discus detail memory complexity parameter complexity time complexity full-batch gnn graphsage clustergcn fastgcn revgnn model reach layer compare complexity proposed cen-dgcnn model table summarize theoretical complexity model represents number model layer represents number node represents batch size node represents number hidden channel represents number neighbor sample node a\vert denotes sparsity graph specifically number non-zero element graph adjacency matrix represents dimension multi-dimensional edge feature encoding noteworthy experiment use double random matrix calculation denoising edge feature matrix may fact adversely affect model operational efficiency scenario hardware platform performance constrained recommend relocating double random matrix denoising operation data preprocessing stage adjustment help alleviate resource overhead model execution given negligible memory footprint occupied model parameter primary focus analysis center memory complexity required store intermediate node feature table comparison complexity full size table evident data presented table memory consumption revgnn independent depth consequently revgnn construct deeper model within memory space however demonstrated experimental result section result analysis model depth held constant cen-dgcnn achieves superior performance revgnn utilizing number channel employed revgnn observation hold great significance term reducing parameter complexity time complexity revealed data table parameter within model parameter complexity time complexity exhibit quadratic growth pattern increase number hidden channel result exponential rise number parameter training time conversely term introduced cen-dgcnn contributes linearly increment parameter complexity time complexity distinguishes cen-dgcnn ability exponentially reduce parameter complexity time complexity decreasing number channel outpaces linear complexity increase brought new term approach also integrated mini-batch sampling technique mitigate memory complexity term number node however practice learning multi-dimensional edge embeddings may accelerate model convergence speed leading shorter learning time compared low-dimensional edge embeddings figure illustrates comparison runtime cen-dgcnn model using edge feature constructed different dimension observed case 3-dimensional edge feature construction runtime necessarily longer case low-dimensional edge feature construction figure cen-dgcnn model analysis running time cen-dgcnn model different-dimensional edge feature construction attention weight distribution citeseer dataset performance cen-dgcnn influenced value hyperparameter full size image attention distribution demonstrate cen-dgcnn achieves higher attention score learning edge feature analyze attention score learned four model namely gcn gcnii egat cen-dgcnn first define discrepancy measure attention matrix node =\frac :\right degree represents uniform distribution score node used quantify deviation learned attention uninformative uniform distribution larger indicates learned attention score meaningful figure illustrates distribution discrepancy metric attention matrix learned four model citeseer dataset observed attention score learned cen-dgcnn exhibit larger variance indicates cen-dgcnn outperforms model better distinguishes important node learns corresponding attention score effectively parameter sensitivity analysis ensuring fair comparison experiment already introduced relevant experimental parameter setting section model setting section conduct sensitivity test significant adaptive decay parameter cen-dgcnn model figure illustrates node classification accuracy 64-layer cen-dgcnn concerning hyperparameter adjusting value control extent information decay model feature transformation stage fig observed value 0.5 model performance experience significant decline different datasets corresponding optimal value optimal value typically ranging 0.5 conclusion paper first introduce multi-dimensional edge feature representation method overcomes limitation conventional gnns use binary edge representation one-dimensional edge feature representation method enables update learning multi-dimensional edge feature across layer cen-dgcnn providing basis downstream task graph convolution layer multi-dimensional edge feature matrix also used multi-channel filter filter node feature updating multi-dimensional edge feature node feature synchronously model reduces complexity improves computational efficiency additionally propose novel message passing framework obtain refined high-order feature node capturing remote dependency node global structure feature cen-dgcnn based framework achieves deep network structure eliminates node over-smoothing problem thus performing better shallow structure analyze node smoothness cen-dgcnn quantitatively qualitatively layer proving perfectly solve problem node over-smoothing finally demonstrate superior performance cen-dgcnn compared large number baseline gnn model also prove efficacy multi-dimensional edge feature construction method new message passing framework ablation experiment aim apply cen-dgcnn area task future continue improve model