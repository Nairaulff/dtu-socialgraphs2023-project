introduction non-small cell lung cancer nsclc represents frequent form lung cancer treated mainly surgery modern radiotherapy therapeutic approach nsclc patient differ according tothe histological characteristic tumor patient condition treatment path patient locally advanced nsclc currently includes chemoradiotherapy possibly followed immunotherapy early-stage patient however surgical resection followed chemotherapy currently remains potentially curative treatment nonetheless 30–55 patient develop post-resection tumor recurrence within first year therefore early identification patient prone developing recurrence challenge currently still open would allow clinician plan accurate therapeutic surveillance plan several work proposed prediction recurrence-free survival overall survival nsclc patient however state-of-the-art lacking model designed early prediction disease recurrence furthermore although proposed model show encouraging result still suitable clinical application even involve genomic-based model expensive time-consuming procedure recent year artificial intelligence already demonstrated potential defining predictive prognostic model specifically predictive power radiomic feature extracted biomedical image well established scientific community recently radiomics via convolutional neural network cnns extensively used showing strong potential cnns two type custom pre-trained former scientist build network trained execute specific task latter case transfer-learning approach used network first trained million image different class e.g. imagenet recognizing specific pattern like edge dot color gradient shape etc gained knowledge transferred specific set image study work adopted transfer learning approach typically cnns consist several layer convolution max pooling applied image bottom layer close input layer focus local simple feature like edge dot color gradient higher layer instead combine previous feature complex one used train machine learning model however cnns require high computational resource second focus entire image instead portion could contain lesion first vit architecture introduced variety different architecture appeared differently cnns vits consist small number layer decompose image patch gaining information attention mechanism turned reach promising performance even outperforming traditional cnns scenario light innovative algorithm proposed literature aim work compare performance different state-of-the-art deep learning algorithm predict disease recurrence nsclc patient best knowledge state-of-the-art lack comparative study classification performance obtained two architectural family relation problem disease recurrence prediction evaluated reference dataset information would allow lay foundation future study aimed defining validating accurate model personalized medicine therefore preliminary work used various transformer architecture predict nsclc recurrence used public database image ncslc patient recurrence classification comparing performance vits cnns paper organized follows section result material method introduce database patient network architecture section discussion conclusion material method result discussion present result transfer-learning-based model discussing performance result performance diverse transformer family summarized radar plot fig vitb_32 vitb16 fig pvt-b1 pvt-b0 fig swin-tiny swin-small fig figure radar plot performance auc accuracy acc sensitivity sen specificity spe precision pre vitb_32 vitb_16 metric mean value among cross-validation round shown standard deviation full size image figure radar plot performance auc accuracy acc sensitivity sen specificity spe precision pre pvt-b1 pvt-b0 metric mean value among cross-validation round shown standard deviation full size image figure radar plot performance auc accuracy acc sensitivity sen specificity spe precision pre swin-tiny swin-small metric mean value among cross-validation round shown standard deviation full size image among structure evaluated family architecture ptv_b1 show best performance fig highly performing auc value accuracy sensitivity specificity precision 0.90 0.04 0.86 0.04 0.81 0.12 0.89 0.07 0.75 0.11 respectively hand performance cnns shown radar plot fig inceptionv3 fig outperformed structure achieving auc value accuracy sensitivity specificity precision 0.91 0.03 0.89 0.04 0.85 0.05 0.90 0.06 0.78 0.10 respectively figure radar plot performance auc accuracy acc sensitivity sen specificity spe precision pre three cnns resnet50 inceptionv3 densenet201 metric mean value among cross-validation round shown standard deviation full size image additional result fig show histogram validation loss value averaged epoch fold round cross-validation vits pvts swins cnns vitb_16 pvt-b1 swin-tiny inceptionv3 show lowest validation loss histogram within family best trade-off performance achieved loss valued reached inceptionv3 figure example training loss function validation loss plot function epoch training histogram validation loss value averaged epoch round fold cross-validation vits pvts swins cnns full size image discussion conclusion study evaluate performance different deep learning algorithm predicting recurrence nsclc patient analyzing baseline ct. experimental result showed vitb_16 higher performance reaching auc accuracy value 0.84 0.04 0.83 0.05 respectively vitb_32 value equal 0.67 0.05 0.64 0.08 respectively due different architecture indeed vitb_16 decomposes input image patch size pixel vitb_32 patch size pixel therefore patch size smaller transformer encoder attention would higher bringing better classification regard swin case swin-tiny swin-small comparable auc 0.82 0.04 0.80 0.07 accuracy 0.79 0.04 0.77 0.04 respectively best performance among considered transformer technique reached pvt-b1 auc accuracy value 0.90 0.04 0.86 0.04 respectively better performance among considered transformer could depend pvt overlapping patch embedding mechanism allowing transformer extract information image vits swins end best performance study reached via pre-trained cnn inceptionv3 auc accuracy equal 0.91 0.03 0.89 0.04 respectively even cnns perform best considered transformer vits pvts swins still reach high comparable performance regard topic nsclc classification scanned literature best knowledge identified state-of-the-art work mainly use clinical feature radiomic one latter split handcrafted feature extracted via cnns best knowledge use pre-trained vits pvts swins first time specific task nsclc classification table summarizes principal result proposed state-of-the-art according topic clinical task table table state-of-the-art performance achieved previous work nsclc recurrence prediction full size table hindocha predicted recurrence recurrence-free survival overall survival nsclc patient employing clinical feature cohort patient regard task recurrence prediction auc equal 0.69 reached work wang al. example image cohort nsclc patient analyzed using handcrafted-radiomic feature reaching ccuracy equal 0.85 regard nsclc recurrence radiomic study based deep learning model mention work aonpong al. kim al. bove former author used subsample radiogenomic database predict nsclc recurrence implementing genotype-guided radiomic model focusing smaller cohort patient using various state-of-the-art cnns gene expression data extracted image achieving auc equal 0.77 ccuracy equal 0.83 second one kim built various ensemble-based prediction model using database patient including one clinical data handcrafted radiomic feature deep learning radiomic one considered combined best performance combining together auc equal 0.77 accuracy equal 0.73 finally work bove transfer learning approach implemented extracting radiomic feature cropped image around tumor area nsclc radiogenomic dataset via pre-trained cnns reducing number radiomic feature combining clinical data database best reached performance consisted auc accuracy equal 0.83 0.79 respectively considering result model pre-trained cnn inceptionv3 seems outperform state-of-the-art work nsclc recurrence classification topic would like underline comparison state art purely naïve unfortunately work proposed literature clinical task often developed starting private datasets even use public dataset referred author integrated public data private data work presented kim without differentiating result obtained selected subset data according certain criterion could compatible objective work work presented aonpong therefore difficult make objective comparison dataset however model still suffers limitation indeed although data augmentation technique used reinforce training last layer pre-trained network used obtained performance strongly influenced retrospective nature small dimension dataset specifically model need validated robust manner also using external validation set preferably referring sample private data although use public database known allows objective comparison proposed method therefore future intend collect larger database nsclc patient validate optimize proposed model moreover also evaluate public dataset test obtained result another possible future direction research would include investigation transformer architecture correspondent performance moreover study could include combined deep radiomic clinical feature train suitable machine learning classifier predict nsclc recurrence year help explainable artificial intelligence xai detect relevant decisive feature prediction material method experimental dataset work used public radiogenomics dataset nsclc available cancer imaging archive tcia public database consisted subject divided two sub-cohorts r01 cohort patient female male age scan mean range 42–86 stanford university school medicine palo alto veteran affair healthcare system recruited april 7ths september 15th second amc cohort consisting additional subject female male age scan mean range 24–80 retrospectively collected stanford university school medicine based criterion chose focus sub-cohort r01 tumor segmentation binary mask axial available among patient cohort r01 tumor segmentation mask available patient final number patient involved study equal 27.78 recurrence event within eight year first diagnosis patient image dicom format available acquired preoperative scan thickness 0.625–3 x-ray tube current 124–699 80–140 kvp hand related segmentation defined axial image series thoracic radiologist five year experience adjusted using epad software beyond binary tumor mask adopted database includes following clinical feature recurrence value yes age histological diagnosis weight gender value female male histology value adenocarcinoma squamous cell carcinoma otherwise specified pathological value pathological stage value histopathological grade value lymphovascular invasion value absent present collected pleural invasion value yes clinical feature listed table table table clinical feature adopted dataset distribution full size table study clinical data used recurrence feature yes chosen label image classification patient first detected segmentation mask largest tumour area found corresponding slide analysis shown fig figure example binary mask largest tumor area corresponding shown yellow line mark tumor area ct. patient detected correspondence suitably rescaled range specific input size used input vits pvts swins cnns full size image vits pvts swins cnns architecture detecting largest tumor area adopted deep learning transfer-learning approach involving pre-trained vits pvts swins cnns analysis step performed using python programming language tensorflow-keras first original image pixel normalized range reshaped specific input size transformer cnns whole pre-processed image became input various model usual architecture state-of-the-art cnns shown fig consists three key element represented convolutional layer pooling layer fully connected cnn receives input image suitably pre-processed convolutional layer one dedicated learning feature input image instead max-pooling layer responsible reduction size feature map end cnn fully connected layer added stacked way via specific function e.g. softmax sigmoid provides classification figure typical architecture cnn take input image suitably resized elaborates series internal layer consisting convolutional one max-pooling layer fully connected layer final classification full size image architecture transformer shown fig according architecture dosovitskiy al. quite different traditional cnns vits derive original transformer model used natural language processing nlp input object consists one-dimensional word token input image typical size pixel height width channel divided smaller patch number hw/p pixel size input image perform classification task vits equipped encoder receives sequence embedded picture patch together positional data learnable class embedding suspended sequence latter sent classification head coupled output encoder therefore data sequence following original image resized size e.g. normalized decomposed patch obtained patch flattened obtaining linear patch projection learnable embeddings patch projection concatenated positional embedding mark order single patch sequence output transformer encoder sent multilayer perceptron head mlp additional layer work e.g. flatten layer batch normalization layer dense layer unit another batch normalization layer final dense layer sigmoid function shown red dashed box fig provide classification figure proposed architecture vits starting dosovitskiy original input image suitably pre-processed decomposed patch flattened obtaining linear patch projection transformer encoder element sent head mlp provides classification yellow final box placed mlp inside red dashed rectangle indicate new added layer proposed also adopted cnns full size image study performed different experiment using two vit model base model image patch size vitb_16 base model image patch size vitb_32 consisting hidden layer pvts represent variant original vits stated name posse columnar pyramid structure similar traditional cnns work adopted improved version pvts wang introduced linear complexity attention layer overlapping patch embedding convolutional feed-forward network orthogonal original pvts throughout text sake simplicity use term pvt indicate pvt architecture wang considered two model family pvt-b0 pvt-b1 consist four stage characterized channel number output stage reduction ratio head number expansion ratio feed forward layer number encoder layer 1–4 hyperparameters equal whereas 1–4 pvt-b1 double correspondent pvt-b0 swin transformer another transformer architecture name state shifted window key idea type transformer build hierarchy starting small-sized patch gradually merging neighbouring patch deep tranformer layer self-attention layer next one window shift resulting new one adopted two type architecture consisting swin-tiny swin-small provided best performance hyper-parameters type swins represented channel number hidden layer first stage swin-tiny small layer number 2,2,6,2 2,2,18,2 tiny one small analyzed transformer ideal image size set pixel regard traditional cnns used three well-established state-of-the-art cnns different family resnet50 densenet201 inceptionv3 python tensorflow-keras resnet50 requires input image size pixel total layer differently inceptionv3 need input image pixel total layer end densenet201 accepts input image pixel size total layer learning model built transfer learning model using pre-trained vits pvts swins cnns imagenet natural image dataset train dataset ncslc patient predict recurrence event application transfer learning vit pvt swin architecture consisted replacing last layer following layer flattening layer plus batch normalization one dense layer gelu activation function followed another batch normalization final dense layer classifier sigmoid activation function red dashed box fig show added layer scheme also adopted cnns replacing gelu relu activation function added dense layer new network trained image classification task implemented stratified tenfold cross-validation external round entire dataset patient fold cross-validation dataset corresponding element used training set whereas remaining corresponding element used test set study model trained epoch fold cross-validation batch size equal element adam optimizer initial learning rate 10–4 used optimize weight network handle imbalancing dataset sigmoid focal cross-entropy used loss function balancing factor modulating factor equal 0.25 2.0 respectively considering database relatively small make analysis robust implemented data augmentation process addition transfer-learning approach using three built-in kera transformation random flip random rotation random contrast data augmentation added additional layer model training phase model used predict probability score used compute performance via scikit-learn library function performance classification nsclc recurrence pre-trained vits pvts swins cnns evaluated term area curve auc accuracy sensitivity specificity precision metric computed round stratified cross-validation end final performance specific model evaluated average value corresponding standard deviation better balance metric youden index test performed