introduction world population projected increase 9.9 billion global demand various meat animal product set increase next decade therefore dire need increase food production intensifying production almost amount land using resource put pressure animal husbandry sector well need produce animal using limited land water natural resource mean need find new innovative approach produce food huge challenge animal scientist despite vast genetic wealth address new technology adopted animal farm evolving traditional high-tech farming operation becoming automated use sensor increasing aspect farm management reducing drudgery labour also leading exponential increase amount data generated daily basis leading exponential increase farm data traditional method conventional strategy quite able keep enormous data resulting declining trend production especially developing country artificial intelligence transforming industry big way offer solution analytic problem animal husbandry veterinary science would help proving many aspect farm management important reducing mortality improving productivity efficiently handle data also draw inference hitherto unknown technique posse capability present conventional technique modelling tolerance method considerably higher statistical methodology requirement assumption hypothesis testing addition benefit like capability handling non-linear imprecise noisy data make area science much flexible conventional statistical model use artificial intelligence farming practice rapidly becoming popular however study comparing popular supervised learning algorithm ranking still scanty research comparison various machine learning technique animal science prediction disease performance hatchability lactation genetic merit body weight disease diagnosis prediction immunity even molecular study like transcriptomics rna sequencing gene expression genetic selection etc study stated algorithm like artificial neural network support vector machine k-nearest neighbour etc found useful case better conventional approach due large amount data scientist reported multiple algorithm promising solving various problem animal science prediction future performance one crucial area done accurately could help making important decision improving production well income study therefore undertaken compare popular algorithm rank per ability make prediction sheep farm data attempt also made fine-tune model deploy-able model could developed result missing value dimensionality reduction result indicated imputation effectively removed missing value dataset considering variable dataset prediction body weight variance principal component analysis total feature retrieved create pca dataset dataset created using feature original dataset score greater way number feature within dataset dataset containing feature selected pca feature selected final dataset pca score greater score first principal component 1357.04 29.97 20.24 13.68 11.68 4.29 multicollinearity effectively reduced pca pair plot multicollinearity pca+ dataset body weight given fig bayesian ridge regression ridge regression bayesian ridge regression rmse mae coefficient determination correlation coefficient pca dataset 1.084 0.872 0.940 0.979 dataset 0.926 0.816 0.957 0.992 fca dataset 1.179 0.93 0.923 0.974 ridge regression rmse mae coefficient determination correlation coefficient pca dataset 1.082 0.871 0.940 0.979 dataset 0.939 0.822 0.955 0.991 fca+fs dataset 1.178 0.930 0.924 0.974 respectively result obtained bayesian ridge regression ridge regression similar dataset highest correlation coefficient figure pair plot multicollinearity pca+ dataset full size image figure hyperparameter optimization graph iteration full size image artificial neural network hyperparameter optimization graph thousand iteration given fig result training anns given table result indicated pca+fs dataset converged earlier two datasets result obtained hyperparameter optimization refined heuristically model could improved anymore one may infer application good searching algorithm case enough obtain optimum result three datasets pca dataset showed highest correlation coefficient 0.977 dataset also highest number neuron per layer dataset also showed lowest mse mae loss compared datasets dataset alone performed better pca+fs dataset pca dataset reduction number feature dataset enough achieve highest predictive ability dataset search result yielded sigmoid activation function well low learning rate appropriate one prediction body weight hyperparameter tuning stochastic gradient descent sgd adam performed well optimizers activation function relu sigmoid performed better rest hyperparameters trained relu rectified linear unit adam adaptive moment estimation best optimizers activation function respectively number hidden layer tree model application genetic algorithm increase number iteration correlation coefficient also increased also seen number iteration higher correlation coefficient table training result artificial neural network prediction body weight full size table genetic algorithm genetic algorithm sufficiently able predict bodyweights sheep efficiently algorithm prediction power genetic algorithm lowest among trained algorithm body weight prediction among three pca pca+fs well datasets body weight prediction pca+fs dataset yielded highest correlation coefficient true predicted breeding value number generation fitness threshold pop size activation mutation rate rmse mae correlation coefficient pca dataset 0.980 0.001 1.930 1.248 0.835 0.874 pca dataset 0.980 0.001 1.322 1.031 0.917 0.944 dataset 0.980 0.001 1.363 1.036 0.929 0.940 respectively best model evolved using genetic algorithm number generation fitness threshold population size activation mutate rate rmse mae correlation coefficient 0.980 0.001 1.322 1.031 0.917 support vector machine dataset highest correlation coefficient test label hyperparameters grid search performed hyperparameters gamma kernel linear table give result obtained training testing algorithm linear kernel consistently outperformed rbf kernel say weight prediction data linearly separable support vector machine body weight prediction using default parameter kernel rbf rmse mae correlation dataset 1.569 1.005 0.832 0.944 respectively pca+ dataset 1.461 1.012 0.861 0.959 respectively pca 1.538 1.025 0.834 0.956 respectively hyperparameter optimization revealed best hyperparameters gamma kernel linear dataset gamma 0.0001 kernel rbf pca dataset gamma 0.001 kernel rbf pca dataset best-trained model following parameter gamma kernel linear regression tree random forest bodyweight prediction hyperparameter tuning improved prediction result random search performing better grid search breeding value prediction prediction except grid search gave best correlation result bootstrap true max feature auto search algorithm highest correlation 0.990 obtained dataset grid search without hyperparameters dataset performed best regression tree dataset highest correlation compared datasets algorithm hyperparameter tuning improved prediction ability random forest table gradient boost feature selection dataset highest correlation coefficient gradient boost algorithm without hyperparameters training result algorithm given table polynomial regression highest correlation found dataset average correlation reaching 1st-degree polynomial gave best-fit model training result algorithm given table mae value pca fs+pca datasets 1.096 0.709 1.078 respectively table result obtained regression tree random forest gradient boost full size table xgboost dataset highest correlation coefficient testing dataset xgboost algorithm value indicated table time elapsed running algorithm greatest pca+fs dataset wall time pca +pca datasets respectively colsample bytree learning rate max depth min child weight estimator subsample pca dataset 0.7 0.05 0.5 dataset 0.7 0.1 fs+pca dataset 0.7 0.01 0.5 0.7 respectively nearest neighbour highest correlation true predicted value found pca dataset table pca dataset highest n-neighbours using hyperparameter tuning neighbour pca +pca datasets 7,4,3 respectively mar body weight value prediction predicted true value correlation coefficient 0.993 applying multivariate adaptive regression spline highest correlation coefficient found dataset value indicated table table result obtained xgboost knn polynomial regression mar full size table algorithm ranking bodyweight prediction mar algorithm gave best prediction based correlation coefficient table breeding value prediction tree-based algorithm gave best result random forest highest correlation coefficient table dataset outperformed pca pca+fs datasets case except genetic algorithm neural network trained hyperparameter optimization well heuristic modelling knn narrow margin genetic algorithm dataset lowest number feature gave best correlation coefficient case principal component regression pca dataset performed best bayesian regression outperformed ridge regression small margin correlation true predicted value given fig table ranking algorithm prediction body weight full size table figure pair plot multicollinearity pca+fs dataset full size image figure correlation true predicted value algorithm full size image discussion overall value taken birth data meticulously recorded parameter recorded later life animal missing value universal real-world datasets use winsorization give distribution desirable statistical property also published literature lowering weight influential observation removing unwanted effect outlier without introduction bias anderson converted much higher range viz upper lower data 90th percentile little introduction error two-sided winsorization approach used study also reported better one-sided approach chamber hamadani result present study indicate number feature effectively reduced dataset using principal component analysis substantially lowered effective number parameter characterizing underlying model body weight taken various age weaning greatest feature score expected also evident growth curve various animal body weight important parameter feature selection shown researcher increase learning algorithm working term computation time accuracy result pca reducing multicollinearity correspond result many author pca reported literature one common method reduce multicollinearity dataset dataset high multicollinearity feature selection lessens number total feature without dealing multicollinearity present within dataset reported literature multicollinearity doe affect final model predictive power reliability model prediction ridge regression bayesian ridge similar reported also used various machine learning technique prediction weight reported high value approaching 0.988 tenfold cross-validation training model used also reported appropriate however used 20-fold cross-validation study predict breeding value high coefficient determination 0.92 also stated kumar adebiyi estimation weight measurement prediction disease reported value 0.70 0.784 0.74 prediction body weight three egyptian sheep breed morkaraman sheep malabari goat respectively value well coefficient correlation pca dataset greater pca+fs dataset may inferred pca effective technique data reduction also data reduction dataset caused loss variance dataset compared heuristic modelling optimization algorithm took time execute number computation increase become increasingly difficult solve consume higher higher computational power sometimes even causing system crash optimization algorithm test much higher number option available tune best mode result indicate three datasets trained comparable term correlation coefficient training error pca+fs dataset converged earlier two datasets upon hyperparameter tuning may number feature within dataset two hence convergence occurred earlier two datasets important training efficiency especially datasets large computational power available researcher limited three datasets trained using hyperparameter optimization heuristic modelling pca dataset showed highest correlation coefficient 0.977 one may infer pca efficiently took care selection feature could sufficiently explain variance data alone performed better pca+fs dataset say explained variance may lost technique used together reduction number feature dataset alone enough achieve highest predictive ability dataset higher correlation prediction fat yield 0.93 predicted ann shahinfar peter used mlp-ann model achieve predictive correlation 0.53 birth weight 0.65 205-day weight 0.63 365-day weight much lower prediction khorshidi-jalali mohammadabadi compared anns regression model arriving body weight cashmere goat found ability artificial neural network model better however unlike result value 0.86 ann genetic algorithm performed poorly compared algorithm lower-than-expected value may also reason genetic algorithm seldom used direct regression genetic algorithm also reported better suited optimizing large complex parametric space svm dataset highest correlation coefficient test label hyperparameters grid search performed linear kernel consistently outperformed rbf kernel suggesting weight prediction data linearly separable rbf kernel reported perform better nonlinear function estimation preventing noise high generalization ability ben-hur also observed nonlinear kernel gaussian polynomial lead slight improvement performance compared linear kernel however using linear kernel long reported lower correlation coefficient 0.497–0.517 prediction quantitative trait alonso also used different svr technique prediction body weight reported higher prediction error mae 9.31 8.00 10.98 11.74 9.61 7.90 technique huma iqbal also used support vector regression prediction body weight sheep reported correlation coefficient mae rmse 0.947 0.897 3.934 5.938 respectively close value present research hyperparameter tuning improved prediction result random search performing better grid search breeding value prediction prediction except grid search gave best correlation result random search similar grid search yet consistently reported produce better result comparatively effectively searching larger promising configuration space due difference relevance hyperparameters different model hand grid search sometimes becomes poor choice constructing algorithm different data set hyperparameters improved prediction ability random forest also published huma iqbal also used regression tree prediction reported mae 0.896 4.583 also used random forest prediction body weight sheep reported correlation coefficient mae rmse 0.947 0.897 3.934 5.938 respectively compared model many author stated random forest method variant produce lowest error lower value random forest reported jahan reported 0.911 bodyweight prediction balochi sheep çelik yilmaz also used cart algorithm reported lower value present study 0.6889 adj 0.6810 0.830 rmse 1.1802 respectively also suggested important choice modelling complex relationship variable compared many model researcher based feature similar result reported present study random forest also generally found outperform decision tree accuracy reported lower gradient-boosted tree boosting algorithm reported perform well wide variety condition however important mention convergence algorithm also depends large extent data characteristic morphometric parameter along body weight used prediction body weight high correlation study highest variation body weight reported accounted combination chest girth body length height prediction body weight xgboost outperformed gradient boost algorithm prediction bodyweights xgboost algorithm accuracy training speed found better also published bentéjac compared xgboost several gradient-boosting algorithm xgboost algorithm also shown achieve lower error value comparison random forest niang xgboost advanced regularization may reason improved model generalization capability greatest correlation found pca dataset mean technique better prediction made using least number feature support vector regression gave slightly better convergence k-nearest neighbour also stated ramyaa study phenotyping subject based body weight knn result also reported somewhat biased towards mean extreme value independent variable affect result present study dataset gave highest correlation coefficient using multivariate adaptive regression spline algorithm presence greater number feature two datasets could contributed value closer one obtained study 0.972 obtained mar algorithm prediction fattening final weight bull reported çelik yilmaz used mar bodyweight prediction well reported slightly higher value equal 0.919 rmse equal 0.604 equal 0.959 mar algorithm reported flexible model revealed interaction effect minimized residual variance bodyweight prediction mar algorithm gave best prediction based correlation coefficient breeding value prediction tree-based algorithm gave best result dataset outperformed pca pca+fs datasets case except genetic algorithm neural network trained hyperparameter optimization well heuristic modelling knn narrow margin may attributed greater number feature present within dataset contributing causing addition additional explained variance within dataset towards predicted variable bayesian regression outperformed ridge regression small margin going say multicollinearity within dataset cause convergence issue also supported within literature conclusion recommendation artificial intelligence promising area potential make accurate prediction various aspect farm management thus viable alternative conventional strategy deployable reusable model developed study prediction body weight month age model high prediction ability tree-based algorithm generally outperforming technique regression-based task customized deployed farm would help taking informed decision farm modernization would thus beneficial animal production farm economy thus contributing larger goal achieving food security method data preparation predict body weight data year 2011–2021 corriedale breed used collected organized sheep farm kashmir total number data point available study initial raw data included animal number brand number ear tag dob sex birth coat litter size weaning date parent record dam number sire number dam weight dam milking ability parturation history coat colour time birth body weight weekly body weight 4th week fortnightly weight 6th fortnight monthly body weight 12th month monthly morphometric measurement weaning weather data daily temperature humidity disposal record treatment record feature determined heuristically well using technique discussed later raw data cleaned duplicate row many missing value removed data imputation done iteratively using bayesian ridge regression winsorization used handling outlier data appropriately encoded standardization also done achieved dividing subtracting mean feature dividing standard deviation data split training testing optimal train test split heuristically determined testing data equal training data equal per cent dataset total training dataset validation validation data proportioned per cent training data decrease number input variable dataset select one contributing variance dimensionality reduction performed using principal component analysis pca feature selection pca statistical technique convert correlated feature set uncorrelated feature linearly done orthogonal transformation feature selection done python based f-test estimate degree linear dependency two numerical variable input output feature selection performed original datasets extracting feature pca input variable constant across method used study eliminate bias uneven number features/ input variable could cause training process thus three datasets created principal component analysis dataset pca primarily pca technique used dimensionality reduction feature selection dataset f-test estimate degree linear dependency two numerical variable used dimensionality reduction pca+fs dataset wherein technique used achieve much-reduced number feature pure morphometric measurement also used predicting body weight using anns constituted dataset used prediction weaning weight done morphometric measurement scarce dataset weaning body weight month age used label weaning weight also used label one algorithm machine learning technique total algorithm employed study prediction weight parameter done using body measurement well earlier body weight input attribute artificial neural network hyperparameters optimized using search-grid random-search algorithm later heuristic tuning well comparison following machine learning algorithm done study bayesian ridge regression brr technique work principle output drawn probability distribution single value due inclusion probabilistic approach model expected train better prior coefficient thus derived using spherical gaussian regularization tested effective approach multicollinearity cost function lambda term penalty shrink parameter thereby reducing model complexity get unbiased estimate default parameter alpha alpha used hyperparameters shape rate parameter distribution artificial neural network popular machine-learning technique inspired neuron found animal neural system neural network therefore group units/nodes connected together form artificial neuron connection similar neuron number like signal actual brain transmitted signal among artificial neuron output calculated non-non-linearity added sum input particular neuron larger picture network neuron formed many neuron aggregated layer number neuron denser neural network formed addition many inner layer make network deep hyperparameter range pca+fs pca datasets respectively artificial neural network iteration learning rate 0.001 0.5 pca +fs dataset 0.001 0.5 pca dataset 0.001 0.5 dataset dropout rate 0.01 0.9 pca +fs dataset 0.01 0.9 pca dataset 0.01 0.9 dataset hidden layer pca+fs dataset 1–5 pca dataset 1–7 dataset =1–10 neuron per layer pca+fs dataset pca dataset dataset batch size per layer pca+fs dataset pca dataset dataset activation optimizers option datasets tanh sigmoid relu adam rms sgd support vector machine supervised machine learning algorithm svm useful solving regression svr classification svm problem svm work creating maximum-margin hyperplane transformed input space way solution optimized quadratic optimization problem used derive hyperplane solution parameter grid search parameter support vector machine range param grid equal 0.1 gamma equal 0.1 0.01 0.001 0.0001 kernel equal rbf sigmoid linear randomized search conducted prespecified hyperparameters estimate best one hyperparameter range grid search random search respectively bootstrap true true false max depth none evenly spaced value max feature equal auto log2 auto log2 sqrt estimator equal 5–13 evenly spaced value classification regression tree algorithm cart cart algorithm work building decision tree decision tree work gini impurity index arrive final decision analogous actual tree branching fork represents decision predictor variable segregated towards either many branching point end end node arrives final target variable random forest random forest similar tree-based algorithm theory however utilizes ensemble learning method wherein many decision tree constructed arrive solution optimum thus average prediction obtained tree taken final output gradient boosting tree-based ensemble algorithm utilizing many weak prediction decision tree thus final model built stage-wise allows optimization arbitrary differentiable loss function make algorithm better many tree-based one gradient boost algorithm hyperparameter option learning rate 0.001 0.01 0.1 estimator subsample 0.5 0.75 max depth random state xgboost also decision-tree-based algorithm making use gradient boosting framework arriving optimum solution xgboost extra randomization parameter penalization tree proportional shrinkage leaf node well newton boosting hyperparameter tuning xgboost grid search taken learning rate 0.001 0.01 0.05 0.1 max depth min child weight subsample= 0.5 0.7 colsample tree 0.5 0.7 estimators= objective reg squared error polynomial regression polnominal regression take monomial regression step ahead relationship independent dependent variable represented nth-degree polynomial technique useful non-linear relationship dependent independent variable degree polynomial checked polynomial regression mean algorithm polynomial regression implemented using sklearn package python best parameter algorithm derived using hyperparameter tuning well nearest neighbour simple effective machine learning algorithm non-parametric learning classifier proximity predicting data point assumption similar point would close plot thus predicted value taken average number nearest neighbour point similar point similar would found close grid search employed knn range 2–11 multivariate adaptive regression spline mar mar combine multiple simple linear function aggregate forming best-fitting curve data combine linear equation aggregate equation useful situation linear polynomial regression work mar algorithm also used three datasets k-fold cross-validation split repeat used genetic algorithm technique solve constrained unconstrained optimization problem heuristic adaptive search algorithm belonging larger class evolutionary algorithm inspired natural selection genetics genetic algorithm simulate survival fittest among individual generation solving problem generation consists population individual represent point search space evaluation metric model evaluation four scoring criterion used since task hand regression mean squared error mse given mean absolute error mae given coefficient determination presented eq. correlation coefficient represented aligned mse i=1 x_i-y_i aligned aligned mae i=1 aligned aligned r^2= r^2 aligned aligned cov aligned equal actual value ith observation calculated value ith observation represents total number observation