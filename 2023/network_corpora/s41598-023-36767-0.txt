introduction availability user-generated information online offline document achieved considerable growth past year since emergence online social network platform web community blog wikis social medium growth indicates increasing need analyzing user-generated opinion opinion mining also known sentiment analysis one emerging application textual data analytics natural language processing computational linguistics specify orientation particular opinion usually positive negative neutral polarity short opinion text generated user become essential source information people thought feeling anything event result play crucial role wide range application many genre marketing medicine polarity detection method opinion mining literature divided three group document sentence aspect level based granularity method predict attitude review sentence aspect respectively another perspective opinion mining divided computational linguistic machine learning method first approach utilizes linguistic background polarity review specified computing score based occurrence lexical word assumes positive word appear higher probability negative word positive sentiment modern approach applies machine learning method learning classifier popular classifier including support vector machine svm naïve bayes neural network bayesian network adaptive neuro-fuzzy inference system successful classifier various application used specify attitude opinion approach sentiment analysis viewed binary classification problem show better prediction performance former method labeled data sampled domain expert used train model however performance trained classifier significantly dependent feature produced unstructured text well-known feature unigrams bigram etc embed text vector space vector representing text high-dimensional feature vector consisting noisy redundant irrelevant feature suffer data sparsity therefore feature representation prime importance machine learning method feature selection feature extraction two main approach handle mentioned problem feature selection method select subset relevant feature whereas feature extraction method transform large number feature new set feature different space feature extraction method higher discriminating power information loss feature selection method however problem current application feature extraction method opinion mining method linear reflect non-linear property high dimensional sentiment data plenty research confirming high dimensional data lie close manifold structure recently classical linear method feature extraction reduction generalized nonlinear manifold technique establishing correspondence high dimensional space intrinsic structure considering topological relationship vast amount machine learning literature focused line research whereas little attention dedicated field opinion mining literature intrinsic structure data represented graph structure making suitable graph highly effective result manifold-based method literature opinion mining graph structure usually constructed -nearest neighborhood -nearest neighborhood graph nearest neighborhood data specified common distance measure euclidean cosine distance furthermore linguistic feature like co-occurance also exploited building graph manifold-based sentiment analysis prime importance graph represents manifold structure opinion data existing either wrong strong connection dissimilar local data weak connection similar local data lead error next algorithm step common manifold-based method suffer mentioned error clarity drawback using euclidean distance specifying locality illustrated fig sentence exhibit positive polarity sentence demonstrates negative polarity however noteworthy euclidean distance sentence posse similar polarity lesser distance sentence dissimilar polarity common feature representation method distance measure mentioned earlier also addressed issue sufficiently figure euclidean distance tf_idf vector sentence dissimilar polarity removing stop word euclidean distance tf_idf vector sentence similar polarity full size image paper focus document-level sentiment analysis attempt resolve challenge assuming input sentiment vector lie manifold motivation assumption line research demonstrate high dimensional data lie interesting manifold one manifold mean near opinion based common distance measure may various attitude needed another prior knowledge imposed resolve problem therefore algorithm based sparse representation technique i.e. sparse manifold sentiment representation smsr developed studied represent sentiment data based two following property high dimensional data self-expressiveness property data reflects data point union subspace expressed linear combination data point sparse representation data point match combination data point locating subspace sparse representation led promising result wide range application including visual recognition image synthesis animation denoising etc recently sparse representation applied sentiment analysis 2,1 -norm sparsity exploited represent micro-videos aim finding main frame including sentiment singular vector decomposition also used sparse representation image sentiment success sparse learning sentiment analysis image video importance graph representation manifold-based feature extraction author motivation incorporating together proposed algorithm learns graph representing manifold data using sparse locality representation exploit spectral property generate new data representation main contribution paper described follows feature extraction sentiment data applied considering laying sentiment data manifold manifold structure opinion discovered local information sparse property data impact sparse manifold-based representation common feature extraction method sentiment analysis explored provide insight various vector representation experiment confirm enhancement proposed method compared effect proposed method parameter investigated obtained result demonstrate validity proposed method wide range value linear svm classifier suitable text data applied extracted feature two benchmark datasets best knowledge wide range research approved performance svm classifier compared classifier paper exploit svm linear kernel fewer parameter compared svm radial basis function polynomial kernel numerous experiment reveal extracted feature highly suitable feature classifier noteworthy mention last three case mentioned highlight main aspect research comparison previous research paper combine key idea sparse representation manifold learning sentiment analysis rest paper organized follows literature review section examines literature review proposed method describe detail proposed algorithm detail experiment result given fourth section followed conclusion end literature review recent year opinion mining become active research area section research enhancing sentiment analysis performance feature representation method including feature selection feature extraction examined feature selection method opinion mining literature fall three category natural language processing nlp based machine learning-based combination nlp-based technique exploit lingual characteristic noun noun phrase adjective adverb feature nlp-based technique achieved high accuracy low recall depending amount accuracy part tagging speech feature selection method based machine learning divided three category including filter approach wrapper approach hybrid method filter approach score assigned feature based evaluation function feature low score eliminated approach low computational cost easy implement feature filtering method information gain chi-square chi2 occurrence frequency z-score logarithm likelihood fisher discriminant ratio minimum frequency threshold consider attribute separately document frequency feature selection method commonly used general text classification pick common term corpus mutual information chi2 five machine learning algorithm consisting support vector machine -nearest neighbor centroid classifier naïve bayes winnow classifier applied sentiment analysis chinese document cekik proposed feature selection method using rough set theory handle sparse opinion data wang exploited improved fisher discriminant ratio feature selection sentiment filter-based technique computational efficiency neglect interaction feature wrapper method feature either added removed step select optimal feature subset greedy manner new feature set selected classifier retrained new feature assessed validation set method suffer high computational cost important wrapper method decision tree model recursive feature elimination heuristic based algorithm wrapper method expensive filter model term computational efficiency since evaluate attribute interaction gokalp proposed wrapper feature selection method using iterated greedy metaheuristic algorithm opinion mining hybrid technique combine group exploiting suitable property achieving accuracy efficiency hybrid embedded method find subset feature machine learning method feature extraction method grouped linear nonlinear method linear method based simple assumption linearity latent semantic analysis principal component analysis two popular method applied opinion mining implementation linear dimensionality reduction easy fast meanwhile represent complex nonlinear data appropriately study opinion mining consider nonlinear structure data local method applying manifold assumption enforcing near data point input space close new feature space kim exploited semi-supervised laplacian eigenmaps reduce dimensionality data visualize sentiment method based manifold regularization utilizing unlabeled data training step proposed another semi-supervised nonlinear feature extraction based laplacian eigenmap initial graph obtained using fuzzy similarity relation semi-supervised dimensionality reduction combined feature weighting also proposed attempt maximize between-class distance minimize within-class distance exploiting local structure unlabeled data time summarization literature given table focus paper representation opinion data using nonlinear feature extraction method considering laying data manifold discovering structure sparse property neglicated literature significance constructing graph manifold-based method studied demonstrated number research study method exploit label information make discriminant graph aim classification discriminant neighborhood embedding dne method introduced optimization problem learning discriminant embedding considering intra-class inter-class adjacency weight adjacency weight simply 0,1 research improved dne extending weight value neighborhood definition theoretical framework improving graph weight learning also proposed mentioned method evaluated image datasets table summarization related dimensionality reduction method sentiment analysis full size table similar existing nonlinear method graph constructed data instead using -nearest neighbor -nearest neighbor heuristic graph learned data paper order detect close data point imposing sparsity constraint sparse representation successfully applied various application recently active research area sentiment analysis 2,1 -norm sparsity incorporation regularization term based laplacian matrix exploited extract sentiment key frame extraction user-generated micro-videos cosine distance video frame vector used compute graph weight sparse representation singular vector decomposition used enhance efficiency deep learning image sentiment analysis ignores local smoothness assumption effective prior knowledge comparison sparse research sentiment analysis focus proposed method short text data attempt learn suitable data dependent graph structure proposed method problem formulation let i=1 collection review opinion sentiment label assigned review labeled training set shown positive negative also may included indicating neutral sentiment considered research table give main notation paper ultimate aim opinion mining predict sentiment label review unknown attitude essential step proposed method given fig table notation full size table figure step proposed method full size image research motivation let i=1 set review vector feature vector representing review extracted various method including statistical lexicon-based combined detail feature extraction method paper given extract feature vector unstructured text subsection feature generated unstructured review consist redundant sparse unusual information enhanced opinion mining task research exploited manifold assumption reduce dimensionality opinion data constructing locality graph data nonetheless studying various method nonlinear dimensionality reduction three following achievement available high dimensional data lie intersecting manifold accurate graph construction underlying manifold important especially intersecting region sparse representation find representation one data based data point subspace local tangent space therefore assumed nonlinear manifold opinion embedded high dimensional input space nearest neighbor construct data point manifold weight reconstruction data learned imposing mentioned assumption prior knowledge proposed exploit sparse manifold based representation efficiently accurately analyze high-dimensional data restoring low-dimensional structure learning graph structure data spectral method exploited extract new representation detail explained sparse manifold representation algorithm subsection finally classifier applied learn model discriminates positive attitude negative one feature space predicts overall attitude within unknown label architecture proposed method shown fig figure architecture proposed method full size image extract feature vector unstructured text preprocessing step carried first stage including removing number punctuation normalizing tokenizing eliminating stop word commonly used feature extracted phase extracting feature popular feature literature n-gram part speech tagging term frequency-inverse document frequency tf-idf combination mentioned feature used experiment input feature described follows n-gram n-gram defined subsequence word given sequence eliminating extra space noisy character two word unigram bag word model simplest model consists individual word presented text bigram model defined pair adjacent word include contextual information higher order n-grams efficient capturing context provide better understanding word position generation inefficient unigram bigram trigram commonly used n-gram model tagging tagging linguistic technique give tag word specifying morphological category noun verb adjective etc lexicon-based feature list including positive negative word score provided sentiment lexicon n-gram considered feature provided list acceptable score term specified based one feature weighted according tf-idf weighting method intuitively tf-idf specifies relevant given term particular document consists two part frequency term specific review inverse proportion term entire review equation classical formula tf-idf related term document review tf-idf log\frac indicates term frequency term review document frequency term review term occur one small set document given tf-idf weight term occur many document finally review represented tf-idf exist alternative technique encoding opinion text vector one approach doc2vec employ neural network generate vector representation opinion fixed length learning process designed map similar document proximate point within vector space resultant vector effectively leveraged within proposed method sparse manifold representation algorithm step representing review feature vector set data point i=1 lying different manifold l=1 aim discover new representation data sparse manifold embedding method considering two assumption data point reconstructed linear combination neighbor minimum neighbor manifold formed sparse representation word infinitely many possible representation data point use data point sparse representation find point subspace assumption illustrated fig point farther whereas lie different manifold located small ball centered containing nevertheless span one-dimensional subspace around close tangent space found sparse constraint figure far comparison lying different manifold whereas nearest neighbor sparse optimization selects reconstruct span 1-dimensional affine subspace passing near approximates manifold adapted full size image idea studied machine learning image classification literature best author knowledge far attention paid sentiment analysis application manifold represented graph structure data point node graph weight connection node node formed vector n-1 weight element without considering obtained reconstruction data point based two mentioned property proximity sparsity following optimization problem aligned subject aligned l1-norm .\vert imposes sparsity constraint given parameter used tradeoff sparsity reconstruction error positive-definite diagonal matrix whose smaller value show nearby point larger value indicate farther point preferring zero value common definition diag\left t\ne j\ne n-1\times n-1 furthermore alternative exponential weight example maybe exploited matrix normalized vector j\ne d\times n-1 optimization problem lasso optimization problem additional affine constraint coefficient vector i=1 obtained optimization problem translated graph edge weight matrix capturing manifold structure data let in-1 obtained via following relation array array place learned weight data suitable index weight matrix describe precisely node representing data point connected node utilizing element sparse solution expected nonzero element originate manifold consequently resulting graph ideally exhibit multiple connected component due fact point within manifold connected connection exist point different manifold reality exists weak connection various manifold obtained graph edge weight established connect data point specified neighbor practice efficiency learning weight restricted nearest neighbor parameter specifies maximum neighborhood size select sparse neighbor ultimately symmetrizing step conducted make obtained weight matrix symmetric spectral property obtained weighted graph utilized learn novel representation data eigenvectors affinity matrix utilized reveal intrinsic data structure three standard step compute normalized symmetric graph laplacian -\frac -\frac weight matrix degree matrix diagonal matrix =diag\left =\sum_ j=1 version laplacian also used compute eigenvectors smallest eigenvalue form matrix column row projection onto smallest eigenvectors used new representation step smsr given alg ultimate goal sentiment analysis predict attitude sentiment conventional classifier could applied new representation data however shown next section eigen vector chosen wide range suitable information lost therefore suitable result achieved exploiting classifier support high-dimensional data svm successful classifier achieved proper result opinion mining literature study linear svm applied appropriate text application fewer parameter svm kernel experimental analysis result datasets proposed method trained tested two famous datasets internet movie database imdb including 50,000 movie review consisting 25,000 positively annotated 25,000 negative labeled review available http amazon review dataset released updated contained 233.1 million review metadata subset original dataset downloaded http usp=sharing last access data 9/21/2021 subset datasets randomly selected including equal positive negative class various feature setting smsr algorithm applied widely used feature including unigrams bigram tagged term sentiment term following four combination unigram lexicon tag combination unigrams tag considered feature tf-idf computed unigram tf-idf unigrams computed bigram tf-idf bigram calculated unigram lexicon sentiwordnet popular sentiment lexicon including positive negative term based part-of-speech tag used setting sentiwordnet frequently exploited opinion mining literature experiment unigrams filtered using sentiwordnet lexicon word whose difference sum positive sum negative score pregiven threshold eliminated tf-idf remaining unigrams computed threshold set 0.3 setting unigrams bigram appear five time eliminated feature combination feature considered input feature constraint feature used proposed method study review selected randomly imdb amazon datasets respectively evaluation measure proposed method evaluated based precision recall accuracy specificity f-measure accuracy give percent review whose attitude predicted correctly precision give percent review estimated positive attitude correctly among review predicted positive one recall fraction true estimated positive attitude among actual positive review f-measure harmonic mean precision recall specificity show percent negative review predicted correctly result fair comparison tenfold cross-validation applied experiment impact smsr investigated applying linear svm above-mentioned feature comparing obtained result following case numerous experiment linear svm input feature regularization parameter set grid search equally logarithmically spaced point -0.5 seven applied classifier sentiment analysis literature including naïve bayesian bagging svm radial basis function kernel svm rbf random subspace j48 random forest two situation first without applying feature selection method second selecting suitable feature using information gain number selected feature set eleven experiment done classifier clarity best result experiment classifier reported classifier weka waikato environment knowledge analysis version 3.9.2 university waikato new zealand exploited apply above-mentioned classifier parameter tuned according table worthwhile mention random subspace j48 random forest selects feature nonlinear manner time training model therefore suitable model assessing proposed method vs. nonlinear feature selection model table parameter value classifier full size table smsr parameter set according table best value two datasets obtained respectively best value imdb amazon respectively table parameter value smsr full size table accuracy forecasting attitude sentiment various method summarized fig imdb dataset considering unigram lexicon tag unigram bigram unigram lexicon shown figure accuracy proposed model various input feature much higher compared method showed new representation unigram lexicon input feature 0.85 error imdb dataset obtained performance measure amazon dataset illustrated fig considering unigram lexicon tag unigram bigram unigram lexicon respectively showed lowest error 8.03 retained unigram feature amazon dataset precision recall specificity various method four input vector given table proposed method outperforms method performance measure best result imdb dataset achieved exploiting unigram lexicon f-measure 99.15 recall 99.60 bigram precision 99.68 specificity 99.69 figure accuracy various method imdb dataset unigram lexicon tag feature unigram feature bigram feature unigram lexicon feature full size image figure accuracy various method amazon dataset unigram lexicon tag feature unigram feature bigram feature unigram lexicon feature full size image table precision recall specificity various method different input feature imdb dataset full size table table show precision recall specificity amazon dataset best result amazon dataset gained unigram f-measure 92.16 recall 94.12 bigram precision 99.45 specificity 99.55 obtained result better method performance measure unigram lexicon feature however several method achieved better result based measure best accuracy attained utilizing proposed feature extraction method four type input feature table precision recall specificity various method different input feature amazon dataset full size table comparison proposed method also compared manifold-based dimensionality reduction manifold represented sentiment semantic analysis graph built combining semantic sentiment distance review semantic distance metric computed cosine similarity metric opinion vector doc2vec model utilized representing opinion vector sentiment distance computed valence aware dictionary sentiment reasoning isomap applied dimensionality reduction finally linear svm tenfold cross validation applied compare proposed method number doc2vec dimension selected 500\right\ best result reported table result show proposed method outperforms method presented however doc2vec model doe generate sparse vector output vector high-dimensional yet proposed method also discover underlying structure data since learns neighborhood data opposed combine semantic sentiment distance based fixed weight table result comparison proposed method full size table discussion detailed analysis parameter sensitivity proposed method assessed section three parameter may influence smsr indicates importance sparsity solution comparison reconstruction error value show sparsity solution dimension output smsr limited number data since number eigenvalue laplacian matrix equal data size specifies number nearest data point input space search neighboring point output space examine effect value parameter obtained result experiment conducted varying wide range different feature initial value set respectively initial value considered imdb dataset amazon dataset initial variable setting varied one one observe effect performance figure illustrate impact varying f-measure accuracy measure two considered datasets figure impact parameter f-measure accuracy proposed method imdb dataset full size image regarding fig observed model large value achieve high performance consequently linear svm exploited classify data deal high dimensional data appropriately fewer parameter svm polynomial rbf kernel illustrated fig doe significantly affect performance mean sparse solution point neighbor manifold successfully found wide range value optimization problem standard deviation accuracy fig 0.07 2.30 shown fig accuracy standard deviation 0.75 1.71 feature learned unigram lexicon tag bigram respectively figure illustrate stability proposed method varying although bigram unigram lexicon feature may affected value parameter compared feature imdb dataset finally evaluated result reveal strength smsr different value sufficiently large value effect parameter aforementioned performance measure similar f-measure accuracy figure impact parameter f-measure accuracy proposed method amazon dataset full size image figure impact parameter f-measure accuracy proposed method imdb dataset full size image figure impact parameter f-measure accuracy proposed method amazon dataset full size image figure impact parameter f-measure accuracy proposed method imdb dataset full size image figure impact parameter f-measure accuracy proposed method amazon dataset full size image conclusion people opinion posted social medium platform impact lot user automated categorizing opinion negative positive feeling utilizing machine learning natural language processing technique gaining significant importance short opinion text convert sparse high dimensional vector lie intrinsic manifold structure discovering manifold structure great effect accuracy obtained result paper investigated sparse manifold representation user-generated review assuming lying data manifold solution optimization problem formulating assumption concludes graph capturing geometrical structure data way near data point manifold connect higher weight different manifold spectral property learned graph exploited portray new representation data proposed representation method applied four feature extracted short unstructured opinion text two benchmark datasets many experiment conducted explore effectiveness manifold assumption sparse property prior knowledge opinion mining term accuracy precision recall specification f-measure result revealed classification performance sentiment ameliorated utilizing proposed feature extraction method specifically combination manifold assumption sparse property prior knowledge opinion mining lead better representation opinion accurate classification sentiment addition impact parameter assessed experiment obtained result seem excellent step toward better representation opinion future study focus enforcing mentioned prior knowledge deep learning-based seniment analysis