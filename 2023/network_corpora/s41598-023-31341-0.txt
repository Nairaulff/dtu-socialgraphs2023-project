introduction rapid growth information technology computing speed led breakthrough hot technology artificial intelligence meanwhile along informatization medical industry increasingly medical data generated progressively question apply medical data disease prevention detection apply developing artificial intelligence technology health care urgently need solved problem apply developing artificial intelligence technology health care answer medical question hot topic current research artificial intelligence offshoot machine learning made significant achievement healthcare industry past decade playing key role medical diagnosis practiced intelligent application important application clinical practice include providing up-to-date information reducing diagnostic treatment error real-time inference health risk alert health outcome prediction currently increasing number disease identified initially predicted machine learning ocampo built lung cancer diagnosis model based neural network achieved auc 0.97 lung cancer identification lyngdoh used multiple algorithm predict diabetes k-nearest neighbor algorithm achieving accuracy meanwhile medical research begun shift traditional disease prevention cellular genetic aspect seeking discover root cause disease eradication machine learning algorithm used wide variety application medical study although various type algorithm main algorithm currently use still traditional algorithm random forest logistic regression support vector machine sound theory however best algorithm disease various algorithm need used different disease cytomegalovirus cmv herpes virus widely infected population infection rate chinese adult initial infection cmv remains lifelong latent state within host cell state periodic subclinical reactivation regulation functional immune system reactivation primary infection occurs patient severe immune dysfunction uncontrolled cmv replication lead clinical manifestation characterized fever myelosuppression tissue-invasive disease status cmv infection achieved analysis high-throughput sequencing result cell receptor beta chain tcrβ cell bone marrow-derived lymphoid stem cell distributed immune organ tissue throughout body lymphatic blood circulation antibody-like molecule called cell receptor tcrs transmit antigenic signal cell cell receptor composed chain tcrs randomly rearranged gene genome major binding region antigen cdr3 complementary determining region also random insertion deletion nontemplate nucleotide n-diversity mechanism junction v–d d–j beta chain v–j gene alpha chain fragment increasing diversity tcr thus human cell receptor would high diversity resulting theoretical variety high practice however subject certain mechanism e.g. positional effect rearrangement completely random human tcr diversity reach approximately order magnitude therefore next-generation cell receptor sequencing important application preventive medicine characterization therapy individual healthy immune system million unique tcrβ chain circulating cell chain generated vdj rearrangement corresponding cell proliferate exposed disease antigen cell response particular antigen observed unexposed compartment subject however response detected presence subject similar immunological background research cross-entropy loss fdr estimate calculated previously published data used calculate different number feature value corresponding p-value threshold logarithmic lattice algorithm logistic regression random forest used operate selecting different threshold thus different number feature respectively result multiple algorithm compared method data acquisition introduction article based research data emerson data publicly available downloaded immunoseq analyzer http compared training sample original article successfully download sample including positive sample negative sample different test sample original article article analyzes sample positive sample negative sample original data specific detail various tcrβ sequence sample research performs simple data prepossessing sample data integration conduct subsequent analysis total training sample cohort high-throughput sequencing pooled final tcrβ data total 113,846,476 unique sequence 53,791,329 positive subject 67,597,252 negative subject identification cmv-associated tcrβs determine key sequence indicate cmv exposure history established confusion matrix sequence calculate fisher exact test value marker independence sequence subject cmv status occurrence cmv-associated tcrβ sequence positive negative group cohort two categorical variable therefore dependent sequence obtained fisher exact test related training subject cohort determination best classification threshold calculation auc obtained combination multiple classification threshold evaluate model classification accuracy according threshold select classification threshold highest accuracy judgment standard model without work classification model automatically adopt default value inside model conducive effective classification model binary algorithm different traditional binary algorithm proposed classify sample cohort2 positive negative group corresponding svm lda biological epistemology applied reduce dimension multidimensional data including unique tcrβs number type tcrβ possessed sample cmv-associated tcrs corresponding different threshold number sequenced specie related sequence overlap two indicator sample cohort corresponding cmv status input four binary classification algorithm train model cmv exposure history classification test sample point corresponding indicator accuracy sensitivity specificity score auc value cross-entropy loss function obtained accordingly obtain best classification effect classification algorithm set binary classification data use grid search traverse kernel function classify data logistic regression algorithm parameter multi_class multinomial solver newton-cg class_weight balanced max_iter 10,000 selected parameter default value parameter adjustment process svm algorithm traverse multiple penalty factor gamma value multiple kernel function poly rbf linear sigmoid obtain best classification effect kernel function linear penalty term random_state addition obtain classification probability calculate auc value set probability equal true regard random forest algorithm several parameter adjustment combination n_estimators max_depth random_state class_weight balanced oob_score true finally selected test result obviously better combination time avoid problem data overfitting gini selected criterion parameter adjustment calculation gini relatively simple compared entropy time due amount data used parameter min_samples_split min_samples_leaf min_weight_fraction_leaf reset time default setting adopted series attempt find parameter linear discriminant analysis found default combination parameter make model perform well enough selection best cutoff value based variation auc value fdr cross-entropy loss score obtained trained model threshold determined optimal fisher exact test threshold algorithm higher auc score better lower fdr cross-entropy loss better describe index briefly follows fdr also known positive false discovery rate mean proportion sample error judgment among sample found positive proportion false false discovery discovery calculated addition mean expected value ratio false rejection rejecting true hypothesis number rejected null hypothesis fdr cross-entropy loss function also known logarithmic loss function calculated follows cross entropy loss=-\frac i=1 log log area curve auc defined area roc curve enclosed coordinate axis score regarded harmonic average model precision recall maximum value minimum value f1\ score=\frac tp+fp+fn article select four major item measurement four algorithm accuracy sensitivity specificity kappa score accuracy=\frac tp+tn tp+tn+fp+fn sensitivity=\frac tp+fn specificity=\frac tn+fp kappa=\frac accuracy- =\frac tp+fn tp+fp fp+tn fn+tn tp+tn+fp+fn number correctly classified positive sample number correctly classified negative subject represent number falsely classified negative positive sample respectively result identification cmv-associated tcrs fisher exact test based number sample cmv-associated tcrβ present positive negative sample training sample cohort number sample without data sequence confusion matrix sequence built calculate p-value fisher exact test therefore correlation sequence obtained fisher exact test related cohort training data little direct correlation test data cohort combination different threshold p-value thus different number tcrβ sequence selected number cmv-associated tcrβ sequence corresponding threshold 309,406 respectively graph created based threshold value number cmv-associated tcrβ corresponding threshold value shown fig figure associated tcrβ sequence picture show number cmv-associated tcrβ sequence corresponding different threshold value full size image best cutoff p-value lda svm figure describes performance four algorithm classification effectiveness metric measuring data four classification algorithm x-axis represents different threshold value y-axis donates algorithm different threshold corresponding four evaluation metric shown fig score logistic regression algorithm present trend first rising falling increase threshold higher level threshold threshold increase auc value first increase beginning decrease increasing threshold higher threshold highest fdr false discovery rate first decrease increase increasing threshold lowest cross-entropy loss function decrease slowly first increase rapidly increase threshold lowest second figure show score svm algorithm first increase decrease increase threshold higher level threshold auc value first increase decrease increasing threshold higher threshold fdr error detection rate first increase decrease increase increasing threshold reach highest level 0.0851 cross-entropy loss function svm first decrease increase increase threshold lowest low 0.2609 figure depicts trend random forest algorithm different threshold previous two algorithm score auc still increase beginning decrease increase threshold higher level intermediate threshold fdr false discovery rate cross-entropy loss function fluctuate threshold increase lower figure depicts score auc lda linear discriminant analysis first increase decrease increase threshold higher level intermediate threshold achieved highest level similarly fdr loss function first decrease increase increasing threshold take lowest value figure evaluation metric algorithm score four algorithm logistic regression support vector machine svm random forest linear discriminant analysis lda based four evaluation metric purple red orange blue curve represent score fdr auc cross-entropy loss respectively first three indicator left y-axis cross-entropy loss right y-axis full size image decision boundary algorithm shown fig point fall pink area mean algorithm predicts positive otherwise sky-blue area mean negative figure classification diagram logistic regression algorithm segmentation line presented upward sloping straight line segmentation effect good wrongly classified test sample point easy see naked eye figure show classification graph svm algorithm distribution test sample point due use polynomial kernel function better performance area divided svm algorithm contains certain degree circular structure still area upper left corner classified negative figure classification diagram random forest algorithm overall division image seems fitting easy cause model perform well training sample poorly test sample classification diagram linear discriminant analysis algorithm shown fig closer straight line algorithm division detailed smoother robust classification effect figure scatter plot area classification line testing sample figure depicts scatter plot positive negative decision boundary obtained four classification algorithm trained cohort1 training sample cohort2 test sample x-axis represents total number tcrβ sequence specie per sample y-axis donates number repeat specie associated tcr sequence blue dot represent negative sample cohort red dot represent positive sample cohort pink sky blue region represent positive negative region obtained training algorithm cohort1 training data respectively figure show classification graph algorithm distribution test sample point svm lda respectively full size image also depicted distribution training sample cohort area decision boundary shown supplementary fig best performance algorithm support best cutoff fisher exact test value learning fig appropriate supplementary fig lda svm supplementary fig best cutoff fig show accuracy sensitivity specificity algorithm according best performance corresponding best threshold algorithm three coordinate x-axis accuracy sensitivity specificity left right algorithm lda svm labeled different color significant difference accuracy four algorithm optimal threshold accuracy rate nearly accuracy rate lda highest reaching 92.86 followed svm 91.96 lowest rate 89.29 term sensitivity lda performed well reaching 95.83 three algorithm range indicating lda inclined classify sample positive based higher classification accuracy term specificity svm algorithm performed better 96.88 indicating two algorithm inclined classify sample negative figure lda perform better cmv data figure depicts optimal performance algorithm corresponding optimal threshold value obtain accuracy sensitivity specificity algorithm full size image aspect measuring effect classification consistency kappa coefficient svm lda 0.8346 0.8364 0.8518 0.8364 respectively mean predicted result four model consistent actual classification result discussion algorithm discussion sensitivity degree sensitivity detection positive sample specificity degree sensitivity negative sample accuracy broad measure accuracy sensitivity overemphasized specificity overemphasized overemphasizing importance sensitivity easily make classifier sensitive increase false positive rate actual negative conversely overemphasizing specificity easily make classifier conservative large number positive may missed accuracy sensitivity specificity affected judgment threshold selected best judgment threshold auc classify model calculate corresponding accuracy rate score auc fdr cross-entropy loss fig accuracy sensitivity specificity supplementary fig useful visualizing performance classifier selecting optional classifier fig model higher score auc lower fdr cross entropy loss better classification model addition higher accuracy sensitivity specificity represents better classification effect well seen foregoing accuracy rate lda algorithm tied highest difference sensitivity specificity algorithm larger lda accuracy lda highest sensitivity specificity considered high level time therefore shuffling data best-performing algorithm among four algorithm lda algorithm main reason division data linearly separable lda algorithm suitable choice linearly inseparable lda algorithm unlikely best-performing algorithm research evaluation mentioned article creatively applied four binary classification algorithm two-dimensional tcrβ array high-throughput sequencing diagnose infection history cytomegalovirus turn methodology effective achieving goal overall logic self-consistent addition tried different algorithm different parameter adjustment method calculating accuracy rate best judgment threshold determined auc consequently accuracy rate result greatly improved average accuracy rate reach however still many ensemble learning algorithm neural network algorithm obtain better result whether multidimens scientific research exploration attempted future article whether binary classification algorithm ensional classification algorithm tried obtain critical tcr sequence expression disease whether methodology paper also applicable disease conclusion article four binary classification algorithm proven achieve excellent performance diagnosing cmv exposure history subject unique tcrβ cmv-associated tcrβ perspective auc evaluation dimension lda performs better two-dimensional array cmv virus three algorithm summary current study revealed important signal linear division model lda effective division effect nonlinear separable algorithm random forest relatively inaccurate probably two-dimensional distribution cmv data sample linearly separable importantly t-classifier may potential diagnostic method cmv even virus