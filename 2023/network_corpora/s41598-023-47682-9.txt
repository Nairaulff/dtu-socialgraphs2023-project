introduction generative pretrained transformer gpt large language model llm powered artificial intelligence developed openai order understand generate human-like language llm pretrained substantial volume text data internet employing neural network architecture called transformer latest version model gpt-3.5 gpt-4.0 launched march march respectively model available optimized natural conversation chatgpt web app however gpt-4.0 currently accessed paid monthly subscription growing interest around chatgpt led considerable number recent publication investigating impact language model medical environment educational workflow perspective ophthalmology setting chatgpt tested triaging patient symptom answering patient care giver question specific ocular disease knowledge four paper analyzed performance llm answering multiple-choice question simulation board certification exam one included direct comparison human gpt-3.5 gpt-4.0 limited sample american academy ophthalmology aao basic clinical science course bcsc question purpose study compare overall performance human gpt-4.0 gpt-3.5 larger sample multiple-choice question aao bcsc self-assessment program available http subscribed member furthermore comparative performance three respondent according different categorization question including subspecialties practice area calculated method comparative cross-sectional study multiple-choice question included self-assessment test clinical education section aao official website http submitted june chatgpt using gpt-4.0 gpt-3.5 language model multiple-choice question divided set based subspecialty cataract/anterior segment cornea/external disease glaucoma neuro-ophthalmology/orbit ocular pathology/oncology oculoplastics/orbit pediatric ophth/strabismus refractive mgmt/intervention retina/vitreous uveitis additionally masked investigator classified question according practice area diagnostics/clinics medical treatment surgery since chatgpt analyze multimedia content question containing image excluded study consecutive multiple-choice question entered separate chat-rooms almost every response chatgpt discussed topic submitted question selected option chatgpt choose provided answer investigator resubmitted question new chat adding prompt must choose one question mark force definite choice upon submitting multiple-choice option selected chatgpt self-assessment test correct answer revealed result human obtained past completed examination stored aao website rather direct engagement human participant answering question percentage human chose distinct option revealed website regrettably aao doe provide information number human respondent level education degree proficiency ophthalmology question answered incorrectly human user categorized difficult answered correctly least human considered easy aao question chatgpt output chosen answer along percentage human selected multiple-choice option recorded secondary outcome word count question response chatgpt calculated ass verbosity word count response difference correct incorrect answer correlation length question length discussion provided data entered electronic database via microsoft office excel microsoft corp. redmond analyzed ibm spss statistic version 29.0 ibm armonk new york ethical approval informed consent required since study involve human participant statistical analysis number correct answer human gpt-4.0 gpt-3.5 compared chi-square fisher exact test analysis categoric variable analysis repeated subspecialty set practice area kolmogorov–smirnov test performed determine normality data paired student test normally distributed variable used compare word count response provided gpt 4.0 gpt 3.5 independent student test performed compare word count response provided chatgpt question answered correctly answered wrong univariate analysis used compare word count question answer chatgpt appropriate value expressed mean standard deviation word count chatgpt response always rounded test two-sided value 0.05 considered statistically significant ethic approval observational study human recruited local ethical committee comitato etico area centro regione calabria exempted study requirement ethical approval result june self-assessment test http included multiple-choice question 4.7 excluded analysis incorporated image total question submitted gpt-4.0 gpt-3.5 table present detailed summary question distribution across different subspecialities practice area table question distribution across different subspecialties practice area full size table ensure study adequately powered detect meaningful difference group responder conducted power analysis based percentage question answered correctly gpt-4.0 84.3 gpt-3.5 69.5 human 72.9 previous study lin al. also employed multiple-choice question aao bcsc program analysis indicated achieve 0.80 power 0.05 significance level study include sample size question comparison gpt-4.0 gpt-3.5 question gpt-4.0 human notably larger sample question gpt-3.5 human case chatgpt select answer first query question 2.73 gpt-4.0 3.61 gpt-3.5 1.29 0.25 upon resubmitting question additional prompt must choose one multiple-choice option always selected overall gpt-4.0 gpt-3.5 answered correctly 82.4 65.9 question respectively mean percentage question answered correctly human 75.7 17.2 corresponding 763–785 question number correct answer gpt-4.0 significantly higher human user 14.02 0.0002 gpt-3.5 72.82 0.0001 conversely gpt-3.5 compared unfavorably human 23.65 0.0001 subspecialty group gpt-3.5 performance significantly variable across different subspecialties 31.64 0.0002 particular gpt-3.5 obtained highest percentage correct answer ocular pathology/oncology 77.1 lowest pediatric ophthalmology/strabismus 53.3 conversely gpt-4.0 human showed consistent result significant difference across subspecialty group 13.24 0.15 3.06 0.96 respectively despite fact gpt-4.0 performed better human subspecialties difference statistically significant glaucoma 5.00 0.02 oculoplastics/orbit 4.04 0.04 post-hoc power analysis performed check whether lack significant difference subspecialties could determined insufficient sample size power analysis confirmed suspicion showing low statistical power comparison subspecialty group gpt-3.5 showed worse performance obtaining significantly lower score human cataract/anterior segment 9.64 0.002 glaucoma 5.11 0.02 neuro-ophthalmology/orbit 8.56 0.003 pediatric ophthalmology/strabismus 12.29 0.0004 table fig present result human gpt-4.0 gpt-3.5 across subspecialties table number percentage correct answer human gpt-4.0 gpt-3.5 across different subspecialties practice area full size table figure percentage correct answer gpt-4.0 gpt-3.5 human according subspecialty group indicates value 0.05 indicates value 0.01 full size image practice area group gpt-4.0 gpt-3.5 showed significant difference performance among practice area 6.86 0.03 6.43 0.04 respectively gpt-4.0 gpt-3.5 obtained best score diagnostics/clinics 83.7 68.1 respectively worst surgery 74.6 57.0 respectively conversely human user achieved better consistency significant difference across practice area 0.31 0.86 specifically gpt-4.0 performed better human diagnostics/clinics 13.78 0.0002 medical treatment 3.08 0.08 gpt-3.5 obtained significantly lower score human practice area always 0.01 table fig present result across practice area figure percentage correct answer gpt-4.0 gpt-3.5 human according practice area group indicates value 0.01 full size image difficulty group total 9.0 question answered incorrectly human user majority question included refractive 23.5 ocular pathology/oncology 17.1 subspecialties difficult case mean percentage correct answer human 40.1 7.5 gpt-4.0 gpt-3.5 performed better human question 53.3 47.8 respectively without reaching significancy always 0.05 difference gpt-4.0 gpt-3.5 also significant 0.54 0.46 overall 11.7 question answered correctly human evaluate major mistake number time chatgpt chose wrong answer question tracked mean percentage correct answer human subset 96.4 1.2 question answered incorrectly gpt-4.0 98.3 gpt-3.5 92.5 significant difference gpt model human 0.2 however difference gpt-4.0 gpt-3.5 almost reached significant level 0.059 figure present result easy difficult question figure percentage correct answer gpt-4.0 gpt-3.5 human easy difficult question full size image word count analysis average word count answer provided gpt-4.0 significantly lower produced gpt-3.5 156–163 201–211 respectively 0.0001 furthermore significant increase word count response generated question answered incorrectly compared answered correctly gpt-4.0 0.0001 gpt-3.5 0.02 according univariate analysis word count response gpt-4.0 gpt-3.5 positively correlated word count question always 0.001 0.212 0.153 respectively discussion study compared performance human gpt-4.0 gpt-3.5 large ophthalmology test based multiple-choice question instead running simulation inside aao bcsc web app includes random question extracted database entry comprehensive self-assessment test available http containing total question evaluated question extracted large database bcsc program always user thus improving reproducibility result compared future study knowledge study included largest number ophthalmological question submitted chatgpt pubmed keywords gpt ophthalmology year 2021–2023 difference correct answer among three study group statistically significant gpt-4.0 achieved best score 82.4 followed human 75.7 gpt-3.5 65.9 interestingly gpt-4.0 gpt-3.5 showed consistent result across different practice area compared human diagnostics/clinics easiest practice area 83.7 68.1 respectively medical treatment following closely 83.4 65.1 instead surgery hardest area model 74.6 57.0 possible hypothesize answer clinical question easier deduct web resource surgery may involve highly specialized technique protocol may well-represented data language model trained likewise surgical knowledge often relies image video included training database concerning subspecialties gpt-3.5 also demonstrated significant inconsistency whereas gpt-4.0 achieved degree consistency much closer human performance performance gpt-4.0 gpt-3.5 analyzed considering difficult question defined question answered incorrectly human user remarkably model performed better human challenging subset difference statistically significant 40.1 human 53.5 gpt-4.0 47.8 gpt-3.5 outcome easiest question defined answered correctly human comparable among model human user 96.4 human 98.3 gpt-4.0 92.5 gpt-3.5 although multiple-choice question word count chatgpt response impact selection correct answer succinctness still play significant role user experience especially healthcare environment clear concise answer generally preferred long complexity topic respected overall length chatgpt answer directly proportional word count question gpt-4.0 output significantly succinct gpt-3.5 vs. word hand response significantly longer question answered incorrectly language model although explaining technical reason behind behavior beyond scope study observed excessive verbosity often associated evasive generic response hinting potential uncertainty resolution question author performed similar research ass potential chatgpt ophthalmology lin compared performance gpt-4.0 gpt-3.5 human setting aao bcsc self-assessment program simulated exam question three group respondent scored 84.3 69.5 72.9 respectively result consistent finding 1.9 3.6 2.8 respectively however lin marked non-answers incorrect included additional prompt must choose one force answer difference design limit relatability comparison two study furthermore lin took alternative approach ass difficulty question instead evaluating difficulty based percentage human user answered correctly question masked investigator classified question first-order fact recall higher-order evaluative/analytical task problem-solving recently moshirfar evaluated performance gpt-4.0 gpt-3.5 human answering question statpearls question bank respectively scoring direct comparison completely appropriate since two different datasets question used score respondent lower found 9.4 10.9 17.7 respectively study difficulty score ranging provided statpearls result human affected difficulty grading gpt-3.5 showed notable decline performance level difficulty increased trend much evident gpt-4.0 limitation study lie fact aao self-assessment test exclusively reveals percentage human selected certain multiple-choice option without disclosing number respondent degree knowledge additionally reproducibility chatgpt answer assessed herein although previous study antaki reported almost perfect repeatability despite great potential generative many challenge outstanding issue emerging first source massive amount data used train llm subjected rigorous quality control high risk false misleading information regard crucial understand chatgpt doe really understand question instead answer based probability correlation text found within training datasets statistical pattern low quality data may introduce algorithmic bias possibly reinforcing old outdated knowledge finally generate hallucination consisting completely fabricated information factual inaccuracy logical inconsistency nonsensical response malevolent chatgpt also conceivable posing serious threat integrity scientific research instance many author reported chatgpt could produce seemingly authentic scientific manuscript abstract avoiding plagiarism check conclusion gpt-4.0 represented substantial improvement gpt-3.5 achieving higher performance compared human self-assessment test american board ophthalmology examination however chatgpt still limited performance inconsistency across different practice area especially come surgical/practical knowledge currently utility chatgpt extends beyond comprehensive repository ophthalmological knowledge based language model demonstrated remarkable capability real-time data interpretation potential revolutionize teleophthalmology service providing almost instant response patient query well assisting ophthalmologist clinical decision-making research administration acknowledging chatgpt shortcoming hallucination potential abuse crucial successful integration technology medical setting