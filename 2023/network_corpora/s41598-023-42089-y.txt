introduction reinforcement learning achieved great success demonstrated human superhuman ability various task mastering video game game success natural apply technology scientific field require complex control capability fact used study quantum control quantum error correction optimization variational quantum algorithm also used optimize structure parameter neural network called neural architecture search specifically train agent sequentially add different neural network component convolution operation residual connection pooling automatically generates high-performance neural network evaluating model performance adjust component structure already comparable human expert specific task effectively reducing neural network use implementation cost quantum algorithm proven exponential quadratic operational efficiency improvement solving specific problem compared classical algorithm integer factorization unstructured database search recent study variational quantum algorithm vqa applied quantum computing many scientific domain including molecular dynamical study quantum optimization various quantum machine learning qml application regression classification generative modeling deep reinforcement learning sequence modeling speech identification distance metric learning transfer learning federated learning however designing quantum circuit solve specific task easy requires domain knowledge sometimes extraordinary insight recently deep reinforcement learning-based quantum architecture search qas-ppo approach proposed automatically generate quantum circuit via proximal policy optimization ppo algorithm without expert knowledge physic specifically qas-ppo ppo method optimize interaction agent quantum simulator learn target quantum state interaction agent sequentially generates output action candidate quantum gate quantum operation placed circuit fidelity generated quantum circuit evaluated determine agent whether agent reached goal process performed iteratively train agent despite success qas-ppo neither strictly limit probability ratio old new policy enforce well-defined trust domain constraint resulting poor performance paper proposes new deep reinforcement learning-based qas approach named trust region-based ppo rollback qas qas-tr-ppo-rb automatically build quantum gate sequence density matrix specifically inspired research work wang adopt improved clipping function implement rollback behavior limit probability ratio new strategy old strategy prevent strategy pushed away training moreover optimize strategy within trust region replacing clipped trigger condition based trust region guarantee monotonic improvement experimental result several benchmark task demonstrate proposed method observably improves policy performance algorithm running time compared original deep reinforcement learning-based qas method rest paper arranged follows preliminary present preliminary reinforcement learning advantage actor-critic a2c proximal policy optimization ppo quantum architecture search qas-ppo method reviewed quantum architecture search deep reinforcement learning method proposes new deep reinforcement learning-based qas algorithm called trust region-based ppo rollback qas qas-tr-ppo-rb specifically adopt improved clipping function implement rollback behavior limit probability ratio new strategy old strategy prevent strategy pushed away training experiment present several experimental comparative result automatic generation quantum circuit multi-qubit target state show superiority presented method finally conclude paper conclusion preliminary reinforcement learning reinforcement learning algorithm maximizes value function called value-based reinforcement learning unlike value-based learns value function reference generate decision step another method called policy gradient method strategy function a|s parameterized parameter affected optimization procedure rise gradient ascent expected total return r_t one classic example strategy gradient algorithm reinforce algorithm standard reinforce algorithm parameter updated along direction a_t|s_t r_t\ unbiased estimate r_t however strategy gradient method affected variance r_t making training difficult reduce estimate variance keep unbiased learning function state b_t s_t baseline substracted return value result a_t|s_t r_t-b_t s_t advantage actor-critic a2c estimation value function common choice baseline b_t s_t s_t choice usually result much lower variance estimation strategy gradient using approximation value function basic line quantity r_t-b_t=q s_t a_t s_t regarded advantage function s_t a_t action state s_t\ intuitively one see advantage nice nasty action compared average value state s_t example s_t a_t equal given time-step clear whether a_t\ good action however also know s_t equal say imply a_t\ may bad conversely s_t equal advantage -5\ meaning value action well average s_t therefore action good approach called advantage actor-critic a2c approach strategy actor value function critic proximal policy optimization ppo strategy gradient method policy optimized gradient descent according policy loss function policy -\log a_t|s_t however training may suffer instability step size policy update small training process slow hand step size larger training high variance proximal policy optimization ppo solves problem restricting strategy update step size training step specifically ppo introduces loss function called clipped proxy loss function restrict strategy change small range help clip consider ratio probability action present strategy probability anterior strategy q_t =\frac a_t|s_t a_t|s_t old q_t mean action higher probability present strategy old one q_t mean action probable present strategy old one new loss function defined policy q_t a_t a_t|s_t a_t|s_t old a_t a_t=r_t-v s_t advantage function however action current policy much probable previous policy ratio q_t\ may large leading large policy update step circumvent problem original ppo algorithm add constraint ratio range 0.8 1.2 modified loss function defined follow aligned policy -\min q_t a_t clip q_t a_t aligned clipping function clip denoted aligned clip q_t array 1-\epsilon q_t 1-\epsilon 1+\epsilon q_t 1+\epsilon q_t else array aligned 1-\epsilon ,1+\epsilon represents clipping range 0,1 clip hyperparameter common choice 0.2 finally value loss entropy bonus added total loss function usual policy +c_1l_ value -c_2h h_t -\begin matrix a_j|s_t a_j|s_t matrix entropy bonus used encourage exploration value r_t-v s_t value loss quantum architecture search quantum architecture search qas class approach using algorithm quantum simulated annealing qsa quantum evolutionary algorithm qea quantum machine learning qml quantum reinforcement learning qrl intelligent algorithm voluntarily search best quantum circuit given target quantum state existing research work show quantum circuit generated qas method based variational quantum algorithm reached even surpassed quantum circuit designed based human expertise however quantum architecture search algorithm automatically generate quantum circuit discrete environment often need evaluate performance many quantum circuit different structure resulting colossal resource consumption recently zhang presented differentiable quantum architecture search dqas method expanded space searched discrete domain continuous domain used gradient descent optimize entire quantum circuit generation process achieve relatively high performance quantum architecture search deep reinforcement learning given original quantum state ... target quantum state goal produce quantum circuit convert original state target state within specific fidelity threshold en-jui kuo use pauli measurement observation often-used setting quantum mechanic adopt two algorithm ppo a2c respectively achieve goal specifically environment represents quantum computer quantum simulator agent hosted classic computer interacts environment iteration step agent selects action set possible action consisting different quantum operation agent update quantum circuit based selected action environment test newly generated circuit computes fidelity given target quantum state currently developed state quantum state calculated fidelity reached exceeded predefined threshold round end agent receive positive feedback reward elsewise agent receive negative feedback reward process continues maximum number step required iteration terminate optimization algorithm interaction realized using reinforcement learning algorithm a2c ppo given number qubits n\in initial quantum state target state tolerance error set quantum gate goal algorithm discover quantum circuit construsting objective function aligned aligned 1-\epsilon composed gate distance metric two quantum state larger better paper use fidelity distance given two density operator fidelity two operator usually expressed ^2\ particular case represent pure quantum state i.e. respectively original expression reduced inner product two quantum state furthermore en-jui kuo verified performance proposed deep reinforcement learning-based qas algorithm using bell state greenberg–horn–zehlinger ghz state target quantum state respectively bell state achieves maximal two-qubit entanglement aligned bell =\frac aligned generate bell state en-jui kuo picked observation expectation value pauli matrix qubits 0,1 j\in action set aligned =\bigcup _0^ n-1 u_i x_i y_i z_i h_i cnot_ i+1 mod2 aligned n=2\ two qubits u_i pmatrix exp i\theta pmatrix single-qubit rotation applied -th qubit around -axis x_i ^i_x\ denotes pauli- gate likewise y_i\ z_i\ h_i\ represents hadamard gate cnot_ cnot gate -th qubit control bit -th qubit target bit action total ghz state multi-qubit generalization bell state equal superposition lowest highest energy state created aligned ghz =\frac aligned generate 3-qubit ghz state en-jui kuo adopted expectation value individual qubit pauli matrix resuting observables aggregate action en-jui kuo selected single-qubit gate sixe cnot gate two-qubit gate despite success qas-ppo neither strictly limit probability ratio old new policy enforce well-defined trust domain constraint resulting poor performance former problem mainly due inability ppo eliminate incentive pushing away strategy latter situation primarily due essential difference two type constraint used ppo trust region policy optimization trpo respectively method section address issue propose new deep reinforcement learning-based qas approach called trust region-based ppo rollback qas qas-tr-ppo-rb realistically adhering proximal property-bound strategy within trust region method significantly improve original deep reinforcement learning-based qas approach term policy performance sample efficiency analysis proximal property ppo ppo limit strategy reducing probability ratio old new policy however practice known probability ratio limited clipping range significant factor problem limiting mechanism eliminate excitation overall target function pushing out-of-the-range q_t beyond limit moreover ppo doe explicitly impose trust domain constraint probability ratio old new policy i.e. -divergence two strategy even probability ratio q_t bounded corresponding -divergence s_t old necessarily bounded ppo rollback quantum architecture search mentioned analysis proximal property ppo ppo method used qas-ppo method strictly constrain range probability ratio limiting mechanism eliminate motivation drive q_t beyond limiting range fact q_t often deviate constraint mechanism ultimately lead poor performance solve problem replacing clip function clip rollback function whose mathematical expression follows aligned q_t array -\alpha q_t 1+\alpha 1-\epsilon q_t 1-\epsilon -\alpha q_t 1+\alpha 1+\epsilon q_t 1+\epsilon q_t otherwise array aligned represents hyper-parameter control intensity rollback new overall target function q_t exceeds limit range rollback function q_t produce passive stimulation therefore offset excitation overall target function certain extent rollback operation prevents probability ratio q_t squeezed strongly original clip function pseudocode ppo rollback quantum architecture search approach shown trust region-based ppo quantum architecture search mentioned analysis proximal property ppo clipping function original deep reinforcement learning-based qas method probability ratio element clipping trigger condition make difference ratio-based constraint trust domain-based constraint used constraint probability ratio enough constrain -divergence ultimately lead poor performance therefore replace ratio-based clipping function trust domain-based clipping function formally strategy exceeds trust domain probability ratio tailored aligned q_t array q_t old s_t old q_t else array aligned hyperparameter q_t old =1\ constant strategy exceeds trust region s_t old incentive updating strategy removed although clipped value q_t old may make proxy target function uncontinuous discontinuity influence optimization parameter gradient affected constant value general proposed qas-tr-ppo method combine advantage trpo ppo theoretically reasonable subject trust domain easy implement need one-rank optimization one hand approach doe require -divergence s_t old optimize s_t old calculates determine whether q_t compared ppo used original method method different strategy metric limit strategy specifically unlike ppo ratio-based metric a_t|s_t a_t|s_t old used impose element-by-element constraint sampling action point method trusted domain -divergence matrix a_t|s_t old a_t|s_t a_t|s_t old matrix based trust region impose summation constraint action space crucially central willingness ratio-based regulation impose relatively strict restriction action old strategy doe like old small may result finite sample efficiency strategy initialized wrong solution contrary trust domain-based approach adopted prejudice tends show higher sample efficiency reality finally pay attention importance operation variant ppo operation function denoted aligned policy -\min q_t a_t q_t a_t aligned schulman proposed additional operation made policy become lower bound unclipped target function q_t a_t\ express target operation namely q_t a_t\ policy violates trust region stop updating even target value original value q_t a_t q_t old a_t\ operation virtually provides remedy problem therefore rewritten aligned policy array q_t old a_t s_t old q_t a_t q_t old a_t q_t a_t else array aligned way ratio clipped target value increase policy violates constraint trust region-based ppo quantum architecture search method follows shown algorithm trust region-based ppo rollback quantum architecture search however tailoring based trust domain may still problem unbounded probability ratio strategy exceeds trust region method proposed doe provide negative incentive leading poor performance therefore solve problem combining tailoring based trust domain rollback mechanism aligned tr-rb q_t array -\alpha q_t old s_t old q_t else array aligned shown exceeds trust domain proposed tr-rb q_t method produce negative excitation trust region-based ppo rollback quantum architecture search method follows shown algorithm experiment experimental setting optimizer paper employ adam optimizer training agent a2c ppo ppo-rb tr-ppo tr-ppo-rb case adam one gradient-descent method calculates self-adaptive learning rate parameter furthermore adam store exponentially damping mean gradient g_t\ square g^2_t\ aligned _t= t-1 1-\zeta g_t aligned aligned _t= t-1 1-\zeta g^2_t aligned _1\ _2\ hyperparameters use _1=0.9\ _2=0.999\ papaer _t\ v_t\ adjustable according following equation offset bias towards aligned _t= m_t 1-\zeta ^t_1 aligned aligned _t= 1-\zeta ^t_2 aligned parameter _t\ method time step updated according following equation aligned t+1 =\theta _t-\frac +\epsilon aligned quantum noise quantum simulator paper consider two form error gate error measurement error gate error refers defect quantum operation algorithm execution measurement error refers error generated quantum measurement process specifically gate error consider depolarizing noise replaces state qubit stochastic state probability gate measurement error think stochastic flip probability meas immediately actual measurement use following noise configuration simulation software test manifestation agent proposed approach error rate gate meas 0.001 error rate gate meas 0.005 density matrix quantum state generic form density matrix quantum state basis aligned =\sum p_j aligned p_j\ denotes probability quantum system pure state matrix p_j=1 matrix example density matrix bell state adopted paper bell =\frac corresponding density matrix given aligned bell bell =\frac aligned quantum state tomography quantum state tomography qst reconstructs quantum state quantum system quantum measurement play important role verifying benchmarking quantum device various quantum information processing task expand density matrix pauli basis qubits aligned =\frac 2^n i_1 i_1 i_1 i_n aligned 4^n-1\ measurement operation required determine minus one due conservation probability =1\ generally measurement using 4^n-1\ linearly independent projection operator uniquely determine density matrix special case projector pauli operator therefore number measurement increase exponentially qubit number pose huge challenge verifying multi-qubit quantum state experiment limited number shot expectation value i_1 measured within certain accuracy paper adopt ibm qiskit software package perform quantum state tomography simulation customized openai gym environment use customized openai gym environment verify performance proposed algorithm experimental environment objective quantum state fidelity threshold quantum computation backend real machine simulator software set user form parameter addition user also customize noise mode specifically use following parameter setting build test environment observation agent receives pauli- expected value qubit action agent choose quantum gate run specific qubit reward successfully reaching goal agent receive -0.01\ reward every step encourage exploring shortest path agent reach goal obtain reward parameter setting paper think five algorithm paper parameter setting shown follow a2c learning rate =10^ discount factor =0.99\ ppo =0.002\ =0.99\ clip range parameter c=0.2\ update epoch number k=4\ ppo-rb =0.002\ =0.99\ clip range parameter c=0.2\ update epoch number k=4\ slope rollback =-0.3\ tr-ppo =0.002\ =0.99\ clip range parameter c=0.2\ update epoch number k=4\ klrange =0.03\ tr-ppo-rb =0.002\ =0.99\ clip range parameter c=0.2\ update epoch number k=4\ klrange =0.03\ slope rollback =-0.3\ noise-free environment performance 2-qubit bell state firstly show experimental comparison result different deep reinforcement learning-based qas method generating 2-qubit bell state scratch noise-free environment shown fig find deep reinforcement learning-based qas method successfully train agent synthesize bell state however neural network proposed algorithm obtains better policy performance running time method figure show bell state quantum circuit generated proposed method noise-free two-qubit system figure comparison average score runtime different deep reinforcement learning-based qas method quantum architecture search noise-free two-qubit system full size image figure quantum circuit bell state generated agent noise-free two-qubit system full size image 3-qubit ghz state secondly show experimental comparison result different deep reinforcement learning-based qas method generating 3-qubit ghz state scratch noise-free environment shown fig find deep reinforcement learning-based qas method successfully train agent synthesize ghz state however neural network proposed method reach optimal policy performance faster algorithm running time compared method fig provide quantum circuit ghz state generated proposed method noise-free three-qubit system figure comparison average score runtime different deep reinforcement learning-based qas method noise-free three-qubit system full size image figure quantum circuit ghz state generated agent noise-free three-qubit system full size image 4-qubit ising spin glass state thirdly focus classical problem combination optimization namely ising spin glass energy function aligned c=\frac j=1 _i^z _j^z+\sum i=1 _i^z aligned represent independent gaussian stochastic variable zero-mean zero-variance j^2=h^2=1\ ^z\ spin take value use metropolis algorithm calculate ground state hamiltonian system ising spin glass model target quantum state show experimental comparison result different deep reinforcement learning-based qas method generating 4-qubit ising spin gal state scratch noise-free environment shown fig observe deep reinforcement learning-based qas method successfully train agent synthesize ising spin gal state however neural network proposed algorithm obtains better policy performance running time method figure show ising spin gal state quantum circuit generated proposed method noise-free four-qubit system figure comparison average score runtime different deep reinforcement learning-based qas method noise-free four-qubit system full size image figure quantum circuit 4-qubit ising spin gal state generated agent noise-free four-qubit system full size image noisy environment performance 2-qubit bell state fourthly show experimental comparison result different deep reinforcement learning-based qas method generating 2-qubit bell state scratch noisy simulation environment shown fig observe deep reinforcement learning-based qas method successfully train agent synthesize bell state however neural network proposed algorithm obtains better policy performance running time method figure show bell state quantum circuit generated proposed method noisy simulation two-qubit system figure comparison average score runtime different deep reinforcement learning-based qas method noisy two-qubit system full size image figure quantum circuit bell state generated agent noisy two-qubit system full size image shown fig trust region-based clipping method qas-tr-ppo qas-tr-ppo-rb divergence also smaller qas-ppo especially qas-tr-ppo show enhanced restriction ability divergence even doe incorporate rollback mechanism furthermore proportion out-of-range probability ratio qas-tr-ppo-rb much original qas-ppo training process probability ratio divergence qtr-tr-ppo-rb also much smaller qas-ppo rollback operation divergence regarded penalty regularization term aligned penalty =q_ -\alpha old aligned penalty-based method usually notorious difficulty adjusting trade-off coefficient ppo-penalty address issue adaptively adjusting rollback coefficient achieve target value divergence however penalty-based ppo doe perform well clipping-based one difficult find effective coefficient-adjustig strategy across different task method introduces clipping strategy assisst restricting policy i.e. penalty enforced policy trust region policy inside trust region objective function affected penalty term mechanism could relieve difficulty adjusting trade-off coefficient alter theoretical property monotonic improvement show practice found qas-tr-ppo-rb robust coefficient achieve better performance across different task clipping technique may served effective method enforce restriction enjoys low optimization complexity seems robust analyse monotonic improvement property use maximum divergence instead i.e. aligned policy q_t a_t- array t+1 t+1 old t+1 t+1 old q_t a_t q_t old a_t else array aligned maximum divergence also used trpo theoretical analysis objective function also posse theoeretical property guaranteed monotonic improvement let new =\arg policy new trpo =\arg denote optimal solution qas-tr-ppo-rb trpo respectively follow theorem theorem max_t a_t 1-\gamma ^2\ old new trpo new old =e_ s^t a^t s^t a^t proof firstly prove two property new trpo note =e_t q_t a_t -\alpha t+1 t+1 old new trpo optimal solution aligned e_a new trpo e_a old s_t aligned assume new optimal solution exists t+1 e_a t+1 t+1 e_a t+1 old t+1 construst new policy aligned array old e_a t+1 t+1 e_a t+1 old t+1 else array aligned contradicts optimal policy besides also obtain s_t\ exists least one a'\ old therefore aligned policy new trpo +\zeta old =l_ policy new trpo -\alpha old new trpo aligned prove new trpo optimal solution policy three case satisfies old exist a'\ old s^t\ aligned aligned policy +\zeta old =l_ policy -\alpha old policy new trpo -\alpha old new trpo =l^ policy new trpo +\zeta old aligned aligned satisfies old aligned aligned policy +\zeta old =l_ policy -\alpha policy -\alpha old policy new trpo -\alpha old new trpo =l^ policy new trpo +\zeta old aligned aligned prove case satisfies old exist s^t\ t+1 old t+1 aligned aligned e_a policy =e_a t+1 -\alpha e_a t+1 old -\alpha e_a t+1 new trpo -\alpha t+1 t+1 old new trpo =e_a policy t+1 new trpo aligned aligned construst new policy aligned array new trpo t+1 else array aligned aligned aligned policy +\zeta old =e_ policy +\zeta old policy +\zeta old =l_ policy -\alpha old policy +\zeta old new trpo =l^ policy new trpo +\zeta old aligned aligned summary new =\zeta new trpo new trpo old =\zeta old 3-qubit ghz state show experimental comparison result different deep reinforcement learning-based qas method generating 3-qubit ghz state scratch noisy simulation environment shown fig observe give neural network architeure method performs signficantly better original deep reinforcement learning-based qas method term runtime policy performance figure show ghz state quantum circuit generated proposed method noisy simulation three-qubit system figure comparison average score runtime different deep reinforcement learning-based qas method noisy three-qubit system full size image figure quantum circuit ghz state generated agent noisy three-qubit system full size image 4-qubit ising spin glass state finally show experimental comparison result different deep reinforcement learning-based qas method generating 4-qubit ising spin gal state scratch noisy environment shown fig observe although deep reinforcement learning-based qas algorithm successfully train agent synthesize ising spin gal state neural network proposed approach obtains better policy performance running time method fig show ising spin gal state quantum circuit generated proposed method noisy four-qubit system figure comparison average score runtime different deep reinforcement learning-based qas method noisy four-qubit system full size image figure quantum circuit 4-qubit ising spin gal state generated agent noisy four-qubit system full size image conclusion paper present new deep reinforcement learning-based qas approach named trust region-based ppo rollback qas qas-tr-ppo-rb automatically build quantum gate sequence density matrix specifically inspired research work wang adopt improved clipping function implement rollback behavior limit probability ratio new strategy old strategy moreover optimize strategy within trust region replacing clipped trigger condition based trust region guarantee monotonic improvement way method improve original deep reinforcement learning-based qas method policy performance algorithm running time