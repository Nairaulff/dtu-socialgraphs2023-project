introduction experimental methodology three dimensional tomography internal microstructure material refined considerably past decade growing include variety modality x-ray computed tomography optical imaging electron imaging energy dispersive x-ray spectroscopy electron back-scattered diffraction ebsd data among others case volume material interrogated slice slice slice stacked reconstructed using variety software tool three dimensional microstructure data often laborious generate compared collecting single section provide unique insight including thing like topology microstructural feature like grain pore precipitate true shape size distribution feature property fatigue oxide transport sensitive three dimensional arrangement feature novel manufacturing process like additive manufacturing also demand three dimensional characterization microstructure fully link processing structure advancement made robustness closed loop collection data corruption data still happen may significant problem non-destructive technique data recollected destructive technique like serial sectioning may often case slice thousand data poor quality lowering accuracy reconstruction corruption happen number reason including removing material via mechanical polishing laser/ion abalation planned electron source nearing end life resulting low signal image magnification microscope incorrect slice brightness/contrast setting result over/under saturated image adjustment control software made prevent issue hard a-priori imagine every reason slice subset data corrupted however data collected properly hypothesize possible infer substantial amount missing data current common practice fill missing slice copy layer missing slice reasonable serial sectioning case slice collected high enough frequency data stay slice slice still room improvement nearest neighbor replacement type approach transformer model used fill text data tantalizing framework also filling missing data sequential image data since introduction transformer backbone many impressive breakthrough natural language processing nlp general form transformer excellent processing sequential information parallel consequently use spread across multiple domain computer vision speech processing bioinformatics case transformer appealing ebsd data reading inherently sequential represents real physical structure early large transformer model bert gpt pretrained copious amount text data intention learning feature model language association word phrase collection self-supervised task unlike supervised task classification requires labeled datasets self-supervised task involve automatically generating label unlabeled data unscrambling sequence shuffled sentence force model vaguely learn structure data technique remained fairly intact even scaling billion-parameter large language model llm enormous model process much longer sequence course hundred billion token language unit comprising sequence character taking inspiration similar popular self-supervised task nlp training procedure involves randomly masking slice ebsd volume model predict masked slice slice recovery task closely related masked image modeling masked autoencoders inpainting computer vision even though may applicable exists plenty transformer computer vision want model also computationally efficient scale method leverage fact ebsd volume sparse structure dynamic operate differently typical image video regarding efficiency fact ebsd produce high dimensional data mean final model computational footprint ignored taking consideration contribution following propose novel method recover missing ebsd data consisting scalable transformer model straightforward projection algorithm produce superior result compared existing method accentuated zoning recovery accuracy grain boundary method appear perform poorly demonstrate despite trained solely synthetic data transformer generalize real ebsd data without additional training still outperforming baseline robustness out-of-distribution ebsd data overcomes major limitation relatively low amount available real ebsd data deep learning requires result suggest serial sectioning experiment using similar experimental parameter test datasets collection time effectively slashed little error predicted voxel orientation collection every fourth slice bypassed current lengthy procedure ebsd significant improvement efficiency background terminology notation paper use bold capital letter e.g matrix capital calligraphic letter e.g tensor scalar represented plain uppercase lowercase letter e.g tuples used describe shape structure n_1 n_2 n_3 shape n_1 n_2 n_3 vice versa dimension index starting shape tuple size dimension value shape tuple index refer multiple volume share first dimension e.g n_1 n_2 n_1 n_2 n_3 n_1 n_2 n_3 use capture remaining possibly different dimension e.g shape n_1 n_2 transformer section outline technical detail encoder-only transformer based first introduction source describes encoder-decoder decoder-only transformer interested reader input processing first layer transformer embedding layer single length sequence -dimensional vector vector transformed vector size input sequence token language task transformation could linear one complex nonlinear one another neural network result embedded sequence shape next positional encoding added embedding inject positional information vector sequence positional encoding either fixed learned multi-head self-attention idea self-attention find varying degree association element sequence transformer typically implement multi-headed self-attention within encoder layer allows model learn different type association let input sequence attention mechanism _h^q _h^k _h^v head additionally let row-wise softmax operator attention head _h\ defined aligned _h^\top aligned _h^q _h^k _h^v\ query key value matrix respectively next result concatenated linearly transformed produce output _o\ note _h\ computed parallel main source inefficiency computation _h^\top\ levy computational memory cost n^2 consequently utility transformer limited long sequence without proper hardware becomes dire issue dimensionality input grows image video flattened long sequence thankfully substantial work reducing complexity efficient attention approximation mechanism linformer reformer big bird many others axial self-attention come high-dimensional data ebsd volume sequence length explodes flatten patchify case many transformer computer vision input apply vanilla self-attention making attention computation serious bottleneck furthermore since observe lot structure ebsd data reasonable utilize simplified attention mechanism model axial attention run self-attention mechanism along dimension input tensor instance cube voxel attends voxels row column depth greatly reduces amount computation memory especially higher order tensor intuitively axial attention appropriate hypothesize since local information quite uniform nearby voxels likely grain long range information also included axial attention highly likely also obtain information grain boundary term implementation formula multi-headed attention reused let n_1 n_k single -dimensional input n_1 n_k defines shape volume embedding size example suppose interested finding axial attention along -th dimension proper dimension permutation flattening reshape tensor shape i\ne n_i n_k compute multi-headed attention treating first dimension batch model repeatedly use output axial attention compute axial attention along next dimension feedforward block next major component create transformer feedforward block multilayer perceptron single hidden layer activation function often either gaussian error linear unit gelu rectified linear unit relu function although input output size identical hidden size much larger consequently although self-attention computational bottleneck feedforward block dominant source parameter model putting together component ready define general architecture transformer first input processed embedding layer positional encoding layer start input progress encoder sequence alternating self-attention feedforward block training stability residual connection normalization layer inserted attention block feedforward block finally feature encoder linearly projected desired shape output visualization vanilla axial transformer mainly differ attention implementation depicted fig figure transformer architecture left either vanilla attention center axial attention right axial transformer produced simply substituting full attention axial attention although axial attention significantly layer scale much favorably high-dimensional data attention layer take three input query key value full size image method propose transformer model learn missing slice ebsd data followed projection step smooth voxel value described detail following section methodology summarized fig due limited real ebsd data goal train model large diverse synthetic dataset demonstrate generalize real ebsd data evaluate two nickel superalloy ebsd volume one alloy in625 one alloy in718 following repository contains code method http figure method overview transformer take sequence cubochoric ebsd slice one unobserved slice input produce output identical shape index unobserved slice contains orientation prediction along grain boundary sole contributor loss function output processed projection step assigns voxel grain based predicted orientation neighboring voxels full size image data description preparation dataset synthetic experimental includes orientation information every voxel image experimental data substatial amount preprocessing done handle alignment data clean noise complete description preprocessing may found chapman stinville particular remove grain smaller voxels 3^3\ average orienations per grain operating grain-average orientation simplifies orientation prediction problem doe represent real complexity orientation field often exhibit subtle local variation chose focus grain-averaged orientation first implementation primarily simplify initial interpretation transformer prediction additionally publicly available version in625 dataset contained grain-averaged orientation original volume containing euler angle shape n_1 n_2 n_3 n_1 n_2 n_3\ physical dimension final dimension represents euler angle needed define orientation additional array computed input data cubochorics volume shape n_1 n_2 n_3 last dimension contains cubochoric coordinate converted original euler angle voxel cubochoric coordinate chosesn since euclidean metric used regressing transfomer model euclidean distance point euler angle space doe necessarily relate angular distance point also true point cubochoric space cubochoric representation equal-volume mapping onto grid opposed equal-angle mapping euclidean distance cubochoric space approximates euclidean distance unit quaternion small misorientations since euclidean distance valid metric operate completely cubochoric space assumption euclidean metric reasonable approximation similarity point space volume shape n_1 n_2 n_3 assigns identification number denote grain voxel belongs number unique vector cubochoric coordinate associated needed training number used smooth model output evaluate model accuracy experimental data found segmenting grain using misorientation tolerance synthetic data generated alongside orientation data boundary volume shape n_1 n_2 n_3 containing boolean value indicating voxel grain boundary specifically voxel boundary voxel least one neighbor different number two voxels neighbor share face illustrate fig contains example slice scaled shifted cubochorics boundary in625 in718 figure three consecutive crop real ebsd slice in625 left in718 right top row show cubochoric value scaled shifted better visualization bottom row show region boundary note height width slice voxels original shape full size image synthetic volume generated via dream.3d synthetic training validation volume shape respectively within dream.3d generate training volume independently varying mean grain size mean transformation per grain particular generate volume mean grain size 2.5 twin also generate volume mean twin frequency fixing mean grain size 2.3 due nature software parameter unitless always synthetically increase decrease granularity volume example slice synthetic volume see fig grain size distribution dataset shown fig probability plot natural logarithm normalized grain size shown ideal lognormally distributed data would lie straight line plot general grain size primarily lognormal near mean noted deviation lognormality tail known phenomenon figure example synthetic slice cubochoric value scaled shifted visualization keeping mean grain size fixed show generated slice specify mean frequency twin per grain respectively twin show generated slice specify mean grain size respectively slice height width voxels full size image figure probablity plot showing grain size distribution dataset compare real test datasets synthetic training datasets without twin compare real test datasets synthetic training datasets twin grain size represented sphere equivalent diameter normalized distribution mean full size image training detail first describe data augmentation step number unique cubochoric coordinate quite limited color shift transformation along augmentation critical note use word color loose treat cubochoric coordinate color channel computer vision particular augmentation include random linear color shift rotation flip using synthetic volume generated dream.3d train axial transformer model self-supervised fashion similar masked language modeling task volume first normalized along three cubochoric index training input sampled randomly chosen volume physical dimension randomly permuted cropping necessary due computational limit also act another form augmentation sample _\star 64\times one central slice along second dimension randomly masked index masked unobserved slice define 0,1 64\times mask elsewhere model input output _\star 64\times respectively figure change slice captured boundary voxels top row show two consecutive slice bottom row show respective slice boundary red difference two slice overlaid white note difference slice lie fully boundary voxels slice full size image since ebsd produce structured data leverage design effective loss function simple mean-squared-error mse loss function across voxels would sufficient voxels input observed would risk learning identity function would produce fairly low loss value better approach would find mse missing slice analogous masked language modeling objective function however based fig see many voxels remain unchanged across multiple slice slice-to-slice change lie along grain boundary see fig since voxels within grain easy predict value source difficulty recovering boundary voxels therefore opt mse loss function considers boundary voxels missing slice formally letting index missing slice 0,1 missing slice boundary map boundary loss function use aligned _\star _\star aligned following broadcasting rule way defined _0\ number unobserved boundary voxels summary loss function essentially average mse error across unobserved boundary voxels using scheme train 8-headed 8-layer model using stochastic gradient descent momentum parameter 0.9 weight decay 1\textrm using cosine schedule warm-up learning rate 0.01 step decay learning rate total gradient step taken batch sample performance plateau around halfway model learnable positional encoding attention head embedding size feedforward size feedforward block consists two convolution window size along dimension separated gelu see fig visualization architecture notably model compact transformer consisting slightly million parameter also apply dropout recall transformer return predicted cubochoric coordinate voxel shape input masked slice contributes loss function nearest neighbor projection final step use output transformer continuous value assign voxel grain produce smoother slice fewer intra-grain variation prepare dictionary relating observed grain cubochoric coordinate first assign voxels whose neighbor voxels share face reside previous next slice observe voxels act like anchor provide neighbor voxels difficult classify empirically improves recovery next begin projecting voxels prioritizing one neighbor assigned include observed voxels previously projected voxels way first project voxels known information neighborhood first gradually build information obscure neighborhood projection determined minimum _2\ distance relevant cubochoric coordinate pulled dictionary based neighboring summarized fig projection algorithm essentially turn transformer output missing slice converted cubochoric coordinate euler angle example see bottom row fig figure nearest neighbor projection process output transformer assign voxels grain two dimensional toy example shown white box denote voxels projected yet input contains transformer output sandwiched observed adjacent slice first anchoring step assigns voxels whose neighbor adjacent slice grain voxels sequentially projected neighbor indicated small arrow starting voxels observed previously projected neighboring voxels bottom row showcase real projection example in625 dataset show target slice transformer prediction processed output via projection respectively capture voxel-wise _2\ difference projection full size image experiment method defined evaluate performance synthetic real ebsd data additional training fine tuning even though method overwhelmingly outperforms baseline term recovery also point weakness avenue improvement three baseline compare one k-nearest neighbor knn voxel determined vote based observed previously assigned among neighboring voxels exact process projection step including anchoring procedure except instead using distance metric voxels assigned vote system tie broken randomly another method currently employed simple solution missing slice copy adjacent slice replace missing slice usually maintains fairly decent accuracy since change slice slice minuscule compared number voxels copying previous next slice two baseline performance change slice slice captured boundary voxels define two different accuracy metric using denote accuracy voxel recovered slice overall accuracy accuracy considering boundary voxels recovered slice boundary accuracy latter pose greater challenge model consistently lower overall accuracy influence non-boundary voxels often dominates report mean accuracy standard deviation across validation sample performance one method heavily correlated performance others instance particular slice difficult recover one method likely difficult method better compare performance transformer baseline obtain difference accuracy sample namely find _\star _\star _\star accuracy metric ground truth _\star\ output _b\ transformer baseline respectively computing across sample result distribution accuracy improvement synthetic volume first generate independent synthetic volume shape setting volume divided nonoverlapping segment shape one five interior slice masked result validation sample setting validation metric displayed fig varying mean twin frequency fig varying mean grain size figure overall accuracy boundary accuracy overall accuracy improvement boundary accuracy improvement synthetic volume varying twin frequency error bar represent one standard deviation full size image figure overall accuracy boundary accuracy overall accuracy improvement boundary accuracy improvement synthetic volume varying mean grain size error bar represent one standard deviation full size image observe method accurately recovers missing slice baseline every synthetic validation sample since slice observed positive improvement overall boundary accuracy performance gain much apparent boundary voxels furthermore transformer performance much lower variance among baseline knn achieves closest accuracy method still underperforms comparison expected difference method baseline diminishes scenario get simpler larger grain size fewer twin since baseline already producing accurate result real ebsd datasets seek understand well model transfer real data running trained model in625 in718 volume subdivide volume nonoverlapping subvolumes slice height width voxels subvolume partitioned nonoverlapping segment shape representing single test sample sample contains one masked slice among five central slice end sample in625 sample in718 discarding small set sample whose missing slice boundary voxels sample would trivially result exact recovery regardless model use average accuracy accuracy improvement volume shown table fig respectively recovery example see fig table average overall accuracy boundary accuracy across sample real world datasets including standard deviation full size table figure overall boundary accuracy improvement in625 in718 test set note y-axes different scale full size image surprisingly even though model trained exclusively synthetic data observe transfer well real ebsd data still outperforms every baseline nearly every sample interquartile range positive difference accentuated boundary voxels test result in718 dataset much higher variance likely due slice proportionally fewer boundary voxels in625 slice qualitatively model stronger capability recovery thin feature knn visually tends ignore subtle structure figure four random example prediction missing slice in625 top two row in718 bottom two row test set row separate example contains target prediction made transformer knn previous slice following slice respectively full size image limitation method provides superior result compared baseline also seek understand circumstance may underperform one challenge rapid change slice make small minority test input instance -th slice missing set grain present k-1 -th different set grain k+1 -th slice model lot freedom decide one present degree missing slice case arise face grain twin perpendicular slicing direction however observe issue method using second row fig example bright green grain large crescent shape slice missing slice hardly present following slice model knn strives find middleground ultimately misclassify many voxels similar scene play fourth row fig thus future work includes designing better loss function mitigate error emphasizing scenario training another limitation arises projection method recall local projection method project voxel one observed previously assigned voxels order encourage grain connectedness mean long range dependency may ignored favor smooth structure furthermore connectedness guaranteed thin feature angle example edge case matrix illustrated fig though scenario fairly rare practice work could done improve projection algorithm particular use grain-averaged orientation directly impact design projection step transformer predicts real-valued orientation utilize projection step snap predicted value nearest grain-averaged value compare original training data moving orientation value grain-averaged require adaptation projection step potentially require modification underlying transformer architecture figure matrix example projection observed previously projected neighbor suboptimal even giving ground truth projection involves using neighboring slice predict center slice white square indicate pixel projected yet black arrow point possible neighbor pixel choose project top row show example projection operation smooth thin feature lie completely missing slice bottom row show example projection operation may disconnect grain full size image future direction also better integrate dynamic orientation data method example use mse loss operating cubochoric value mathematically rigorous metric representation initially attempted utilize angular metric loss encountered instability training believe related trigonometric function used loss computation immediate improvement current approach would operating orientation representation compute stable loss represents true angular difference representation space addition current data augmentation method borrowed computer vision technique particularly color shifting guarantee preservation misorientation relationship twin parent grain taking additional special consideration may enhance performance method conclusion presented novel method using transformer followed projection algorithm recover missing ebsd data vastly outperforms baseline notably even though model trained synthetic data still recovers accurate slice real ebsd data baseline wide margin making powerful data processing tool faulty ebsd reading furthermore model open possibility skip every fourth slice data collection using every skipped slice three collected slice side input model potentially reducing collection time future work involves addressing limitation scaling method considering general case altering projection algorithm apply consecutive missing slice within sample term scaling hope observe emergent behavior similar scaling llm training data seen vast improvement performance versatility beyond ebsd would interesting investigate method applicability high dimensional material science datasets