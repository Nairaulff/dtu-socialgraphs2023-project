introduction locomotion fish aquatic swimmer many desirable characteristic energy efficiency agility stealth inspired design many biomimetic robot design fish-like robot include assemblage rigid link actuated motor imitating motion tail fin motor-driven flexible link elongated snake eel like robot soft robot making use dielectric elastomer electroactive polymer fluidic elastomer actuator robot internal reaction wheel design small size fish-like robot resulting constraint actuation power require robot harness fluid structure fluid–structure interaction efficient agile motion control robot therefore requires modeling fluid–structure interaction determines force moment robot motion control robot either physical simulated high fidelity moving unstructured environment complex unmodelled governing physic traditionally presented many challenge low-fidelity model using simplified formula drag lift force lead model amenable control capture key physic play especially important role swimming small-scale robot limited actuation deep reinforcement learning drl hold considerable promise motion control robot complex dynamic swimming robot reinforcement learning particularly useful precise governing model absent reinforcement learning requires acquisition large amount data robotic system form state control action resulting state update data acquired either experiment field test simulation robot interaction environment case mobile robot experiment field test could prove expensive unsafe robot result simulation play important role reinforcement learning robotics simulation theory generate large amount data low cost exploring large subset state space challenging explore experiment true case robotic manipulator inverted pendulum toy system whose physic well understood efficiently quickly computed however governing physic complex state space high-dimensional possibly discontinuous dynamic high fidelity simulator often impractical significant model reduction use surrogate model essential control swimming robot important example governing physic complex enough reinforcement learning tool choice control dynamic fluid-structure interaction challenging simulate reinforcement learning widely applied many area robotics fluid control swimming robot part received much attention notable recent exception employed reinforcement learning task predicting efficient schooling configuration pair swimmer larger school gait generation efficient start escape pattern however important problem related mobile robotics station keeping velocity tracking disturbance path tracking addressed area fish-like swimming robot due computational challenge running large number high-fidelity simulation address challenge curriculum learning transfer learning context swimming robot observed applying reinforcement learning using deterministic policy gradient dpg algorithm complex task much computational time spent going initialized random policy intermediate policy sub-optimal qualitative similarity optimal policy going policy optimal policy often fast intermediate policy doe trained using high-fidelity state data instead cheaply trained using lower-fidelity simulation model transferred higher-fidelity environment complete final step training efficient multi-model training demonstrated fluid flow control extend swimming problem utilize knowledge physic fish-like swimming specifically two important qualitative feature swimming estimate intermediate policy finding optimal policy faster first feature fish-like propulsion enabled periodic tail beating carangiform fish body undulation anguiliform fish least steady state motion suitable reduced velocity space feature modelled limit cycle created periodic forcing second surprising feature swimming hydrofoil resembles cross-section fish-like body approximately modelled motion nonholonomically constrained system surprising feature arises kutta condition creates vorticity sharp corner body moving otherwise approximately inviscid fluid act nonholonomic constraint swimming hydrofoil swimming robot paper modeled free-swimming joukowski hydrofoil propelled steered internal reaction wheel particularly simple surrogate model emulates dynamic swimmer nonholonomic system known chaplygin sleigh periodic torque chaplygin sleigh produce figure-8 limit cycle velocity space similar structure observed limit cycle swimmer analytical approximation limit cycle function forcing frequency amplitude torque well inverse dynamic problem finding periodic torque generate limit cycle shown torque steering chaplygin sleigh path tracking chaplygin sleigh using vector pursuit method demonstrated since interested transfer learning using low-dimensional model chaplygin sleigh revisit problem simultaneous velocity tracking steering using reinforcement learning framework dpg agent trained generate action torque steer chaplygin sleigh parameter fit match dynamic swimming joukowski foil agent trained chaplygin sleigh model track limit cycle velocity space specified translational velocity dpg agent chaplygin sleigh transferred fluid simulation training steer hydrofoil track speed second training requires fewer simulation fine tune dpg agent precision tracking reference velocity step-by-step curriculum reinforcement learning circumvents problem high computational time simulate physic system allows way imprint qualitative physic dpg agent paper set forth framework using physics-informed surrogate model train dpg agent subsequently trained using data generated fast simulation fluid–robot interaction high fidelity computational method need used second step two reason going intermediate sub-optimal policy optimal policy slowed significantly importantly computation fluid–robot interaction intended another intermediate step actual experiment paper organized follows section nonholonomic constraint chaplygin sleigh joukowski foil inviscid fluid short review nonholonomic constraint particular reference chaplygin sleigh provided kutta condition joukowski foil shown formally similar constraint section periodic forcing limit cycle reduced velocity space limit cycle shown exist via simulation reduced velocity space chaplygin sleigh hydrofoil excited periodic torque make use panel method simulate motion hydrofoil section parameter estimation surrogate model select two set chaplygin sleigh parameter model swimming hydrofoil different translational velocity section reinforcement learning describe reinforcement learning framework curriculum enable path tracking chaplygin sleigh transfer skill simulated swimming hydrofoil nonholonomic constraint chaplygin sleigh joukowski foil inviscid fluid chaplygin sleigh chaplygin sleigh cart shown fig knife edge small inertia-less wheel rear point supported single castor wheel front allows motion direction sleigh also assumed internal reaction wheel whose angular acceleration apply torque sleigh configuration manifold physical system s^1\ rotor angle angular velocity coupled rest system eliminate rotor coordinate except torque reducing system parameterized locally denote position sleigh center mass denotes fixed frame angle relative horizontal generalized velocity tangent space denoted t_q ^3\ spanned combination generalized velocity standard basis 0,1,0 0,0,1 three basis vector respectively translation rotation spatial frame denoted _s\ x-y\ body frame collocated mass center rotated yaw angle respect spatial frame denoted _b\ x_b-y_b\ velocity spatial frame transforms velocity body frame ~\dot rotation matrix figure chaplygin sleigh shaped joukowski foil slip constraint transverse y_b\ direction internal reaction wheel shown grey circle joukowski foil singular distribution vorticity red circle corresponding positive counterclockwise vorticity blue circle corresponding negative clockwise vorticity otherwise inviscid flow full size image assume rear wheel prevents slipping transverse y_b direction roll freely longitudinal direction along x_b dim t_qq velocity constraint point given aligned -\sin aligned body frame term standard basis t_q velocity sleigh restricted lie subspace span\ ^\intercal -b\sin b\cos ^\intercal complementary subspace defined span\ -\sin physically mean allowable motion sleigh translate along longitudinal x_b direction spin velocity i.e fixed frame velocity center sleigh spin angular velocity sleigh constraint point doe translate center sleigh translate -b\sin b\cos distribution smooth assignment subspace tangent space w\in t_q nonholonomic closed jacobi-lie bracket vector field lie-bracket two vector field w_1\ w_2\ formally defined w_1 w_2 w_2 w_1 w_1 w_2\ setting vector w_1 w_2\ basis vector defining span respectively lie bracket yield w_1 w_2 -\sin ^\intercal showing constraint nonholonomic see detail nonholonomic constraint swimming inviscid fluid nonholonomic constraint locomotion body fluid easiest realize one considers motion body corner inviscid fluid one common example body relevant flight fish-like swimming motion joukowski foil whose geometry described mapping boundary circle radius r_c\ centered origin mapped plane joukowski transformation aligned a^2 +\zeta aligned geometric parameter refer plane foil motion foil plane plane circle motion circle plane symmetrical shape foil shown fig pre-image sharp trailing edge foil given a-\zeta _c\ assume fluid could contain singular distribution vorticity form point vortex shown fig motion fluid governed linear superposition potential function following milne-thomson complex potential describing velocity fluid body frame reference x_b-y_b\ see fig may decomposed term dependence translation foil rotation foil point vortex form aligned v_1 w_1 v_2 w_2 w_3 n=1 w^n_v aligned w_1\ w_2\ w_3\ rigid-body kirchoff potential function due translation foil x_b\ y_b\ direction rotation out-of-plane axis respectively potential function w^n_v due point vortex circulation _n\ located _n\ outside circular cylinder constructed according milne–thomson circle theorem term image vortex circulation -\gamma _n\ located inside cylinder r_c^2 thus w^n_v 2\pi -\frac r_c^2 image vortex inside cylinder introduces net circulation around cylinder consistent kelvin circulation theorem development net circulation around foil introduces lift force foil essential propulsion complex velocity fluid foil plane body fixed frame related complex velocity fluid circle plane equation aligned d\zeta aligned a^2 +\zeta since fluid modeled inviscid boundary condition body foil allow fluid slip along surface additional constraint velocity fluid necessitated geometry foil pre-image trailing edge foil singularity joukowski transformation i.e. derivative =0\ seen complex velocity fluid circle plane zero ensure velocity fluid trailing edge foil foil plane doe become undefined kutta condition kutta condition requires pre-image _t\ trailing edge aligned d\zeta v_1\frac dw_1 d\zeta v_2\frac dw_2 d\zeta +\omega dw_3 d\zeta n=n dw_v^n d\zeta aligned formal calculation show dw_1 d\zeta dw_2 d\zeta -2\ term dw_3 d\zeta subsequently denoted -b\ constant determined numerical value parameter _c\ denoting velocity fluid trailing edge due distribution point vortex u_v n=n dw_v^n d\zeta kutta condition re-written aligned -\dot u_v aligned equation constrains velocity foil affine-nonholonomic constraint condition constrains velocity system phase space affine distribution form w-w_0 w_e w_0 -u_v\sin u_v\cos ^\intercal w_e span v^1 subspace containing allowable vector field w_1\ w_2\ associated motion foil vector field v^1 velocity point vortex affine-distribution simply cartesian product configuration manifold vortex vector field v^1 represent translational velocity vortex potentially dependent compatible constraint position orientation foil fixed periodic forcing limit cycle reduced velocity space limit cycle reduced velocity space chaplygin sleigh equation motion chaplygin sleigh calculated straightforward manner assume chaplygin sleigh experience viscous resistance move ground actuated periodic torque generated periodic oscillation reaction wheel lagrangian system i_c\dot ^2\ mass sleigh moment inertia coordinate center mass assuming viscous resistive force motion described rayleigh dissipation function =\frac +\dot c_u\ viscous damping coefficient translational rotational velocity respectively euler–lagrange equation aligned -\frac q^i _j-\frac +\tau aligned -\sin obtained complementary space _j\ lagrange multiplier constraint external force torque acting sleigh varies j=1\ _1\ nonzero consequently denoted henceforth equation transformed body frame using ~\dot obtain dimensionless reduced velocity equation aligned ^2-\frac c_u aligned aligned u-c_ i+m b^2 aligned u=\dot +\dot translational velocity constraint due nonholonomic constraint evolution velocity governed two equation instead three evolution configuration variable given aligned aligned aligned u\cos b\sin aligned aligned u\sin b\cos aligned torque due reaction wheel periodic limit cycle exists reduced velocity space sleigh move along serpentine path time averaged path converging straight line illustrated sample result fig figure left sample serpentine trajectory chaplygin sleigh mean converges straight line right reduced velocity space trajectory converges figure-8 limit cycle sample trajectory simulated swimmer starting rest forced periodic torque inset figure show convergence limit cycle reduced velocity space similar chaplygin sleigh indicating similar underlying dynamic velocity scaled body length per second bl/s swimmer move along serpentine path black average path converging straight line full size image limit cycle reduced velocity space hydrofoil consider hydrofoil modeled naca symmetrical airfoil contains internal reaction wheel moment inertia i_r\ angular velocity _r\ oscillatory motion reaction wheel generates periodic torque hydrofoil given -i_r\dot _r\ simulate motion due periodic torque using vortex panel method panel method form computational fluid dynamic cfd relies entirely potential flow theory point line vortex method boundary structure surface decomposed discrete panel source vortex distribution panel flow doe pas surface lack viscous effect reduce simulation fidelity compared modern meshed navier–stokes solver panel method lower computational cost easily incorporate body movement due lack meshing reason panel method continue preferred simulation tool high reynolds number fluid interaction problem flow largely dominated inertial effect easily captured panel method neglected viscous effect comparatively insignificant includes dynamic swimming large and/or fast swimmer experimentally validated panel method long used quantifying efficiency flipper shape flapping airfoil kinematics explaining flow dynamic full fish model though decreasing cost computing power lead increasing use meshed finite volume technique swimming problem new panel method still development still widely used swimming problem including anguilliform cetacean swimming vortex panel method panel source distribution strength _i\ varies panel vortex distribution strength constant body neumann boundary condition applied stipulates flow pass panel midpoint u_i u_i\ flow velocity vector center panel relative body _i\ surface normal unit vector control point kutta condition enforces condition static pressure continuous two panel midpoint adjacent tail i.e. p_1=p_n\ enforce condition vortex shedding occurs tail calculating change circulation body every time step applying opposite circulation wake panel tail shed point vortex equal circulation center structure system allows solved independently flow velocity due specific source vortex panel linear unknown strength unknown _i\ value found solving linear system equation arising neumann boundary condition _i\ found flow field fully determined pressure distribution around body computed unsteady bernoulli equation implemented calculating velocity potential point body relative leading edge via path integral fluid velocity along body neglecting circulation time derivative potential calculated finite difference moving potential reference point result value varies time equal across body added pressure body closure value effect calculated force moment pressure resultant torque force acting body computed additional linear dissipation force applied body degree freedom linearly proportionally velocity prevents drag-free gliding would otherwise possible without skin friction snapshot sample simulation motion vortex wake due periodic torque joukowski foil shown fig panel code validated steady flow comparing computed lift coefficient known experimental value range angle attack also validated unsteady flow compared rotor-driven swimming hydrofoil experiment found swimming model result similar trajectory experimental system parameter estimation surrogate model similarity limit cycle chaplygin sleigh hydrofoil trajectory plane response periodic control input torque together similar nonholonomic constraint system motivates use chaplygin sleigh surrogate model swimming foil limit cycle chaplygin sleigh reduced velocity space depend parameter c_u accurate surrogate model parameter need chosen resultant limit cycle due periodic torque nearly mean value amplitude hydrofoil accomplished first gathering data simulating motion hydrofoil simulation second long data acquired time increment dt=0.1\ forcing two different model fit one a=1.0\ another a=1.2\ sensitivity result forcing amplitude used fit surrogate model explored surrogate model fit simulation using lower forcing amplitude resulting smaller u_0 0.606\ swimmer referred low speed surrogate model latter model fit simulation using higher forcing amplitude higher u_0 1.90\ referred high speed surrogate model time step current state action next state stored buffer optimization routine performed minimize least square error aligned p_0 _b\left ^2-\frac i+m b^2 aligned figure show trajectory swimmer blue surrogate model red obtained low speed model left high speed model right case surrogate model trajectory reduced velocity space converge limit cycle qualitatively similar swimmer pair limit cycle left apparent mismatch limit cycle largely due error 10\ mean value speed u_0\ evident fig show evolution red low speed surrogate model evolution state variable hydrofoil also shown fig blue error angular velocity derivative negligible hydrofoil nonholonomic surrogate model error small value 0.021 low velocity surrogate result error mean translational velocity u_0\ 0.066 error u_0\ higher velocity surrogate model significantly lower 3.11 10^ figure limit cycle surrogate chaplygin sleigh red swimmer blue periodic forcing demonstrating convergence similar limit cycle trajectory reduced velocity space two set limit cycle shown one due applied periodic torque =\sin limit cycle left due =1.2 limit cycle right vector field governing equation surrogate chaplygin sleigh red swimmer blue lower velocity unit shown simulated swimmer chaplygin sleigh state dimensionless full size image note surrogate modeling fit parameter three degree freedom rigid body model approximate complex high-dimensional interaction vortex wake hydrofoil therefore mapping dynamic swimmer chaplygin sleigh unique surrogate model mapped non-uniquely dynamic different swimmer example low speed surrogate model dimensionless parameter value found m=0.93\ i=0.98\ b=0.068\ c_u=0.041\ =0.0043\ high speed surrogate model corresponding parameter m=0.36\ i=0.93\ b=0.17\ c_u=0.026\ =0.058\ moreover error dynamic surrogate model true swimmer model increase changing velocity forcing use surrogate model subsequent curriculum learning demonstrate qualitative similarity physic mainly efficient motion lie invariant manifold important training quantitative similarity two surrogate model quantitatively dissimilar parameter reinforcement learning control fish robot challenging due high complexity fluid–body interaction need periodic motion popular current approach based central pattern generator cpg neural assembly found vertebrate periodic output provide rhythm periodic locomotion artificial attempt reproduce functionality often determine deflection actuator oscillator parameter determine amplitude frequency relative phase actuator doe typically result swimming behavior exact oscillator parameter typically chosen heuristically though formal parameter optimization result large improvement swimming speed must performed algorithm particle-swarm optimization select parameter optimize reward entire trajectory open-loop fashion reinforcement learning emerged efficient way optimize controller parameter pure trajectory optimization utilize knowledge intermediate state within trajectory improve controller policy provided control action function state controller parameter teach controller perform oscillation based feedback using curriculum instead encoding oscillator directly controller architecture allows parameter optimization performed comparatively trajectory simulated swimmer apply deterministic policy gradient dpg algorithm curriculum learning control foil panel method simulated swimming environment track reference path reference velocity path tracking algorithm decomposed two part pure pursuit algorithm determines target turning angle given path geometry dpg-trained actor performs specified turn desired velocity pure pursuit algorithm simpler two given sequential list point defining path list vector spanning center pursuer point constructed target point x_i y_i vector selected initialized every time interval vector reconstructed based current pursuer position constant specifies sight horizon increased iteratively inequality becomes false vector taken pursuit vector angle fixed frame recorded target local frame error current desired heading defined =\theta_ target -\theta limit continuous path tracking error zero pursuit algorithm implemented path tracking chaplygin sleigh fedonyuk trajectory chaplygin sleigh plane torque periodic serpentine time averaged trajectory converges straight line sight horizon small i.e u_r reference velocity u_r\ stroke frequency pursuit vector target oscillate rapidly direct body back path interferes agent derived stroke frequency contrast control method chasing distant point tends smooth small-scale feature path minimize oscillation trajectory desired path sharply curved track chord cutting required path take d=10\ body length u_r speed considered find appropriately small capture detail path considered small enough interfere significantly frequency oscillation curriculum learning curriculum learning three training step supervised step pre-training policy model given control surrogate chaplygin sleigh dpg training step optimally control surrogate chaplygin sleigh model third step transferring trained model fluid–robot simulation environment training supervised step pre-training sleigh model given control chaplygin sleigh imprint actor known control algorithm initial policy exploration enable actor learn policy reward function control goal seen result fig following section first supervised step inspired previous work control surrogate chaplygin sleigh velocity tracking purely sinusoidal input steering purely proportional control investigated initializing actor perform superposition control method arbitrarily chosen constant better starting point fully random actor initial control function selected aligned =0.3 -0.05 aligned constant target converges limit cycle u_0 0.32\ initial condition neighborhood zero pre-training control function arbitrary target velocity within pre-defined feasible range u_0 max could achieved without affecting subsequent algorithm pre-training generates deterministic policy s|\theta weight explicit time dependence generates similar periodic gait generated time-dependent prescribed forcing achieved collecting state action vector trajectory using latter time-dependent policy finding _1\ network architecture corresponding weight explained section deterministic policy gradient tracking limit cycle heading angle chaplygin sleigh minimize sum error a-\mu s|\theta ^2\ point sampled trajectory using stochastic gradient descent chosen trajectory length t=100\ second interval dt=0.1\ optimization done epoch prevents fitting particularly important data used one trajectory schematic pre-training procedure shown fig figure schematic pre-training encode limit cycle feature reduced velocity space periodic gait policy output actor schematic illustration application modified dpg algorithm train policy make surrogate chaplygin sleigh track limit cycle heading angle full size image deterministic policy gradient tracking limit cycle heading angle chaplygin sleigh knowing prescribing target velocity optimal control problem reduced regulation problem find policy target maximize cumulative reward aligned r=- u-u_t ^2-0.2 -0.1 _e| aligned otherwise denoted aligned t_0 t_f aligned regulating u_t\ policy control torque state feedback though u_t\ strictly state system doe dynamic except prescribed variable speed tracking instance must relayed policy together state allow appropriate control perspective dpg algorithm state henceforth refer reward function designed simultaneously minimize error angle velocity tracking mutually exclusive goal maintaining target velocity requires flapping motion result periodic deviation desired heading angle l^2\ norm traditionally used cost function regularization problem regression penalizes large deviation target value much harshly small one make impossible select weight angle tracking reward adequately penalize small bias penalizing flapping oscillation much would result swimming speed well velocity target however allowing small bias exist tracking angle cause swimmer deviate prescribed path time problem alleviated introducing l^1\ term angle tracking error penalize small bias similarly l^1\ term included velocity tracking reward reduce small negative bias velocity result oscillation magnitude-velocity tradeoff specific weight tuned surrogate model repeated trial approach similar problem include using analytical harmonic balance calculation solve inverse problem finding biased sinusoidal input capable reaching desired limit cycle chaplygin sleigh extension case hydrofoil tracking limit cycle shown approach extended path tracking tracking variable velocity approach successfully proportional control heading angle clear way expand approach simultaneous velocity tracking absence standard control method even path tracking problem chaplygin sleigh associated difficulty carrying similar approach swimmer consider problem well suited reinforcement learning since action space allowable control continuous dpg ability utilize entire action continuum make suitable problem dpg implementation explained graphically fig explained text denote u_t define policy neural network weight reward maximization problem approached sufficiently general numerical optimization algorithm perturbing weight recalculating trajectory associated cumulative reward monte–carlo approach typically arrive maximum requires many trajectory iteration extensive training process combined take long time achieve convergence optimal policy policy gradient method make much faster also calculating value given state-action pair using critic network quantifying value action context specific state allows specific update policy perform higher-value action requires fewer trajectory monte–carlo approach one cumulative reward per simulation many action generating data allows finer training simple term effective promote curtail specific action promote curtail set weight based value average action traditional policy gradient algorithm policy probability distribution estimate appropriate action discrete pre-determined set effective many benchmark problem feature discrete often small set action commonly game discrete on/off control limitation continuous spectrum allowable action available dpg ability utilize entire action continuum make intuitive choice problem formally traditional dpg algorithm feature two function approximators actor critic _1\ _2\ weight define approximation approximators used algorithm typically neural network due utility universal function approximators ill-suitedness competing discrete approximators table continuous state action space weight typically initialized gaussian distribution empty experience buffer also generated use greedy exploratory policy sampled 1-\epsilon chance returning probability returning random value pulled normal distribution deviation environment simulated result stored experience buffer form next state transitioned discounted expected future reward updated equal sum current reward discounted approximated future reward known next state process known bootstrapping written aligned target =r+\gamma aligned discount factor new set action selected maximize expected reward performing one step gradient ascent aligned target =\mu +\alpha aligned gradient computed efficiently along one dimension finite difference approximation aligned a+d a-d aligned gradient also computed automatic differentiation efficient finite difference method higher-dimension action space updating network target target supervised learning problem minimize least-square-error using epoch adam algorithm stochastic gradient descent algorithm momentum supervised approach departure original algorithm instead update set weight single step gradient descent find higher optimization speed adam algorithm compared deterministic gradient descent make supervised approach faster problem use adam parameter recommended page except =10^ update repeated time new batch environment simulation generated appended experience buffer ensuring enough data perturbation local current trajectory allow local optimization selected initial parameter value n=10\ m=10\ chaplygin sleigh training n=10\ m=1\ fluid simulation training additionally used value =0.01\ =0.99\ =3\ a=10^ experience buffer size 10^5\ training progress value u_t\ simulation drawn widening probability distribution first training performed u_t=1\ iteration target velocity trajectory drawn uniform distribution 0.8 u_t 1.2\ target velocity held constant within trajectory broadened 0.6 u_t 1.6\ 0.4 u_t 2.0\ 0.2 u_t 3.0\ iteration respectively gradual introduction different target velocity serf another layer curriculum reduces risk non-convergence one modification made dpg algorithm described thus far critic initialized random weight actor pre-trained actor update disabled defining =0\ first iteration point critic grown consistent actor learn rate reset =0.01\ episode reward sleigh environment qualitatively seen reach peak asymptotic convergence solution actor critic transplanted fluid simulation state used policy input remain however chaplygin sleigh correspond full state feedback simulated swimmer limited state feedback insufficient even fully describe motion rigid body let alone high-dimensional fluid lack observability make training swimming simulation data expensive underlying markov assumption every state-action pair fixed probability distribution state transition accurate though overall system markov complete state-action pair doe transition deterministically specific next state state given policy small subspace true state appears system non-markov perspective agent reduced data efficiency combined increased computational cost generating data make training slow term wall-time dense neural network popular choice function approximator dpg dense network contains one layer neuron value neuron computed weighted sum neuron previous layer different weight neuron value passed activation function passed next layer process repeated reach neuron read output selected 6-layer dense neural network featuring input layer size actor size critic layer neuron rectified linear unit relu activation single output node linear activation policy function =\tau used smoothly attenuate prescribed output bounded torque value improves convergence select _m=4\ range allowed forcing find large enough perform effective control small enough cause numerical problem fluid simulation reinforcement learning in-house implementation dpg python tensorflow accessed kera api neural network implemented kera supervised update actor critic network performed model fit kera supervised learning utility batch state transition experience buffer environment implemented python state transition saved interval 0.1\ sleigh simulation trained -greedy exploration strategy =0.2\ fluid simulation instead follows gaussian exploration approach normally distributed noise deviation 0.2 added every action exploration reduces large discontinuity torque seen -greedy exploration turn cause large magnitude shed vorticity result unrealistic wake result velocity path tracking supervised step pre-training sleigh model given control chaplygin sleigh imprint actor known control algorithm starting policy exploration enable faster learning policy reward function control goal figure show reward training low speed surrogate model pre-trained output periodic control action given red curve show reward agent learning policy track high speed u_t 2.5 blue graph tracking low speed u_t=0.8 figure show reward function training actor without pre-training sub-optimal policy instead using actor network weight initialized gaussian noise figure total reward epoch training low-speed chaplygin sleigh surrogate model pre-trained actor actor without pre-training red blue line show reward learning policy track high speed u_t=2.5\ versus low speed u_t=0.8\ respectively limit cycle resulting policy learned surrogate model target velocity 0.8 1.5 2.5 blue black red respectively well policy training green reward function policy case full size image pre-training torque amplitude 0.3 torque required track low high speed velocity 1.2 respectively despite pre-training suboptimal policy pre-trained actor learns faster better reward converging higher value -350\ blue lower speed reward agent without pre-training converges -800\ also taking epoch limit cycle generated reduced velocity space result trained policy shown three different target velocity 0.8 1.5 2.5 fig along limit cycle produced pre-training u_0 0.32\ seen actor adapts generating quazi-periodic trajectory generating consistent limit cycle near desired velocity also becomes efficient converting rotation translation blue trained green untrained trajectory similar maximum amplitude trained trajectory achieves higher velocity mean value velocity limit cycle shown fig error doe decay due fact reward function sum reward accrued minimizing error u-u_ well error serpentine motion sleigh result necessary non-zero error heading angle reduced smaller amplitude torque slows sleigh learned policy reward shown fig compromise achieving minimizing error heading angle next stage curriculum learning agent trained surrogate chaplygin sleigh transferred train fluid-hydrofoil simulation environment using panel method velocity heading angle tracking problem aim improving sub optimal policy surrogate sleigh actor total trajectory swimmer dpg iteration used adapt policy chaplygin sleigh swimmer rate 16.1 min per dpg update 5.49 min per trajectory generated intel xeon gold 6148f processor comparison batch chaplygin sleigh trajectory generated 6.1 result training time curriculum much smaller would usually required swimmer directly trained without intermediate training surrogate chaplygin sleigh model figure show reward function epoch using low speed surrogate sleigh model track high red speed u_t 2.5\ low blue speed u_t 0.8\ figure trajectory hydrofoil training low speed surrogate model actor reward function epoch training fluid-hydrofoil simulation reward executing optimal policy tracking velocity heading angle starting rest trajectory reduced velocity space due optimal policy actor trained surrogate model produced optimal policy actor trained additional fluid-hydrofoil simulation velocity tracking hydrofoil two tracking velocity simultaneously tracking heading angle color legend—tracking speed u_t 0.8\ blue u_t 1.5\ black u_t 2.5\ red full size image reward beginning first epoch due optimal policy learned surrogate sleigh reward training fluid–hydrofoil interaction simulation doe change significantly however seeming lack learning deceptive borne examining velocity hydrofoil figure show trajectory reduced velocity space ignoring second transient solution swimmer produced optimal policy actor trained surrogate sleigh model fig show trajectory swimmer produced optimal policy actor trained fluid–hydrofoil simulation trajectory reduced velocity space visit larger range value training fluid simulation velocity swimmer doe converge target velocity doe oscillate mean value close target velocity target velocity high red shown fig strategy forgoes compromise oscillation angle velocity seen chaplygin sleigh training instead feature interval high-torque low-reward period acceleration followed high-reward period low-torque coasting low-frequency periodic component velocity pitch angle high-velocity tracking swimming strategy shown fig respectively style burst-and-coast swimming frequently observed fish consistent result swimming literature strategy doe emerge training surrogate model doe emerge high swimming velocity transfer swimming simulation even continuing training continued training fluid simulation expands range target velocity burst-and-coast strategy used similar result seen policy trained high speed surrogate model trained using fluid-hydrofoil simulation learn policy track different velocity heading angle reward function training doe increase significantly fact decrease seen fig picking optimal policy using actor produce highest epoch reward described section deterministic policy gradient tracking limit cycle heading angle chaplygin sleigh produce reward shown fig burst coast technique seen fig tracking high velocity red steady error seen tracking lower velocity blue figure training high-speed surrogate model actor reward function epoch training fluid-hydrofoil simulation reward executing optimal policy tracking velocity heading angle starting rest velocity tracking hydrofoil two tracking velocity simultaneously tracking heading angle color legend—tracking speed u_t 0.8\ blue u_t 2.5\ red full size image emphasized lack significant increase reward function training fluid-hydrofoil simulation compared reward obtained optimal policy surrogate sleigh actor failure training attributed two reason first reason reward function consists sum term individually seek minimize error tracking velocity heading angle oscillatory nature motion resulting oscillating heading angle guaranteeing reward always negative second reason subtle state used training actor swimming merely subset kinematic variable swimmer include distribution vorticity fluid kinematics swimmer nearly two different distribution vortex field moreover action torque given state produce transition completely different state depend distribution vortex field since vortex field observed variable state training state-action pair constant probability distribution making system seemingly non markov perspective agent combination surrogate model curriculum learning outperforms direct reinforcement learning policy using fluid-hydrofoil simulation tracking velocity heading angle figure show learning curve direct training fig b–d show trajectory generated trained policy training iteration reward converges lower value seen fig reward per step due optimal policy -3\ shown also lower reward due optimal policy seen fig value -1\ per step result poor velocity angle tracking consequently graph show large error target velocity heading angle figure without surrogate model curriculum learning reward function epoch training reward function executing optimal policy hydrofoil starting rest tracking low blue speed high speed red simultaneously tracking heading angle case reward lower trained curriculum largely due higher velocity error swimmer tasked reaching low target speed instead coasting near-stop full size image last step path tracking give time dependent determined pure pursuit algorithm input amongst state dpg agent trained fluid–robot simulation environment show result three simulation agent tracking path also simultaneously tracking specified time-varying velocity figure show swimming hydrofoil tracking straight line sinusoidal path circle case reference velocity hydrofoil shown fig va–vc reference velocity piecewise constant either increase decrease midway simulation time action torque due policy attenuation inverse tangent case shown fig ta–tc control torque sinusoidal pre-training significant amplitude modulation multiple harmonic result series training different model curriculum fine tuned policy burst-and-coast strategy figure pure-pursuit based path tracking simulated swimmer straight line sinusoidal path circle target velocity case straight line sinusoidal path circle shown dashed line respectively torque generated track straight line sinusoidal path circle shown respectively full size image conclusion reinforcement learning method mobile robotics particularly swimming robot complex unmodelled physic require large amount data covering large subset relevant state space expensive time consuming obtain simulation experiment result paper show utility surrogate model curriculum-based reinforcement learning wherein dpg agent trained step control planar swimming robot step agent learns generate control certain feature dynamic robot-environment action agent trained perform series increasingly complex task tracking limit cycle reduced velocity space surrogate model tracking heading angle speed surrogate model finally transferred learn task fluid–robot interaction environment approach reduces computational time importantly creates physics-informed reinforcement learning framework proof concept demonstrated limited range curriculum dpg agent trained instance path tracking result fig policy trained constant value target compensates variable target value prescribed pure pursuit algorithm higher torque value resulting higher velocity deviation reference velocity improvement possible including variable target case curriculum robot considered paper planar swimming oscillating hydrofoil fish-like robot trained using similar framework one proposed paper fish-like robot usually one body segment skeleton joint could elastic recent work example show robot elastic joint tunable stiffness enables faster efficient swimming surprisingly analogous result exist ground-based multi-segment nonholonomic system see example effective stiffness tunable periodic forcing lead different limit cycle varying efficiency multistable configuration fast turning nonholonomic system used surrogate model multi-body fish-like robot using curriculum learning framework low-latency velocity feedback either inertially body external measurement actor trained fluid surrogate model directly transplanted physical system new experience buffer generated training continued physical system little required setup approach extended three-dimensional swimming suitable surrogate model three dimension additional control goal stabilization roll pitch may exist simultaneously tracking path and/or velocity simulation fluid–robot interaction acquiring large experimental data set case even challenging necessitating physics-based reinforcement learning surrogate model current paper describes preliminary proof concept framework