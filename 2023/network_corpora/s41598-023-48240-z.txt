introduction frequent challenge social science epidemiology study sample representative entire population study based biased sample suitable drawing conclusion lead biased inference social process every phase research project e.g study design data collection data analysis induce bias dichotomous exist various intensity crucial researcher validate presence bias case existence mitigate much possible currectly analyze interpret finding example bias survey conducted specific city although goal generalize result whole country online survey depends individual user participation people strong opinion topic higher likelihood participate survey leading self-selection bias however desirable draw conclusion user resident conclusion drawn without bias correction misleading description population produced ultimately false conclusion drawn bias completely removed without gathering data bias reduction method strive mitigate bias much possible reducing bias lead consistent finding therefore saving time resource fewer repeated study required article proposes bias reduction method based positive unlabeled training learning combination auxiliary information population data take advantage available representative population data set rigorously collected prevent potential bias representing target population high precision call method maximal representative subsampling representative distribution used remove element biased data set create maximal representative subsample resembles representative distribution figure show structure method figure overview maximal representative subsampling gutenberg brain study gesis survey conducted german population contrary gesis conducted representative auxiliary information gesis linked help machine learning algorithm detect mitigate bias algorithm output subsample reduced bias call maximal representative subsample data set visualization differ size represent number sample contain full size image detail central aspect train random forest differentiate instance biased data set representative data set instance biased dataset trained classifier predicts probability instance belonging either two datasets predicted probability used choose element removed biased data set make data set indistinguishable element high probability thus overrepresented significantly different removed training sampling removal repeated classifier differentiate data set anymore classifier capable enough assumed generated subsample representative aim generated subsample evaluate research question required level significance necessary keep removal instance minimum pruning data increase uncertainty lower statistical power decreasing insight downstream analysis especially harmful initial data set already small method make data set indistinguishable classifier also decrease maximum mean discrepancy mmd distribution furthermore variable contained representative data set therefore used training shown tend approach representative ratio well training variable dependent variable correlate typical use case arises data acquisition concluded bias identified late support additional data collection effort situation researcher would begin obtaining representative data set accurately reflects study population biased representative data set overlap significantly term variable mitigate bias prevent introduction artificial bias optimal case representative data set encompasses independent variable thereby increasing likelihood distribution aligning representative one potential source acquiring representative data set include data archive census market research institute specializing opinion analysis tested effectiveness gutenberg brain study population-based study main aspect study measure resilience influence voting behavior participant randomly chosen invited participate survey downside survey carried one city representative demographic country possible source selection self-selection bias additionally representative data german population accessed leibniz institute social science allensbach institute public opinion research allensbach representative data set employed reduce bias two separate experiment subsequently experiment conducted census income assumed representative population united state illustrate deal larger data set artificial bias induced allows gain insight debiasing process paper structured follows first give detailed overview proposed method followed result discussion reduce bias twice information two different representative data set analyze change time afterward test long-term behavior influence variable included representative data set use large public census data set introduced artificial bias compare bias reduction method generate sample weight respect quality bias reduction usage sample weight downstream task paper end outline applying statistical task summary present article contains following contribution propose two new approach soft-mrs reduce selection bias develop machine learning method increase representativeness data set iteratively adapting weight using auxiliary information representative reference data set validate application sample weight downstream classification task demonstrate use representative subsample fit statistical model related work bias reduction algorithm exist great variety rely different type additional auxiliary information class probability computing sampling distribution using make appropriate correction technique cell weighting raking use auxiliary information compute sample weight used mainly reduce non-response non-coverage bias propensity score adjustment psa classifier trained estimate participation propensity individual non-representative data set commonly used method psa logistic regression recent research showed different machine learning method advantageous estimating propensity score point yet clear algorithm preferred different distribution class distribution rarely known distribution estimation complex task high-dimensional space recent method perform distribution matching training testing set without density estimation kernel mean matching kmm non-parametric method directly produce sampling weight without distribution estimation reweighting sample mean training test sample reproducing kernel hilbert space close author showed kmm improves learning performance compared training unweighted data case even better true sample bias distribution another method kullback-leibler importance estimation procedure kliep also directly estimate importance sample without density estimation implement natural model selection procedure kliep find importance estimate kullback-leibler divergence true test input density estimate minimized key advantage kmm training sample cross-validated test input sample allows accuracy model selection improved number training sample typically limited test input sample readily available existing method estimate sample weight machine learning algorithm iteratively optimize sample weight none combine discriminative machine learning method iterative refinement developed method match distribution without explicit density estimation machine learning method allows model complex dependency approach fine-tune sample weight iteratively methodology maximal representative subsampling section describes detail component mrs. based learning train semi-supervised binary classifier positive unlabeled data used task matrix completion multi-view learning furthermore classify data stream time series detect event like co-occurrence graph learning requires positive set mixed set contains positive negative sample purpose train classifier classify test set label unknown positive negative sample training element treated element negative class adapt learning experiment using representative data set denoting one part biased data set unlabeled set remaining sample classifier assigns element probability value element representative non-representative data set probability used remove element biased data set specified stopping criterion fulfilled stop current sub-sample assumed representative returned confirm make data set indistinguishable also increase distribution similarity mmd measure distance two distribution used mmd computed norm difference distribution feature mean reproducing kernel hilbert space given two set sample x_i\ i=1 ^n\ y=\ y_i\ i=1 ^m\ one compute empirical estimate mmd following way aligned aligned mmd m^2 j=1 x_i x_j j=1 x_i y_j n^2 j=1 y_i y_j aligned aligned radial basis function kernel aligned aligned x-y\vert 2\sigma aligned aligned similarly gretton determine heuristically setting mean distance sample aggregated sample contains sample data set pseudocode given algorithm input representative data set non-representative data set number element get dropped every iteration number element dropped every iteration trade-off accuracy computing time sample dropped per iteration faster method reduces number iteration dropping sample result fewer retrained classifier thus potentially reducing accuracy runtime furthermore depends size representative data set size non-representative data set type classifier iteration outer loop line 2–13 instance removed biased data set classifier distinguish measured area receiver operating characteristic curve auroc current algorithm stop auroc -0.5| =0.001\ experiment iteration classifier trained calculate current auroc test stopping criterion algorithm maximal representative subsampling full size image optimizing hyperparameter requires either run completely different configuration perform nested cross-validation iteration approach make method computational intensive would lower acceptance soft-mrs hand doe use cross-validation doe need train additional classifier test stopping criterion requires one optimization per iteration determine instance removed obtain probability instance 5-fold stratified cross-validation line 3–10 every round cross-validation classifier optimized trained data set predict probability hold-out set line 7–8 probability aggregated hold-out set line returned use classifier predicts probability selected random forest intuitive fast train standard hardware trained random forest predicts probability belonging either positive negative class holdout set weight element line highest probability set remove line removing new auroc computed verify stopping criterion experiment stopping criterion auroc decision tree minimal cost-complexity pruning stopping criterion met current sample weight vector returned otherwise algorithm continues next iteration ablation study included supplementary material soft maximal representative subsampling removing sample restrictive smaller data set potentially reducing size much undermining analysis response concern developed soft variant soft-mrs instead discarding sample entirely i.e setting weight zero soft-mrs take different approach calculating adjusting sample weight soft-mrs algorithm start first setting sample weight uniform distribution line sample weight iteratively adjusted data set indistinguishable outlined line 3–16 experiment used mmd measure difference data set validate stopping criterion update loop continues mmd longer decrease employing patience iteration objective adjust sample weight order minimize distance data set algorithm soft maximal representative subsampling full size image weight instance updated multiplicative update see line pseudocode explanation style reminiscent adaboost additive regression classifier kept simple compared effectively single unpruned decision tree inhibited growth minimum weight fraction per leaf per iteration required make scheme work make training classifier factor number test set cross-validation number tree random forest faster training classifier mrs. additionally one save time mmd stopping criterion versus cross-validated cost-complexity pruned tree saving running time invested optimization hyperparameters shown line 5-13 pseudocode therefore following step performed iteration update weight line 5–14 hyperparameter configuration decision tree classifier trained distinguish representative non-representative data set predict probability line 6–7 weight update _i\ sample depends predicted probability p_i\ calculated linear function 1.5 p_i\ line 9–10 aim sample predicted high confidence belong representative sample obtain higher sample weight one high confidence belonging non-representative sample lower sample weight predicted probability decrease weight contrary prediction increase weight change bigger predicted probability deviate 0.5 decrease difference get smaller approaching keeping weight constant updated weight selected greedily choosing set sample weight lead lowest mmd value line stopping criterion met weight current iteration applied downstream task line discussion deriving knowledge biased data set problematic hence desirable reduce potential bias source gathering process sometimes researcher detect bias late already impossible gather additional data case preferred course action employ bias mitigation method existing debiasing method based either successive optimization procedure discriminative model method combine paradigm addressing gap developed two method soft-mrs iteratively adapt sample weight machine learning employ learning remove discriminating sample soft-mrs performs multiplicative update sample weight providing alternative approach bias reduction iterative adaptation weight method contributes creation unbiased representative sample initially biased datasets benefit method follows uniform sample weight introduces advantage assigning equal weight remaining sample equal contribution simplifies subsample interpretation mitigates risk overemphasizing sample underrepresented group contrast weighting method sample receive low weight still used ensures reduced effective sample size potentially benefiting downstream task term running time interpretability e.g case instance-based method also advantage resulting sample directly used input algorithm incorporate sample weight without additional sampling step bias reduction soft-mrs achieved lowest mean rank term mmd relative bias reduction reduction reliant auroc stopping criterion could improved adapting use mmd however adjustment would shrink dataset diminishing statistical power auroc stagnates around 0.5 mmd stop decreasing necessitating removal sample improvement downstream classification although statistical significance may always observed application downstream classification consistently show competitive result case underscoring potential enhance classification task biased datasets adaptability whereas previous point concern end user method final point concern method developer wish adjust develop soft-mrs provide versatile modular machine-learning based framework bias reduction allowing adaptation various use case potential adjustment include selecting different classifier modifying stopping criterion one could explore alternative method determine sample remove soft-mrs alternative weight update function adaptability specific use case render variant well-suited wide range task however also inspire research involving different module combination downstream task conclusion introduced method soft-mrs present valuable tool researcher practitioner addressing bias data set adaptability demonstrated effectiveness bias reduction comparability existing method position versatile approach wide range application ongoing exploration optimal use case continuous experimentation refine extend utility method tackling bias real-world scenario result statistical correction gutenberg brain study first experiment performed bias statistical correction gutenberg brain study population-based sample healthy adult aged year live city located southwest germany mainz main research question resilience influence political participation acquisition study began random selection potential participant official local resident register based information invitation letter study sent potential participant participant participated telephone survey determine resilience participant german version brief resilience scale used consists six different question resilience considered positive adaptation past ongoing exposure potential adverse effect stressor question rated five-point likert scale although half question negatively formulated half positively evaluate questionnaire reversed coding negatively formulated item calculate mean six item additionally participant asked willingness participate upcoming political election time survey participant contacted election inquire actually participated survey aimed investigate potential correlation resilience political participation however people greater resilience high interest politics could willing participate leading self-selection bias additionally people resident city university located therefore population contains increased percentage resident higher education compared rest country survey data overrepresent higher income higher education group participant selected primarily academic environment therefore conclusion drawn generalized entire population germany used auxiliary information two representative data set perform different statistical correction way validate work different data set first representative data set originates data archive social science maintained gesis leibniz institute social science hold comparable representative study politics psychology acquired data set gesis removing participant missing value includes german-speaking population permanent residence germany gesis provided auxiliary information perform statistical correction algorithm repeated ten time mean standard deviation metric calculated run sample removed iteration maximal representative subsamples contained remaining sample figure give overview metric changed execution mrs. auroc decrease constantly time exception fig vertical black line mark iteration remaining sample chosen maximal representing subsample additionally roc curve beginning every time sample removed fig visualized roc curve converge random line lie almost sample removed progression mmd confirms constant decrease distribution distance data set except end biased data set becomes small reliable comparison fig figure auroc roc curve mmd statistical correction auxiliary information gesis element dropped per iteration algorithm run sample remain experiment repeated ten time dark line represent mean surrounding shade visualize standard deviation vertical line indicate iteration remaining sample declared maximal representative subsamples top number represents number remaining sample roc curve given four different step full size image second experiment show general result robust choice representative data set small change result expected due different intersection variable distribution representative data set auxiliary information used another representative data set allensbach institute public opinion research cross-sectional data set collected august september normalized face-to-face interview conducted nationwide adult year older interviewer trained could ask question event uncertainty based official german statistic microcensus individual selected met criterion quota sample respect age sex education professional position region city size removing sample missing data sample remained ten iteration performed drop rate one sample figure show result statistical correction allensbach auxiliary information general decreasing trend metric minor change occur due different distribution intersection representative data set initial mmd allensbach lower mean auroc decrease slower nevertheless stop around iteration shown fig maximal representing subsamples contain sample sample removed algorithm closer roc curve diagonal indicates classifier longer distinguish representative non-representative data set mmd decrease iteration get small showing distance distribution decrease removal sample fig figure auroc roc curve mmd statistical correction allensbach auxiliary information five sample dropped per iteration algorithm run sample remain experiment repeated ten time dark line represent mean surrounding shade visualize standard deviation vertical line indicate iteration remaining sample declared maximal representative subsamples number top represents number sample roc curve given every time sample removed full size image census income following experiment sought understand method incorporates attribute contained representative sample therefore visible training assumption unseen variable attain representative distribution correlation visible variable allowing draw conclusion subsample used census income data set california loaded folktables contains wide variety demographic attribute large number sample task census income predict income person higher fifty thousand per year biased variable income omitted classifier training would reveal representative ratio every iteration sampled subsample sample census income subsample divided two part one representative data provides auxiliary information statistical correction one artificial bias added used non-representative data set performed three different experiment first experiment fraction negative class reduced second experiment fraction positive class reduced sample bias class kept sample class data set biased represent state-wide income ratio last experiment fraction kept unchanged create second representative data set used examine property already representative data set addition metric previously used relative bias target non-representative data set calculated monitor whether decrease aligned relative bias -\bar aligned representative mean target variable weighted mean non-representative data set first employed debias sample contained decreased number lower-income sample auroc start already low value fig gradually decrease eventually reach random line mmd also continually decrease time fig showing algorithm reduces distance distribution relative bias start high decrease time data set becomes small representative fig noted time stopped returned maximum representative subsample relative bias mmd minimum value figure auroc roc curve mmd statistical correction census income subset containing negative sample low income auxiliary information representative subset vertical line indicate iteration remaining sample declared maximal representative subsamples top number represent remaining sample full size image result remaining experiment included supplementary material method comparison validate applicability real-world task tested soft-mrs five different data set task represent use case dependent variable non-representative sample known simultaneously access representative sample identical feature without dependent variable available one hand tested training weighted data set increase predictive performance representative sample hand distance data set relative bias validated compared soft-mrs naive uniform weighting kmm psa kmm determined heuristically computing mean distance sample aggregated set psa used logistic regression calculated sample weight inverse propensity score experiment performed five public data set going describe largest smallest one regard number sample first biggest data set census employment classification task folktables goal predict whether person employed second data set census income folktables next data set human research analytics task predict whether person want work company training course fourth data set breast cancer wisconsin task predict whether sample described characteristic digitized image malignant benign last smallest data set loan task predict whether person eligible home loan data set characteristic given supplementary material randomly split data set training test set containing sample create artificial bias training set used sample selection mechanism chooses sample often away sample mean i.e s_i|x_i -\sigma =-0.05\ census employment income drew sample every iteration splitting complete data set contains many sample calculated sample weight biased training set test set subsequently decision tree classifier cost complexity pruning trained weighted non-representative data set predict target representative data set test result derived average trial auroc area precision-recall curve auprc calculated using prediction decision tree addition assessed mmd relative bias associated target variable section focus discussion auroc auprc table remaining measure found supplementary material psa achieved twice second best auroc second best auprc improves one data set making method impractical downstream classification task kmm also doe perform well reduces four five data set classification quality possible explanation weak result weight distribution computed infinite-dimensional kernel space could suitable train decision tree whereas weight soft-mrs calculated already using decision tree could advantage downstream classification task work best two smallest data set human research analytic census employment increase three case classification result overall best method mean ranking auroc auprc take larger data set account best mean rank auroc auprc removal negative sample increase classifier focus positive sample increase probability correctly classify increase auprc soft-mrs increase one case auroc two case auprc achieved best result mmd relative bias seen supplementary material table downstream task auroc auprc iteration full size table statistical analysis order address application applied real-world scenario involving psychological survey central aim survey investigate evaluate impact varying level resilience individual participation election purpose soft-mrs employed debias subsequently logistic regression model computed maximal representative subsample weighted sample uniformly weighted sample null hypothesis correlation participation election -value 0.05\ chosen reject hypothesis dependent variable binary performed logistic regression independent variable participation election dependent variable additionally applied allensbach auxiliary information calculate maximal representative subsample allensbach used also contains independent variable could lead better representation distribution figure present histogram allensbach soft-mrs compared representative distribution allensbach evident lack low value two histogram right soft-mrs show new distribution closer allensbach apparent empirical distribution function histogram fig empirical distribution function soft-mrs located allensbach figure show distribution soft-mrs approach allensbach also visualize one weakness weighting method create additional sample especially noticeable region 1.0 2.0 almost sample allensbach doe corrected creates artificial sample soft-mrs increase weight area compensate soft-mrs assigns sample higher-valued high weight compensate missing lower-valued sample lead gap medium-valued sample quantitatively measure difference calculated wasserstein distance relative bias soft-mrs allensbach result 0.337 9.031 uniform weighting 0.220 5.819 0.173 2.477 soft-mrs show method increase similarity representative distribution figure comparison brief resilience scale allensbach soft-mrs. histogram cumulative distribution function histogram full size image finally measured correlation voting behavior logistic regression soft-mrs using likelihood ratio test calculate -values -value 0.453 0.562 soft-mrs 0.624 making survey representative changed -values insufficient evidence reject null hypothesis indicates relying solely suitable indicator participation election important acknowledge number nonvoters individual low already low compensates removing person high however removing element sample typically result decreased statistical power participation voting represents one aspect political participation subsequent research survey could explore influence resilience various type political participation running office affiliating political organization engaging social activism