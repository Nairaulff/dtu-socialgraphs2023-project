introduction one challenge transferring recent advance shape analysis medical field object typical benchmark datasets small moderate size thus memory efficiency often primary concern applied medical image algorithm often exceed available memory even high-end gpu many gigabyte memory example model e.g. chair car airplane etc shapenet collection typically consist thousand point typical high-resolution scan yield million point converted point cloud representation obvious opportunity address memory issue lie exploiting spatial sparsity data medical data set skull inherently sparse voxel occupancy rate low since non-empty voxels carry geometric information shape sparse convolutional neural network cnn save memory computational effort work construct sparse cnn using minkowski engine originally designed spatio-temporal tensor demonstrate apply principle sparse binary volumetric data aim evaluate sparse cnn two skull reconstruction task skull shape completion skull shape super-resolution sparse cnn skull image processed original resolution number axial slice head scan moderate memory requirement result show superiority sparse cnn conventional dense cnn term runtime performance memory requirement sparse data paper extension submission autoimplant challenge http reference first demonstrated feasible use sparse cnn skull reconstruction task empirically analysed advantage regular cnn compared ref major improvement work summarized follows edge skull used ref available gpu memory rather low resulting suboptimal skull reconstruction work use whole dense skull thanks extended gpu capacity superiority claim sparse cnn ref substantiated experimental evidence work comparing sparse cnn dense counterpart regarding reconstruction accuracy computation efficiency e.g. memory usage training speed besides shape completion show work proposed sparse cnn also used shape super-resolution variety binary voxel grid representation besides skull show sparse cnn used integral component medical image segmentation task refine binary segmentation mask initially produced dense cnn-based segmentation network related work shape completion shape completion refers process restoring missing region object represented point cloud mesh voxel grid due regularity voxel grid using voxel grid completion take advantage existing well-established cnn architecture auto-encoders designed process image however object originally acquired point cloud voxelized high-resolution voxel grid order preserve geometric detail nevertheless use voxel grid learning-based approach expensive memory requirement grow cubically respect resolution often resulting coarse reconstruction work han dai addressed memory issue voxel grid completion reconstructing high resolution voxel grid two-step coarse-to-fine fashion study work around memory issue using coarse voxel grid e.g. 24\times 54\times 24\ completion figure illustration binarized mri first row second row skull dataset mri skull different resolution midsagittal view mri skull mri skull synthetic defect full size image compared voxel grid point cloud much light-weight efficient representation yuan proposed deep learning framework performs shape completion directly raw point cloud data without voxelization however since point cloud unstructured object size differ number point deep learning deal irregular memory access cnn method shape completion generally used auto-encoder architecture variant address memory issue improve reconstruction quality recent art learning-based reconstruction propose represent shape implicit function reconstruction/completion learnt directly function space since implicit function shape reliant specific resolution shape extracted learned implicit function arbitrary resolution achieving reconstruction continuous space aside memory issue ill-posedness another actively studied problem shape completion considering could exist multiple feasible reconstruction given one incomplete observation recent art propose shape completion framework based autoregressive model e.g. image transformer learn distribution completion multiple feasible completion respecting input sampled ref author exploit advantage implicit representation image transformer learn high-resolution varied shape reconstruction partial observation paper focus solving memory issue learning deterministic mapping defective complete skull represented binary voxel grid skull shape completion clinical application skull shape completion important application craniofacial implant design skull image segmented binary voxel grid high-resolution scan typically resolution 512\times 512\times cnn application size skull image significantly exceeds memory capacity standard desktop gpu previous method either downsample resample skull image smaller intermediate size use patch-wise training inference strategy proposed two-step coarse-to-fine framework generates high-resolution implant reduced memory usage method far optimal downsampling resampling inevitably result image quality degradation consequently deformation skull shape two-step method proposed end-to-end trainable original form patch-based approach requires tailored training strategy make sure cnn capture overall shape distribution human skull besides reported ref reconstructed high-resolution skull would appear patchy due incongruency around border individual patch furthermore ref also showed network would likely learn overall shape distribution skull given entire skull input downsampling compared given portion e.g. bounding box patch skull full-image context help increase network robustness defect pattern generalizability account ideal cnn skull reconstruction take entire high-resolution skull image input output reconstructed skull implant original resolution data spatial sparsity sparse cnn recent approach author adopted hash table exploit sparse binary structure skull image reduce reconstruction time memory consumption instead entire skull volume method reconstructs non-zero voxels store bit-strings voxel occupies one bit memory non-cnn approach requires voxel coordinate stored reconstruction maintain spatial relationship among reconstructed voxels paper propose take advantage spatial sparsity skull data reduce memory consumption using sparse convolution note sparse cnn mean cnn architecture made sparse input data like skull compressed cnn sparse e.g. mostly zero parameter make approach conceptually similar method apply cnn shape high resolution riegler wang used octree representation shape proposed octree-based convolution graham choy proposed sparse convolution defined non-empty point object execution feature extracted non-empty location zero-valued background doe take memory computation resource data generation used two public skull datasets study namely mri skull dataset human connectome project hcp http skull dataset task autoimplant challenge figure memory occupancy shown vertical axis represented total number relevant voxels mri skull datasets different resolution shown horizontal axis represented number voxels sagittal coronal and/or axial image dimension original skull data memory occupancy down-scaled factor ten five mri dataset respectively plot dataset axial dimension differs different image plot black green depict mean mean-min max-mean number voxels full size image mri dataset hcp dataset originally contains structural mri scan selected study training evaluation brainsuite http software used extract skull surface scan note program extract interior exterior skull surface seen fig skull mesh voxelized binary grid representation various resolution 30^3\ 60^3\ 90^3\ 120^3\ fig dataset scan bone structure distinguished based gray value therefore skull simply extracted using thresholding resulting binary voxel grid resolution image resolution resolution varies across scan dataset contains skull training evaluation out-of-distribution test case included also created multi-resolution representation skull z/8 z/4 z/2 illustrated fig datasets portion skull bone around cranium area removed simulate surgical procedure craniotomy experiment skull shape completion fig figure show comparison memory occupancy original skull voxel grid non-zero voxels mri fig dataset fig dataset various resolution specified note plot use number voxels represent overall memory occupancy directly voxel occupies constant space mri dataset stored int8 dataset stored int32 plot show memory usage original skull data grows cubically respect image resolution valid voxels memory usage exhibit approximate linear growth comparison intuitively sparse cnn relying valid voxels would efficient term memory computation dense cnn take entire voxel grid input method use minkowski engine proposed choy backbone sparse cnn minkowski engine originally designed general-purpose tool analysis spatio-temporal data sparse tensor basic data structure sparse tensor generalized representation sparse matrix point empty zero third order sparse tensor expressed aligned x_i y_i z_i =\left\ matrix f_i x_i y_i z_i otherwise matrix aligned coordinate matrix row-wise concatenation coordinate non-empty point f_i n_f non-empty value coordinate x_i y_i z_i n_f number channel point =\begin bmatrix f_1 f_2 ... f_i i+1 ... bmatrix feature vector sparse cnn relies feature computation study use sparse cnn specifically sparse binary volume static data i.e. skull image typical example sparse tensor since majority voxels skull image zero input sparse cnn consists coordinate matrix associated feature vector aligned =\begin bmatrix ... ... ... bmatrix =\begin bmatrix 1\\ 1\\ ... bmatrix aligned number non-zero voxels skull image note coordinate used study refer voxel grid coordinate e.g. instead world coordinate point cloud since skull data binary number channel per voxel one n_f =1\ feature vector format n\times element three-dimensional voxel grid coordinate n\times general data pre-processing step using sparse cnn format input ground truth skull image according similar existing cnn method shape completion use auto-encoder architecture task replaced conventional dense convolutional layer sparse convolutional layer table show configuration layer sparse cnn used experiment list channel number layer number output channel layer longer constant input skull image use i-1 general notation output i.e. feature vector intermediate layer convolution operation coordinate ^3\ sparse cnn therefore defined similar traditional dense cnn aligned i+1 =\sum w^i\mathcal +b_i aligned w^i i-1 b_i\ weight matrix bias intermediate layer ^3\ corresponding coordinate layer i+1\ mapped note unlike traditional dense cnn operates regular voxel grid sequentially sparse cnn requires specifying coordinate mapping order know mapped d'\ non-zero voxels distributed arbitrarily extracting non-zero voxels spatial context within image lost coordinate mapping minkowski engine pair voxel index input ground truth image memorize mapping relationship regular voxel grid leading coordinates-related computation overhead comparable minkowski engine coordinate voxel index stored hash table hash function used fnv64-1a coordinate used hash key retrieve original voxel index associated element feature vector even hash table directly involved feature computation determine element input feature vector mapped element computed according output feature vector shape completion skull shape completion input defective skull output ground truth complete skull divided two sub-tasks reconstructing original defective skull bmatrix bmatrix restoring missing skull bone i.e. implant bmatrix imp imp bmatrix aligned =\begin bmatrix imp bmatrix =\begin bmatrix imp bmatrix aligned aligned imp =\begin bmatrix n+1 n+1 n+1 n+2 n+2 n+2 ... ... ... n+m n+m n+m bmatrix imp =\begin bmatrix 1\\ 1\\ ... bmatrix aligned variable denoting number non-zero voxels generated set coordinate imp imp m\times element imp obviously different different skull instance considering varaitions skull defect according sparse cnn need generate new set coordinate imp value non-zero skull shape completion task m\ne generative sparse tensor decoder table composed generative transposed convolutional layer capable generating new non-zero point absent input given sparse tensor input output transposed convolution written aligned =\sum x-i y-j z-k aligned kernel weight input output coordinate matrix respectively following relationship aligned =\mathcal -\frac ks-1 ... ks-1 aligned denotes outer-product point generated transposed convolution following constraint input coordinate according aligned bmatrix ks-1 ks-1 ks-1 bmatrix aligned see using kernel size greater two would expand span e.g. -ks input coordinate allowing transposed convolution dynamically generate new non-zero point generative task like shape completion specific task generative sparse tensor decoder table trained generate new point maintaining original input coordinate transposed convolution layer table followed pruning layer prune undesirable new point essential maintaining low memory computation cost generative process training ground truth mask teach network keep prune point inference ground truth mask unavailable network prune point feature value lower pre-defined threshold network choose =0\ table configuration number input channel output channel kernel size layer encoder decoder sparse cnn full size table shape super-resolution skull shape super-resolution refers process transforming completed coarse binary skull shape smooth high-resolution representation fine geometric detail input completed skull low resolution ^a\ output completed skull higher resolution ^b\ note skull super-resolution task coarse high-resolution skull i.e. ground truth coordinate system coordinate mapping sparse cnn work properly meaning coarse skull image need up-scaled size target high-resolution image i.e. a=b\ network would fail converge example input resolution 64\times 64\times z/8 ground truth resolution 256\times 256\times z/2 a=b\ mean input ground truth resolution perspective image quality rather mean input interpolated size ground truth up-scaled input still appears blurry coarse lack geometric detail according ref difference up-scaled coarse skull voxel grid high-resolution voxel grid simply arrangement pattern zero non-zero voxels rearranging voxels coarse skull shape upgraded high-resolution representation total number non-zero voxels two type skull show statistical difference therefore use following represent ground truth coordinate matrix feature vector super-resolution task aligned =\begin bmatrix ... ... ... n_0 n_0 n_0 ... ... ... bmatrix =\begin bmatrix 1\\ 1\\ ... 1\\ ... bmatrix aligned assume up-scaled coarse skull ground truth posse variable non-zero point share n_0\ variable common point n_0 -n_0\ non-zero point input need pruned -n_0\ new non-zero point need generated new coordinate therefore sparse cnn specified table still applicable super-resolution task memory usage computation complexity memory consumption neural network come primarily following source training time input ground truth image batch output intermediate layer forward pas network parameter memory usage back-propagation error gradient parameter optimizers test time parameter network input image batch intermediate layer output main source memory usage study compare memory consumption sparse dense cnn network configuration table number parameter param dense sparse cnn configuration param estimated aligned param =\sum _i\times _i\times ks^3+\mathcal aligned number bias layer number output channel layer _i\ assuming parameter stored float32 32-bit sparse dense cnn consume amount memory storing parameter however input ground truth dense cnn original voxel grid sparse cnn valid non-zero voxels required thus sparse cnn consumes significantly memory dense cnn loading input ground truth image batch shown fig similarly size output f^i corresponding intermediate layer linear feature dimension i-1 th\ layer i-1 calculated aligned f^i =\frac i-1 +2p-ks aligned padding stride size respectively according memory consumption intermediate layer output also linear input image size fig memory consumption related back-propagation optimizer tricky calculate study estimate overall gpu memory usage training using nvidia-smi command provided nvidia query system gpu memory usage 50-millisecond interval train training iteration train number training sample batch size set take average queried value final amount memory consumed training considering number non-zero voxels different training sample static memory occupancy caused training network subtracted measurement inference used method except measurement taken network loaded trained parameter run test set floating point operation flop commonly used measure computational complexity cnn flop consumed cnn layer product f^i _i\times _i\ given network configuration _i\ _i\ flop linear f^i thus input image size fig sparse cnn therefore significantly faster dense cnn training inference time configuration figure dsc left right sparse cnn mri dataset different resolution 30^3\ 60^3\ 90^3\ 120^3\ shape completion task horizontal axis corresponds image resolution full size image figure dsc top bottom sparse cnn dataset different resolution shape completion task horizontal axis corresponds image resolution 64^2 z/8 64^2 z/8 128^2 z/4 128^2 z/4 256^2 z/2 512^2 full size image experiment result trained sparse cnn table two task first task skull shape completion mri skull dataset different resolution 30^3\ 60^3\ 90^3\ 120^3\ mri dataset 64^2 z/8 128^2 z/4 256^2 z/2 512^2 dataset dataset set ch1=\ 0.435m parameter mri dataset set ch2=\ 18.14m parameter second task skull shape super-resolution skull dataset different scale 64^2 z/8 128^2 z/4 64^2 z/8 256^2 z/2 64^2 z/8 512^2 128^2 z/4 256^2 z/2 important emphasize data multi-resolution skull representation created downsampling original skull represented voxel grid 512^2 lower resolution whereas mri data skull represented mesh voxelized voxel grid different resolution comparison also trained standard dense cnn configuration sparse cnn shape completion task dataset task used dice similarity coefficient dsc reconstruction error i.e. percentage misclassified voxels evaluate prediction sparse cnn trained using binary cross-entropy loss bce aligned bce =y'\cdot log\sigma 1-y log 1-\sigma aligned dense cnn trained using dice loss dice background i=0\ target i=1\ aligned dice =-2\sum i=0 y'^ +\sum y'^ y'^ aligned sigmoid non-linearity y'\ denote prediction ground truth respectively denotes element-wise multiplication two matrix dice loss ideal choice measuring difference two mask note skull reconstruction task dice loss facto loss function dense cnns output mask however sparse cnn output point e.g. coordinate voxels grid point cloud cross-entropy loss facto choice table show quantitative evaluation result mean dsc shape completion task table also reported performance comparison sparse cnn different number parameter i.e. resolution 64^2 z/8 128^2 z/4 result indicate increasing model complexity sparse cnn would also lead increased prediction accuracy phenomenon well observed traditional dense cnn model worth noting using sparse cnn able train skull image full resolution 512^2 result promising 0.99 dsc 0.12 reconstruction error e.g. 512^2 256\ image voxels misclassified average learning full resolution advantageous compared learning downsampled data quality input ground truth inevitably compromised worth mentioning sparse cnn tends achieve higher dsc higher resolution according result highest dsc achieved full resolution 512^2 using lightweight network contrast gpu memory restriction made training 512^2 image resolution using dense cnn unsuccessful furthermore quantitative result dense cnn significantly worse sparse cnn seen table note shape completion result table directly comparable autoimplant challenge result three reason fair comparison dense cnn used vanilla network configuration sparse cnn challenge submission used complex different dense network architecture combined tailored pre- post-processing e.g. data augmentation achieve result table reported result resolution 64^2 z/8 128^2 z/4 256^2 z/2 dense cnn challenge reported result resolution 512^2 i.e. challenge submission up-scaled coarse output resolution differs different submission 512^2 calculating metric ground truth result reported table apply skull challenge result apply implant obtained taking difference reconstructed ground truth skull note force dense cnn follow architecture specifically designed sparse cnns table rather way around might lead unfair competition two therefore table solely show comparison sparse dense cnn one vanilla setting result produced specific setting generalizable dense cnns general provide external comparison proposed sparse cnn refer dense network 82m parameter trained dataset skull shape completion dsc three variant method reported 0.7547 interpolation 0.7529 voxel rearrangement 0.8587 patch-based training inference quantitatively sparse cnn 0.435m parameter performs significantly better dsc 0.9903 figure show dsc distribution test set mri skull datasets respectively provide quantitative comparison sparse cnn top-ranked dense cnns autoimplant challenge first extracted implant subtracting defective input completed skull reconstructed sparse cnn resolution 512^2\times calculated dsc border dsc bdsc 95th percentile hausdorff distance hd95 original ground truth implant challenge result reported table worth highlighting sparse cnn surpassed best performing dense cnn challenge term border dsc important indicator clinical applicability implant despite significantly lightweight quantitative comparison previous work also given table note current work used sparse cnn configuration different format training data ref boundary skull used training due hardware limitation current work used original full skull voxel grid nevertheless previous work still achieved comparable bdsc state-of-the-art dense cnns without resorting complex post-processing intensive data augmentation ref table quantitative results—dsc skull shape completion task mri datasets different resolution full size table table quantitative comparison sparse cnn top-ranked dense cnns autoimplant challenge regarding implant generation resolution 512^2\times term dsc border dsc bdsc 95th percentile hausdorff distance hd95 full size table figure show comparison estimated memory consumption dense sparse cnn different image resolution training inference well comparison memory consumption sparse cnn different batch size image resolution 64^2 z/8 training table report estimated memory usage increase image resolution image size increase cubically training memory consumption sparse cnn increase approximately linear manner image resolution 256^2 z/2 512^2 resolution memory usage quadruple dense cnn memory usage demonstrates non-linear growth inference memory usage sparse cnn increase linearly resolution memory consumption sparse cnn increase linearly respect batch size furthermore sparse cnn channel posse time parameter channel whereas memory increase two time take indication sparse cnn raising model complexity improve prediction accuracy doe cause dramatic increase memory usage figure show qualitative completion result mri datasets different resolution use average gpu execution time per skull image portray runtime speed sparse cnn training measure duration training iteration compute average time per iteration inference measure time take run entire test set image batch size set one case experimented data using shape completion model resolution 64^2 z/8 128^2 z/4 256^2 z/2 training/inference time per image roughly 0.28/0.22 0.32/0.30 0.71/0.54 excluding data loading see training inference time increase linearly respect resolution note time measured way dense cnn directly comparable sparse cnn variable i.e. amount computational resource occupy runtime controlled measurement figure memory consumption training inference sparse dense cnn different resolution left memory consumption sparse cnn different batch size resolution right full size image table comparison estimated memory consumption second last row training inference sparse dense cnn different image resolution first row full size table table sparse cnn memory consumption second third row different batch size first row resolution 64^2\times z/8 training full size table keep mind sparse cnn time memory growth reported strictly linear especially training high resolution memory time overhead includes space computation reserved voxel coordinate coordinate mapping implementation-related cost figure example skull shape completion result sparse cnn mri skull dataset different resolution first third column example show input defective skull grid prediction ground truth respectively full size image figure shape completion result sparse cnn skull dataset different resolution first third column example show input defective skull grid prediction ground truth respectively full size image table quantitative result dsc skull shape super-resolution task dataset full size table table show quantitative evaluation result super-resolution task table represents super-resolution using sparse cnn represents up-scaling using interpolation figure show dsc distribution see super-resolution sparse cnn outperforms interpolation-based up-scaling besides super-resolution directly lowest-resolution highest resolution i.e. 64^2 z/8 512^2 yield worst result sparse cnn show better performance smaller resolution gap table compare super-resolution shape completion resolution 256^2 z/2 result suggest sparse cnn might better completion task figure dsc top bottom super-resolution task dataset horizontal axis corresponds image resolution 64^2 z/8 128^2 z/4 64^2 z/8 256^2 z/2 64^2 z/8 512^2 64^2 z/8 128^2 z/4 64^2 z/8 256^2 z/2 64^2 z/8 512^2 128^2 z/4 256^2 z/2 128^2 z/4 256^2 z/2 shape completion dash-lined box contain zoomed-in boxplots full size image qualitative result fig demonstrate advantage super-resolution using sparse cnn see missing geometric detail around craniofacial area coarse skull effectively recovered final super-resolution output figure qualitative result skull shape super-resolution first column show coarse completed skull resolution 64^2\times z/8 second last column show interpolation super-resolution result resolution 128^2 z/4 256^2 z/2 512^2 full size image discussion conclusion future work paper presented comprehensive evaluation sparse cnn architecture two skull reconstruction task skull shape completion skull shape super-resolution result show sparse cnn significantly outperforms traditional dense cnn respect speed quality memory efficiency sparse data unlike existing dense cnn-based approach often compromise accuracy memory vice versa sparse cnns inherently designed spatially sparse data therefore exempt accuracy-memory tradeoff achieving high accuracy computation efficiency e.g. lower memory usage skull reconstruction task employing dense cnns rather advantageous sparse cnns early deep learning-based skull reconstruction study suitability convenience since existing dense cnns straightforwardly applied voxel grid data discussed section one limitation current sparse cnn framework minkowski engine used study voxel coordinate well associated coordinate management tool need created stored memorize spatial relationship non-zero voxels convolution causing computation overhead comparison dense cnn another limitation initialized properly generative transposed convolutional layer might generate large amount point cause false out-of-memory error training therefore one future direction worth investigating regularize generative layer use shape prior output prevent network generating random amount point additionally sparse cnn failed out-of-distribution test set autoimplant challenge meaning network overfitting skull defect pattern lacking generalizability even given full-image context training presume network would fail real craniotomy skull well since craniotomy defect tend irregular synthetic defect used shape completion experiment yet decide cause failure future effort issue still required supplementary material provided additional experiment result spatially sparse medical image heart aortic vessel trachea esophagus segmentation task result indicate moderate increase computation memory quality initial segmentation mask dense cnn substantially improved using proposed sparse cnn model