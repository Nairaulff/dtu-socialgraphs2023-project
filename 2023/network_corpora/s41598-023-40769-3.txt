introduction among fluid property hydrocarbon reservoir oil formation volume factor play vital role parameter indicates change volume produced oil reservoir surface condition fact volume oil enters stock tank surface condition volume oil produced reservoir condition enter production well oil volume change reservoir surface condition affected significant pressure reduction bubble point resultant release dissolved gas oil especially large amount solution gas therefore oil formation volume factor defined always equal greater reservoir oil volume specified temperature pressure stock tank oil reservoir oil unprecise prediction parameter could make various process calculation challenging oil engineer process calculation include reservoir simulation inflow performance fluid flow porous medium place-in-oil estimation material balance well test analysis economic analysis ideal method determine pvt property oil sample use experimental test often costly time-consuming hence numerous study predicting pvt property using correlation equation developing various artificial intelligence-based approach table provides brief overview advantage disadvantage aforementioned method despite simplicity applying correlation predicting pvt property especially parameter produce significant error limiting application sensitive activity example estimating original oil place artificial intelligence-based method adequately limit error time cost saving table advantage disadvantage previous employed method full size table estimate parameter good accuracy several study employed potent machine learning method tree-based algorithm support vector machine svm linear/non-linear regression deep learning neural network network-based method adaptive neuro-fuzzy inference system anfis method developed based experimental data reservoir different region purpose part data used training part applied testing model moreover several study combined aforementioned artificial intelligence method optimization algorithm genetic algorithm simulated annealing particle swarm optimizer pso optimizing input parameter utilizing optimization algorithm prior development machine learning model confers benefit including improved performance accelerated convergence enhanced generalization increased efficiency customization according specific requirement improved interpretability model also worthy mentioned study presented result correlation better explain development literature major study past decade discussed following study based development correlation include arabloo used lingo fattah lashin used non-linear regression technique genetic programming based volatile oil reservoir data bank mahdiani norouzi used simulated annealing optimization method presented correlation predicting based common parameter reservoir temperature solution oil–gas ratio api gravity gas relative density claimed proposed correlation improved prediction accuracy compared previous one saghafi proposed model correlation predicting oil formation volume factor using adaptive neuro-fuzzy inference system anfis addition functional correlation implementing genetic programming model proposed based aforementioned parameter another study seyyedattar used tree-based method extra tree addition anfis estimate oil formation volume factor study also extensively discussed model remarkable capability estimate intended parameter wide range feature another major study rashidi combined machine learning optimization method achieve improvement study employed two machine learning algorithm multi-layer extreme learning machine melm least square support vector machine lssvm two method order optimize parameter genetic algorithm particle swarm optimizer pso also noteworthy applying pso method instead halved prediction error reviewed study used artificial intelligence predict based black oil method conventional feature reservoir temperature solution gas-oil ratio api gravity gas relative density larestani utilized multiple machine learning technique ets dts generalized regression neural network cascade-forward backpropagation network conjunction radial basis function multilayer perceptron neural network estimate oil formation volume factor based compositional oil method study used oil composition obtained oil composition analysis common input parameter introduce ets superior model based statistical graphical comparison express model efficiency various comparison made correlation previous machine learning method equation state eos aforementioned study used machine learning neural network estimate parameter despite efficiency method effectively black box hid exact relationship input output prevented distinguishing function clearly overcome limitation wood choubineh used transparent open box tob learning network algorithm led logical accurate prediction note proposed method evaluated predicting oil formation volume factor bubble point study key issue addressed precise estimation reduced computational error furthermore essential method optimized term time computational cost achieve innovative artificial intelligence-based technique employed along simultaneous integration study aimed accurately estimate oil formation volume factor using machine learning method various reservoir pressure temperature range black oil parameter without implementing result oil composition analysis database used training testing model cover wide range pvt data iran oil reservoir including data point constant composition expansion cce differential liberation separator test three advanced soft computing approach rely gradient boosting decision tree gbdt utilized include xgboost gradientboosting catboost hence developed model reliably predict iranian oil reservoir study reservoir pressure parameter also used effective parameter along input parameter including reservoir temperature api gravity solution gas-oil ratio sample express performance gbdt-developed model quantitative qualitative analyzes well comparison previous machine learning approach including random forest decision tree dts extra tree ets based oil composition method used advantage proposed method non-dependence estimation value lower pressure e.g. bubble point pressure remaining part document structured follows model section provides overview fundamental principle algorithm soft computing technique implemented section titled result discussion outline approach taken model creation provides analysis finding subsequent discussion finally conclusion section summarizes key finding study model study utilizes emerging machine learning technique known ensemble combine multiple classifier enhance robustness improve accuracy classification performance technique effective dealing noise compared single-classifier method research employ three ensemble technique utilize gradient boosting decision tree algorithm gradientboosting catboost xgboost reason implementing boosting method discussed follows parallelization scalability many boosting implementation xgboost designed highly parallelizable scalable efficiently utilize parallel computing resource multi-core cpu distributed computing framework speed training process handle large-scale datasets improved predictive accuracy boosting method excel improving predictive accuracy compared traditional machine learning algorithm combine multiple weak model often decision tree create strong ensemble model capture complex relationship data iteratively focusing sample difficult predict boosting method gradually improve overall accuracy model robustness overfitting boosting method effective reducing overfitting utilize technique regularization mitigate risk overfitting training data allows boosting model generalize well unseen data perform consistently different datasets order facilitate comprehensible perception table provides concise overview advantage disadvantage application utilized model table advantage disadvantage application utilized model full size table gradientboosting boosting technique focused iterating reevaluating error step create robust learner combining multiple weaker learner training data used model defined x=\ representing feature interest target data essence method aim find approximate value x\right based following condition x\right =\mathrm arg f\left x\right min f\left x\right f\left x\right cost function arg f\left x\right min f\left x\right value f\left x\right achieves minimum cost function enhances accuracy parameter prediction attaining minimum value weak learner endeavor improve upon reduce error previous weak learner ultimately objective obtain desired regression tree function i.e. parameter represents weak learner decision tree adjusted aligned determined slope x\right updated final step based iteration performed detailed information please refer supplementary file—sect 2.1 —gradientboosting catboost catboost relatively new gradient boosting decision tree gbdt method gbdt known perform well applied datasets containing numerical feature however datasets may contain string feature gender country name feature may greatly impact accuracy final prediction crucial ignore eliminate therefore customary convert categorical string feature numerical feature training dataset unlike gbdt-based method catboost offer notable advantage able handle categorical feature training process mentioned earlier categorical feature inherently non-numerical incorporate model need convert numerical feature commencing training process detailed information conversion method catboost address potential issue prokhorenkova may arise process please refer supplementary file—sect 2.2 —catboost xgboost extreme gradient boosting xgboost algorithm developed introduced chen belongs modern machine learning technique based gradient boosting decision tree algorithm aim minimize error maximize adaptability creating large number tree e.g. approximate estimated value closely possible combining weak learner algorithm build strong learner however algorithm weak learner constructed residual fitting xgboost model extends cost function incorporating first-order taylor information presenting second-order derivative information enhancement enables faster convergence learning process additionally xgboost algorithm includes regularization component cost function help control complexity reduces risk overfitting detailed understanding general process xgboost algorithm please refer supplementary file—sect 2.3 —xgboost provide tangible comprehension fig illustrates proposed algorithm structure figure schematic xgboost algorithm full size image name version package used analysis model development follows numpy 1.22.4 panda 1.5.3 scikit-learn 1.2.2 catboost 1.2 xgboost 1.7.6 seaborn 0.12.2 matplotlib 3.7.1 result discussion model development databank obtained series pvt test various sample iranian oil wide pressure range sample bubble point pressure exceeding bubble point parameter obtained separator test time necessary use cce separator test determine parameter pressure bubble point following correlation used obtain parameter result mentioned experiment cce osb osb odb cce total relative volume cce test osb oil formation volume factor bubble point pressure obtained separator test odb oil formation volume factor bubble point pressure test oil formation volume factor test desired pressure therefore total experimental data point adequately represent iranian crude oil sample collected used develop efficient model estimation greater accuracy feature used sample include reservoir pressure temperature api gravity solution gas-oil ratio physical base also implemented known correlation used estimation important note methodology employed approach relies black oil reduces number feature save time reduce memory consumption lead efficient commercial simulator also five data preprocessing stage applied summarized fig running preprocessing stage model development offer advantage improved data quality enhanced feature representation better handling missing irrelevant data leading improved model performance generalization capability additionally preprocessing allows efficient data transformation normalization scaling enabling model effectively learn pattern relationship data figure data preprocessing step full size image analyzing result presented fig taking account expert opinion outlier detected among collected data although point valid exhibit significant departure majority data sample considerable deviation mean strongly influence parameter coefficient estimated machine learning model may compromise predictive performance hence data point excluded training testing datasets figure data joint plot outlier detection full size image table provides comprehensive overview whole data including train test data range utilized constructing model table statistical range parameter related inputs/outputs employed developing model full size table ensure robust dependable result important note databank randomly split two subset first subset comprising data used train model second subset contained remaining data used evaluate effectiveness model therefore reliability developed model compared blind case regarding evaluation model using testing data comprehensive analysis conducted ensure testing data fall within range training data evaluation based information provided table statistical variable including mean standard deviation calculated training data subsequently verified value reported testing data aligns within acceptable range determined statistical measurement hereby confirmed testing data demonstrates strong alignment characteristic distribution observed training data consequently validation ensures performance model generalizability real-world scenario table statistical range training data full size table table statistical range testing data full size table grid search hyperparameter tuning technique used machine learning find optimal value set hyperparameters produce best model performance hyperparameters model parameter learned data specified beforehand grid search involves defining set value hyperparameter creating grid possible combination hyperparameter value evaluating combination using performance metric accuracy mean squared error combination hyperparameter value produce best performance evaluation metric selected optimal hyperparameter table display control parameter algorithm utilized paper outcome hyperparameters table control parameter employed development application soft computing technique full size table performance evaluation study employed various statistical graphical comparison examine capability adequacy model correlation obtaining statistical indicator presented following average absolute relative deviation aard aard\ =\frac i=1 iexp ipred iexp 100. coefficient determination =1-\frac i=1 iexp ipred i=1 ipred -\overline root mean square error rmse rmse=\sqrt i=1 iexp ipred represents output oil formation volume factor exp represents actual value pred represents estimated value furthermore denotes mean output represents total number data point demonstrate strength model conducted tenfold cross-validation training dataset process cross-validation involves dividing training set subset training model fold validating model remaining data performance model evaluated average value obtained fold study tenfold cross-validation performed resulting rmse-score found 0.0198 xgboost shown table could seen later reported rmse-score suggests xgboost model performed well data used test also dataset used training indicating model highly accurate reliable table performance measure reported tenfold cross-validation full size table combining graphical evaluation statistical indicator facilitates examination model term accurate estimation fig according cross plot uniform distribution prediction along x–y axis suggests model produce accurate prediction majority test data minimal deviation x–y axis case xgboost indicates excellent performance suggests xgboost outperforms method term efficiency overlap predicted value actual value cross-plot evaluation method used ass accurately effectively model perform figure cross plot implemented model xgboost gradientboosting catboost full size image table report statistical indicator developed model result illustrate average absolute relative deviation aard coefficient determination 0.2598 0.9994 respectively xgboost model outperforms model estimation following statistical indicator used comparing model reviewed method term performance table statistical index used describing performance proposed model full size table time memory occupied model additional performance indicator used alongside error analysis based statistical indicator therefore average training time inference time occupied memory reported table indicates xgboost significantly faster requiring time memory train test table time memory assessment modelling approach full size table study find xgboost improved upon gradient boosting decision tree gbdt technique several key area firstly xgboost utilizes second-order taylor expansion first second order improved residual whereas traditional gbdt first-order taylor expansion feature allows xgboost capture complex relationship feature enhance prediction power secondly xgboost incorporates regularization term objective function control model complexity prevent overfitting regularization improves model generalization performance new data overall study concludes combination feature make xgboost highly effective versatile machine learning method lastly xgboost random forest column sampling method reduce chance overfitting hence xgboost model shown excellent learning performance training speed alternative approach evaluating model performance entail analyzing predictive deviation model respect value acquired experimental test across entire dataset assessment narrower range deviation signify superior performance parameter prediction estimation figure show relative deviation developed model revealing xgboost model achieves absolute relative deviation majority dataset outcome serf evidence accuracy efficiency xgboost model figure relative deviation estimated oil formation volume factor value using xgboost gradientboosting catboost model test train data point full size image comparison developed model previous approach previous section employed statistical indicator graphical tool show estimation performance developed model various pressure range xgboost model machine learning method discussed study better performance among others larestani presented machine learning approach based bagging method using compositional oil feature method shown superior previous machine learning method various equation state using statistical indicator presented supplementary file—sect —comparison preexisting approach therefore result study compared larestani result following also note fair comparison databank used testing training process present study larestani comparison compositional study using oil composition method feature normal method including oil composition methane non-hydrocarbons specific gravity molecular weight reservoir temperature pressure feature division oil component three subgroup lumped method larestani estimated desirable output parameter mentioned feature used developing model study include api gravity temperature pressure table compare model study top three model larestani include tree-based bagging method shown table larestani introduced extra tree model lumped mode optimal model analysis indicator suggests model especially xgboost technique outperform method proposed larestani novel advanced machine learning model xgboost reduced ets model error larestani superior model quarter despite using fewer feature compared normal mode/4 compared lumped mode achieved presenting accurate estimation time cost saving independent oil composition analysis suggesting practical economical simulation table performance developed model comparison compositional model full size table paper provided train error besides test error indicate whether model suffers overfitting model significantly lower train error test error indicates potential overfitting difference suggests model fitting training data well struggle generalize new unseen data regard table fig compare bar chart aard rmse statistic model superior model larestani figure show clear graphical representation superior performance developed model especially xgboost error measurement statistic figure error bar chart developed model study comparison best compositional model ets larestani based aard rmse estimating oil formation volume factor full size image mentioned model section black oil model selected order reduce number input feature consequence time memory saving noted likewise larestani selected lumped model significantly reduce number feature compare runtimes occupied memory xgboost method used study lumped model larestani study average runtimes occupied memory reported table result demonstrate xgboost requires significantly computing time memory worthy noted fair comparison computation time developed model within study well developed model larestani study compiled cpu table time memory assessment comparison xgboost model vs. larestani lumped model full size table figure show change different pressure relative bubble point suggesting change lower pressure exceeding bubble point limited change could attributed stable oil composition pressure exceeding bubble point change merely due oil expansion reservoir meanwhile solution gas lower pressure bubble point reduces oil volume closer surface condition fact change oil composition due solution gas evolved affect pressure lower bubble point reduction solution gas reduces therefore representative oil composition crucial parameter oil volume change included feature set better illustrate greater efficiency developed model fig show bar chart prediction error two pressure range higher lower bubble point figure oil formation volume factor vs. pressure curve full size image figure accuracy developed model predicting oil formation volume factor two different pressure range bubble point pressure full size image order establish reliable basis comparison study provides comprehensive analysis reason behind superiority boosting method bagging method specifically extra tree algorithm employ bagging xgboost catboost gradientboosting algorithm utilize boosting technique boosting bagging ensemble method aim improve accuracy machine learning model however approach differ method better depends specific problem dataset bagging bootstrap aggregating method multiple model trained different subsamples data replacement final prediction combination prediction model bagging reduce variance overfitting hand boosting iterative method train multiple weak model sequentially subsequent model try correct error previous one boosting aim reduce bias improve model performance several study compared performance boosting bagging various datasets result mixed study shown boosting outperforms bagging others shown opposite review ensemble method buciluǎ found boosting bagging similar performance many datasets boosting tends perform better datasets small number feature study feature obvious feature high machine learning task boosting method perform better bagging method sample table present experimental value xgboost model estimation four iranian oil sample different pressure also order provide better outlook graphical illustration presented corresponding sample fig figure evidently demonstrates capability proposed model reproducing physical trend different pressure agreement general knowledge hence concluded confidently xgboost model accurately estimate regardless pressure range oil type table experimental value xgboost model estimation four iranian oil sample different pressure full size table figure graphical illustration comparison experimental value xgboost model estimation four iranian oil sample different pressure full size image conclusion aard error associated machine learning algorithm based gbdt namely xgboost gradientboosting catboost present study reported 0.2598 0.3581 0.5615 respectively hence xgboost model attained best result hand result previous study concerning utilization bagging model demonstrate lumped extra tree model best-reported approach larestani exhibit aard error rate 1.1681 result xgboost model successfully improved error value 0.9 comparison lumped ets significant advantage current study considering four input parameter without need applying oil composition data compared bagging model implementing compositional approach along higher number input parameter parameter normal case/7 parameter lumped case additionally another advantage development single model pressure region reservoir ranging low pressure pressure exceeding bubble point despite study previous study employed two separate model higher lower pressure region bubble point furthermore favorable performance xgboost attributed following factor elaborate xgboost algorithm relatively new method based gbdt creates tree equal depth consecutively making faster gbdt-based model due parallel processing also employ regularization technique mitigate overfitting regularization encourages parameter approach zero effectively removing impact certain feature regularization reduces magnitude weight without forcing become precisely zero xgboost model exhibit capability handle missing nan number data value enhancing robustness practicality real-world application addition universal application developed model predicting volumetric property newly discovered reservoir using limited wellhead reservoir data without need running routine pvt laboratory test model trained using available fluid sample pre-developed field specific region world utilized field region one limitation conducted study utilization certain hyperparameters default value optimized future study using appropriate optimization method