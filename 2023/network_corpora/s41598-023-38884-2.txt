introduction increasing popularity mobile device multimedia communication image evolved primary carrier information owing inherent constraint imaging system dynamic unpredictable nature shooting environment dynamic scene image blurring becomes inevitable occurrence thus research attention shifted towards image deblurring method blurred image subjectively affect visual experience also affect subsequent visual task accordingly addressing image deblurring technique dynamic scene emerges crucial problem solve conventional image deblurring method rely either known assumed blur kernel leverage prior information image however method encounter challenge dealing removal blur induced complex factor development deep neural network paved way blind image deblurring method necessitate estimation blur kernel thus method gained widespread use among various deep neural network approach u-net stand improved version fcn fully convolutional neural network incorporated enhancement provides u-net increased flexibility integrating feature multiple hierarchical level making powerful tool image deblurring complexity deep network effectively reduced significantly curtailing number parameter maintaining accuracy within acceptable range additionally scarcity research image deblurring based u-net provided motivation explore improved u-net solving image deblurring dynamic scene main objective follows investigate develop improved u-net architecture tailored image deblurring purpose examine identify key module capable extracting image detail crucial information across different layer thereby enhancing image feature extraction capability improving visual outcome recently u-net improved version gradually applied image deblurring related field image enhancement image restoration super-resolution example liu proposed retinex-unet convolutional neural network learn decompose image result input enhancement network end-to-end training described method enhance image size improve overall visual effect raj proposed residual dense connection-based u-net model fundus image enhancement effectively capture local global information experiment demonstrated proposed model could effectively enhance visual quality fundus image chen introduced u-net-like deep stacked autoencoder neural network model designed restoration image distorted atmospheric turbulence model fuse low-level high-level information greatly ensuring integrity information obtaining high-quality restored image chen proposed deep learning method called ncs-unet method incorporates distinctive feature non-subsampled contourlet transform nsct sobel filter extract valuable information consequently enhances performance noise reduction artifact removal pct image improve low-resolution fundus image fan proposed style-guided u-net incorporates series style-guided u-shape block sub sub enlarges receptive field fully fuse hierarchical feature experimental result demonstrated sgunet robust accurate method mao proposed residual fast fourier transform convolution block used foundation constructing deep network network capable capturing long-term short-term interaction data integrating low- high-frequency residual information experimental result demonstrated improved deblurring performance using approach proposed u-net model containing dense block dynamic scene deblurring model significantly reduces inference time among described method suitable image deblurring specific situation others achieve cross layer flow feature information room improvement feature extraction capability integration wavelet transforms deep convolutional neural network beneficial mitigating image blur meanwhile overcome vanishing gradient problem dsc depth-wise separable convolution residual network address described problem additionally skip connection u-net sometimes introduce redundant information attention mechanism demonstrated effectiveness extracting critical relevant information data providing solution issue drawing inspiration previously mentioned technique present study introduces novel synthetic image deblurring method method combine wavelet transforms dsc depth-wise separable convolution residual network attention mechanism enhance capability u-net architecture aim effectively address challenge associated image deblurring present study offer following contribution 4-layer network based u-net proposed including one encoder one decoder four block mlff module added integrates feature information different layer u-net network change inherent information flow mode conventional u-net network integrates feature information different scale network extract feature information dmrfab module introducing cam channel attention mechanism sam spatial attention mechanism incorporated extract crucial information deep feature improve image deblurring effect accordingly fft introduced loss function frlf proposed allows frequency value image obtained reducing frequency difference image thereby improving deblurring effect proposed model underwent quantitative qualitative analysis using gopro real blur datasets result reveal image deblurring quality significantly enhanced proposed model rest paper organized follows proposed methodology presented detail section proposed method experiment result discussion given section result section discussion respectively conclusion provided section conclusion proposed method proposed method based improved u-net model discussed section model structure proposed method based improved u-net model structure shown fig model includes one encoder one decoder encoder four block extract feature layer convolutional structure 2-2-2-2 every block exception initial block undergoes specific process involving one down-sampling operation using discrete wavelet transform dwt followed two convolutional operation extracted feature layer fused mlff module decoder adopts four block convolutional structure 2-3-3-2 except last block block undergoes two convolution one inverse wavelet transform iwt reduce convolution parameter computational complexity solve degradation problem deep network convolutional block replaced dsc depth-wise separable convolution rdsc residual depth-wise separable convolution encoder decoder connected dmrfab allow richer detailed feature obtained figure improved model structure full size image model mainly composed dsc rdsc group including three rdsc dwt iwt mlff module dmrfab module model 4-layer network based u-net encoder left decoder right input image represented represents height image represents width image represents number channel image processing model involves following three stage encoding stage top bottom first layer one 32-channel dsc one rdsc group composed three rdscs transforms input image represented second layer entered first level dwt using one 64-channel dsc one rdsc group transform input feature information 64\ represented entering third layer second level dwt one dsc one rdsc group used transform entered feature information 128\ represented entering fourth layer third level dwt incoming information transformed 256\ using one dsc one rdsc group one dmrfam subsequently number channel reduced dsc feature information encoder input mlffi\ 1,2,3 fusion different layer feature information decoding stage iwt performed output information fourth layer passed one dsc one rdsc group transformed 128\ followed one dsc operation increase number channel third layer output feature iwt enters one dsc one rdsc group transforming fused information 64\ subsequently one dsc operation performed increase number channel second layer output feature iwt enters one dsc one rdsc group transforming fused information convolutional feature information separated depth becomes output added fused image information initial input model obtain deblurred image final result module model introduced detail dsc improved model introduces dsc decrease number model parameter make network lightweight structure model shown fig dsc composed dwc depth-wise convolution pwc point-wise convolution dwc divide multi-channel feature previous layer feature map single channel convolution kernel convolution subsequently dwc recombines adjusting size feature map previous layer maintaining number channel characteristic image attained dwc convoluted using pwc convolution kernel blend convolution result dwc flexibility alter number output channel needed figure structure dsc full size image rdsc rdsc designed based residual network spread detailed information different layer promote blur reconstruction quality also serf mechanism mitigate issue gradient vanishing rdsc two dsc two leaky relu activation function structure shown fig first dsc leaky relu dsc operation performed input information obtained feature input information fused mean skip connection finally fusion result output leaky relu processing figure structure rdsc full size image dwt iwt dwt iwt respectively replace down-sampling up-sampling function u-net model obtain image information different frequency thereby reducing feature information loss image reconstruction mitigating blurring effect shown fig dwt iwt respectively performed result encoder iwt performed decoder haar wavelet wavelet basis function easy implement operate therefore present study two-dimensional haar wavelet adopted wavelet transform operation divide image signal directional sub-bands filtering effective method realizing dwt firstly one-dimensional high-pass filter represented defined utilized filter vertically down-sample column image subsequently defined employed filter horizontally down-sample row process yield sub-frequency information second step filter used filter vertically down-sample column image used filter horizontally down-sample column result sub-frequency information sub-frequency information four parameter shown x\right -\mathrm 1,1 x\right 1,1 y\right =\varphi y\right =\varphi y\right =\psi y\right =\psi represent row column information image denotes horizontal high-frequency vertical low-frequency information image denotes horizontal vertical high-frequency information image express horizontal vertical low-frequency information image represents horizontal low-frequency vertical high-frequency image information iwt performs inverse operation four sub-images using aforementioned filter thus used fuse original image therefore original image decomposed dwt reconstructed iwt without loss information multi-level wavelet transforms implemented processing according described method two-dimensional haar wavelet transform sum mean value used low-frequency information regarded difference mean value used high-frequency information regarded mlff module existing improved u-net network flow feature information inflexible allowing horizontal information flow layer vertical information flow upper lower layer proposed model different mlff module designed increase flow information different layer u-net integrates characteristic information different layer however straightforward approach involving addition concatenation information source lead redundancy fusion information may restrict expressive capacity neural network drawing inspiration sknets dynamic selection mechanism introduced promote expression ability network therefore mlff module increase flexibility feature flow reduces information redundancy fusion improves performance model method decrease number model parameter produce better effect simple cascade aggregation method structure mlff shown fig including cross-layer flow detail u-net three-layer characteristic flow figure structure mlff full size image shown fig respectively represent layer1 layer3 proposed model mlff module includes two stage fusion selection fusion stage undergo convolution wavelet transformation respectively number channel controlled convolution conv feature size controlled wavelet transform result characteristic information represented subsequently feature element added together obtain fused output selection stage obtained global average pooling gap feature information changed channel convolution layer used change feature vector compact feature vector feature information change c/8 feature vector passed convolution layer three parallel ascending channel obtain three feature vector feature size activation function softmax applied obtain activated subsequently point multiplication operation respectively employed adaptively calibrate feature map finally calibrated feature fused obtain mlff output output expression mlff shown mlffout=s1\times l1+s2\times l2+s3\times model structure show three mlff module namely mlff1\ mlff2\ mlff3\ module differ fusion part different layer different feature transformation feature fusion subsequent selection part mlff1 mlff2 mlff3 represented mlff1=e1\times conv\left e2\right conv\left e3\right mlff2= conv\left e1\right +e2\times conv\left e3\right mlff3= conv\left e1\right conv\left e2\right +e3\times mlffi\ represents output mlff layer model conv convolution kernel used adjust number channel facilitate operation wavelet transform represents feature information level size obtained wavelet transform represents feature size level obtained iwt respectively represent point multiplication addition operation feature element represents mlffi\ fusion multi-layer feature information obtained selection stage activation specifically jth feature component value dmrfab module cnn convolutional kernel process entire image uniformly without focusing specific area attention mechanism ignore certain irrelevant regional information focus key area image learning different method proposed dmrfab module includes dense multi-receptive field module introducing sam cam help multi- receptive field block better extract deep feature information improve feature representation capability ultimately improve module deblurring performance dmrfab module illustrated fig comprises four mrfab unit bottleneck layer mrfab unit responsible extracting semantic feature image bottleneck layer reduces number feature input enhancing model efficiency compactness dense connection enhances transmission image feature make effective use image feature dmrfab shown =g\left\ i-1 i-1 indicates feature map made dmrfab layer series represents converting multiple input tensor single tensor represents output bottleneck layer super parameter bottleneck layer filter size used bottleneck layer structure mrfab used dmrfab module shown fig figure dmrfab module full size image figure structure mrfab full size image shown fig input information firstly input convolution block using convolution kernel feature information four feature extraction branch extracted utilizing convolution kernel extensional rate connection operation fuse parallel feature map five branch shown feature information fused connection operation directed two module module responsible implementing cam module implement spatial attention mechanism output module individually point-multiplied fused information subsequently connection operation applied finally input processed convolution block using convolution kernel described step serf fuse reduce dimensionality feature information represents feature map connecting different branch receptive field represents input mrfab represents operation module represents operation module represents connection operation represents output mrfab r=cat\left array array 1\times 3\times d=1 1\times 3\times d=2 1\times 3\times d=3 array 1\times 3\times d=4 x\end array m=cat\left ca\left r\right sa\left r\right r\right 1\times module map relationship feature channel compression excitation operation structure module depicted fig take input feature map dimension first employ global average pooling gap compress dimension resulting feature vector dimension encodes global context information incoming feature two convolutional layer followed sigmoid activation function module ultimately produce feature size figure structure module full size image module mainly spatial correlation feature structure shown fig figure structure module full size image module transform input multi-dimensional feature one-dimensional feature map spatial characteristic correct incoming feature information module take input feature map dimension utilizes maximum pooling global average pooling combine result form feature map dimension following convolutional layer sigmoid activation function applied generate output feature map fout dimension mathematical expression module represented fout=sigmoid conv\left cat\left maxpool\left g\right avgpool\left g\right indicates input characteristic maxpool indicates global maximum pooling operation avgpool indicates global average pooling operation cat indicates connection operation conv indicates convolution operation sigmoid indicates activation function result dataset training detail present study gopro real blur datasets used experiment gopro dataset composed clear blurred image pair including different scene total image pair used training dataset pair image used test dataset real blur dataset large-scale dataset blurry image clear image real world composed two subset image content first subset consists image obtained directly camera representing original unprocessed image second subset comprises image generated processed camera image processor scenario low light condition including nighttime indoor setting weak lighting scenario encompass typical real-world scene dataset contains total pair image captured various scene real blur dataset serf valuable research data evaluating image deblurring technique based deep learning model real-world setting experiment image pair dataset allocated training proposed model remaining pair reserved testing evaluation strengthen generalization ability model data enhancement operation performed gopro real blur training datasets operation included random rotation adding gaussian noise specifically data augmentation included random flip horizontal left right vertical upside direction well rotation angle degree gaussian noise also introduced average value variance 0.0001 result gopro training dataset expanded image pair image pair real blur training dataset grew image pair 15,032 image pair augmentation technique order prevent model overfitting image training datasets randomly cropped size pixel training period set round initial learning rate set 1e-4 halved every round batch size also set adopted network optimization method adam parameter 0.9 0.999 expedite experimental training model trained using gpu well-suited computationally intensive image processing task experimental environment configuration detailed table employed present research table experimental environment configuration full size table design comparative analysis loss function design loss function mse mean square error allow difference predicted actual value obtained widely used loss function model evaluation present study mse used model training mapping function shown d\left b\right =\widehat indicates mapping function indicates blurred image represents image restored model mse loss function shown mse =\frac i=1 d\left represents mapping function obtained learning model represents label image represents learning parameter network model represents pair image inputted network training dataset latest research shown auxiliary loss besides loss image content image enhancement task common approach involves minimizing distance loss input output feature space technique extensively adopted proven effective achieving improved result still gap real image restored image especially frequency domain widely known since purpose image deblurring recover lost high-frequency component information difference frequency domain space reduced therefore frequency reconstruction loss function frlf proposed present study address aforementioned problem frequency reconstruction loss defined distance real image blurred image measured frequency mathematical expression shown frlf represents fast fourier transform fft transforms image feature spatial domain frequency domain represents potential image restored model represents label image represents sum logarithm image input fft hence loss function used model training present study ltotal shown experiment parameter set 0.1 total mse +\lambda frlf comparative analysis loss function order verify validity proposed frlf compared two related loss function ssim loss function perceptual loss function function applied proposed model tested using gopro test data set loss function loaded using strategy represents different loss function balance output loss function mse different parameter assigned different loss function performance different loss function proposed model shown table table performance comparison loss function full size table total mse +\lambda table show compared mse loss function introduction loss function could improve performance model ssim loss function psnr increased 1.37 ssim increased 0.004 perceptual loss function psnr increased 1.85 ssim increased 0.013 introduction frlf delivered significant improvement led substantial psnr increase 2.07 considerable ssim increase 0.017 frlf particularly effective aid model recovering high-frequency image detail reducing frequency gap consequently conclusion frlf straightforward yet highly effective loss function improving model performance evaluating indicator using psnr ssim evaluation indicator proposed model quantitatively analyzed using gopro real blur test datasets compared model cnn multi-scale cnn spatially variant rnn improved u-net model car gan attentive deep network bdirnet model analyzed using test datasets proposed model table show result indicating proposed model performed well term psnr ssim indicator gopro table performance comparison various method gopro full size table rigorously ass performance proposed model series experiment conducted real blur test dataset model result quantitatively compared existing model including cnn spatially variant rnn car gan bdirnet result shown table indicating proposed model performed better term psnr ssim indicator real blur table performance comparison various method real blur full size table model parameter efficiency analysis thoroughly analyze scale efficiency proposed model comparison made running time model parameter size contrast method restoring image using gopro test dataset comparison result shown table time represents run time required model size represents model parameter size shown table proposed model took time smaller model parameter cnn multi-scale cnn srn-deblurnet bdirnet took longer parameter improved u-net finding could attributed model adding module increasing calculation amount compared therefore proposed model certain advantage parameter scale efficiency table comparison model running time model size full size table discussion visual analysis provide validation proposed model deblurring effectiveness section present visual result gopro test dataset real blur test dataset evaluation conducted comparing deblurred image produced model original clear image serving reference ass analyze quality image deblurring achieved model figure show visual effect comparison gopro test dataset image displayed left-to-right sequence showcasing different aspect deblurring process starting left magnified view initially blurred image next original clear image serf reference evaluation subsequently deblurring result various model displayed restoration image obtained using cnn shown followed result deblurgan srn-deblurnet car gan finally far-right image represents restoration achieved proposed model visual comparison allows direct assessment deblurring effectiveness method gopro test dataset providing valuable insight quality image restoration shown fig restoration image generated cnn juxtaposed blurred image method exhibited degree blurring reduction capability however still manifested noticeable artifact resulting insufficient deblurring effect comparison cnn deblurgan demonstrated enhancement deblurring effectiveness entirely free artifact visual representation srn-deblurnet showed lack conspicuous artifact yet suffered blurriness ground-level detail car gan exhibited superior artifact removal capability fall short restoring heavily blurred region attentive deep network similar deblurring effect method size slightly larger contrast proposed model excelled image deblurring offering notable result capturing extensive image information achieving clearer image restoration displaying reduced susceptibility artifact capability effectively mitigated interference artifact factor figure visual effect comparison gopro test dataset full size image figure illustrates visual comparison real blur test dataset left right sequence includes locally magnified blurred image clear image restoration image generated cnn spatially variant rnn srn-deblurnet proposed model respectively figure visual effect comparison real blur test dataset full size image depicted fig restoration image produced cnn exhibited improvement blurred image nevertheless remained considerably blurry noticeable presence artifact multi-scale cnn achieved superior outcome compared cnn resulting clearer restored image mitigated impact artifact however artifact still present restored image srn-deblurnet bdirnet demonstrated superior artifact removal capability comparison multi-scale cnn resulting clearer restored image however restoration effect area severe blurring ideal proposed model performed well real blur test dataset achieving effective image restoration minimal artifact nonetheless still room improvement compared clear image module performance analysis ass effectiveness proposed module five model experiment conducted model1 excluded dwt iwt model2 omitted dmrfab module alone model3 excluded module model4 mlff module included model5 integrated proposed module result summarized table model2 image evaluation index psnr reached 30.07 ssim reached 0.928 suggests inclusion dwt iwt contributed improved performance model2 transformation effectively captured contextual textural information across different image frequency upon introducing dmrfab module model3 psnr increased 31.28 ssim improved 0.941 demonstrates effectiveness dmrfab module enhancing deblurring capability model4 introduction module led psnr 31.35 ssim 0.943 attributed synergy attention mechanism convolution prioritizes global information selects crucial feature information turn enhanced model deblurring ability finally model module incorporated psnr reached 31.53 ssim rose 0.948 underscore positive impact proposed module enhancing quality restored image combination module improved feature extraction facilitated feature reconstruction aided model learning mapping relationship blurry clear image table quantitative evaluation result different model full size table conclusion paper image deblurring method based u-net model proposed mlff module dmrfab module designed mlff module integrates feature information different layer u-net network change inherent information flow mode conventional u-net network integrates feature information different scale network extract feature information dmrfab introduces spatial attention mechanism channel attention mechanism explores relationship different feature channel spatial relationship different feature overcomes shortcoming single attention mechanism obtains information important part feature obtains deep feature thereby improving effect blur removal additionally fft introduced loss function obtain frequency value image reduce frequency difference image improve effect deblurring average psnr average ssim value gopro dataset 31.53 0.948 respectively real blur dataset 31.32 0.934 respectively higher method therefore present method produce better deblurring effect future work focus center refining model entail effort enhance lightweight characteristic thereby optimizing performance commonly used mobile device