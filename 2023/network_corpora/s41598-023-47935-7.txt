introduction continuous development society new challenge put forward cultivation talent make people pursue high-quality education also continuously raising high requirement educational efficiency hoped within limited time student longer limited time space reduce unnecessary time investment learning high-quality knowledge made online education develop rapidly recent years.compared offline education traditional education online education certain particularity offline education teacher student communicate face face make accurate judgment degree knowledge mastery however online education environment student teacher often communicate network environment quality education often high offline education online education shown fig rely excellent performance deep learning establish knowledge tracking task student past learning trajectory analyze learning trajectory model judge degree knowledge mastery student figure knowledge tracking model full size image since google translate team proposed transformer self-attention mechanism structure received extensive attention field natural language processing computer vision due excellent performance gradually replaced dominant position cyclic neural network model transformer play important role time series prediction restriction model input weight matrix generated attention mechanism solve problem poor interpretability well.however analysis found due large number continuous repeated training problem data set cause transformer assign high weight small number discontinuous learning record calculating weight matrix certain extent affect model discover potential connection different knowledge point paper serf inspiration transformer incorporates convolutional neural network improve attention mechanism perception contextual information issue forgetting process human learning long focus psychological inquiry ebbinghaus university berlin germany created forgetting curve graph indicating human memory decline time rate decline constant order express process forgetting specific number analysis selected three feature calculate forgetting probability interval time last training interval time topic interval time knowledge point one-to-many relationship topic knowledge point knowledge point often distributed different topic contribution work summarized follows addition recurrent neural network rnn model previously used deep knowledge tracing dkt use transformer model foundation model process sequence length model highly interpretable model weight matrix help better understand relationship different knowledge point association overcome problem data set use convolutional neural network cnn perform convolution processing input data attention mechanism better perceive potential connection different knowledge point addition simulation forgetting behavior brain learning process use forgetting behavior one reference standard prediction line human learning process related work section provides brief overview several knowledge tracking modeling method widely used previous research bayesian knowledge tracing bkt model one widely used knowledge tracing model bkt represents learner knowledge status set binary variable represent whether student mastered mastered certain knowledge point training session bkt update probability binary variable using hidden markov model hmm bkt considered method choice field past year improvement made original model variant bkt variant logistic regression item response theory performance difference bkt bkt variant negligible although bkt significant success field also significant problem state student represented well binary variable model individual knowledge concept thus ignoring relationship different knowledge concept deep learning based knowledge tracing dlkt dkt model proposed piech pioneering work field dlkt well basic model rnn foundation dkt rnn memory-based sequence model sequence structure allows conform recency effect learning preserve learning trajectory information .rnns including variant long short term memory lstm gated recurrent unit gru widely used model field dlkt dkt take student learning interaction record input transforms vector input model using one-hot encoding compress sensing dkt rnn hidden state interpreted student knowledge state hidden state passed activated linear layer produce series prediction result length resulting sequence equal number question element represents student predicted probability correctly answering corresponding question although dkt outperforms existing classical method term predictive performance criticized small number scholar due practicality educational application primarily due fact hidden state inherently difficult interpret knowledge state dkt model doe conduct in-depth knowledge interaction analysis resulting poor interpretability context-aware attentive knowledge tracing akt model based self-attention mechanism achieved cutting-edge performance demonstrates utility self-attention mechanism vaswani later employed self-attention mechanism instead rnn entire model framework built transformer model proposed long-term dependency issue transformer model independent rnn framework originally transformer model used machine translation task produced good result later researcher applied knowledge tracking obtained result comparable dlkt model based rnn long sequence dependency problem pandey pioneered use transformer model knowledge tracking proposing self-attention knowledge tracing sakt model choi believed sakt model attention layer shallow proposed deep knowledge tracing transformer saint model address issue enhanced transformer structure including structural information question time information answer model architecture section component model detail definition commonly used data knowledge tracking included positional encoding construct used keep data position information safe demonstration convolutional layer attention mechanism work well weight matrix forgetting factor problem definition generally speaking task defined following form student-learning interaction sequence defined =\left\ number interaction learning interaction usually represented questionâ€“answer tuple mean student answered question moment indicates circumstance answer 0,1 indicates whether question answered correctly mean correct answer mean wrong answer model predicts probability answering next question correctly correct|q_ position encoding rnn sequential structure model already includes positional relationship element transformer completely different model rnn replaces rnn attention mechanism result transformer lack position information model unable determine relative absolute position element sequence position information critical knowledge tracking process training position knowledge point may critical prediction result important must include positional relationship input vector model understand positional information knowledge point input sequence figure depicts original method transformer used position encoding position encoding give history sequence length represents dimension represents distance constant represents position interaction history sequence represents vector corresponding position dimension vector function production position vector defined follows array f\left array if\ if\ array array defined follows array 10000^ 2k/1 array figure tcfkt model construction full size image calculated position encoding combined original data encoding model finally learn dependency position combination method follows array embedding\left position array convolutional layer cnn unique advantage speech recognition image processing special structure local weight sharing layout closer actual biological neural network weight sharing reduces complexity network two main reason adding convolutional layer analysis data set found many student trained large number knowledge point continuous period time know whether caused fact fewer knowledge point real data set student habit question shown fig student answered 6th knowledge point time row student answered 8th knowledge point time row student answered 4th knowledge point time row figure student training record full size image transformer dot product operation problem insensitive context awareness pay attention small number knowledge point appear calculation process example student learns question first question addition operation last question multiplication operation transformer often ignores relationship last question previous question sequence relationship noticed multiplication simplified addition related knowledge system order solve two problem first process encoded data convolution layer connect processed data attention mechanism specific processing process follows array x\left w\left array input data convolution kernel convolutional network attention mechanism query key value embedded vector concept attention originate information retrieval system query match others input information guiding role including information need key matched content information represents information matched value information extracted information simple expression information input feature process student input sequence get obtained multiplying three different weight matrix obtain v\right\ follows array array array array array array input data processed feature vector obtained first feature vector convoluted embedding vector generated encoding process enters attention first attention score vector calculated multiplication point order ensure stability gradient requires normalization attention score next attention score need activated softmax activation function activation result multiplied obtain weight matrix weighted input vector defined follows array attention\left softmax\left qk^ array weight matrix obtained sent feed-forward neural network layer first layer feed-forward neural network relu activation function second layer feed forward network ffn specific definition follows array ffn\left zw_ array forgetting factor one oldest problem experimental psychology forgetting ebbinghaus proposed forgetting curve show people forgetting change time time pass speed forgetting decrease eventually becoming slow process relatively stable level result examine student forgetting behavior learning process using three variable interval time knowledge point interaction interval time topic interaction interval time adjacent topic interaction one-to-many relationship topic knowledge point knowledge point often distributed different topic three time factor forget behavior defined array f\left array f\left p\right understood amount forgetting student certain moment consists three part calculation shown fig figure forgetting factor principle full size image define interval current question previous question n-1 interval knowledge point contained current topic previous knowledge point c-1 interval current topic previous topic q-1 experiment part selected data set real world verify effect model compared three excellent model experimental result show model better model performance result dataset shown table conduct experiment using four real-world public datasets assistments assistments kdd static table experimental data full size table assistments dataset tutoring history gathered online tutoring platform long one commonly used datasets field knowledge tracking assistments collected 708,631 learning record 19,917 student record contained knowledge point 47,124 knowledge point-based question assistments result competition assistment total 942,816 interaction 1,709 student recorded every record knowledge point 3,162 question made knowledge point interaction informative dataset term number interaction kdd dataset contains rich skill interaction information interaction student computer-aided tutoring system kdd made 173,113 interaction record student contains knowledge point question static data set training record collected university course mainly includes field engineering mechanic data set record total 4,000 learner knowledge point 20,000 learning record question result analysis also conducted comparative study weight matrix generated transformer see cnn improve perception ability weight matrix knowledge point appear small amount shown fig analyze weight matrix fig combining original data knowledge point low frequency position visualization brightness relatively unbalanced transformer weight matrix well matched knowledge point weight distribution becomes balanced cnn fusion indicating cnn processing correlation knowledge point better discovered.to test whether forgetting representation method capture student forgetting process took consecutive training record real data set calculated forgetting amount topic time point using forgetting representation method figure depicts end result.each row represents student learning record forgetting probability calculated calculating forgetting probability topic learned time point different color represent different forgetting probability figure comparison weight data convolution full size image figure forgetting factor calculation result full size image see analysis amount forgetting using student record first row example amount forgetting calculated student first training ninth question color indicates amount forgetting relatively high 9th question appears second time amount forgetting decrease 9th question retrained short period time amount forgetting decrease even result consistent acceptance law forgetting furthermore calculating forgetting amount combine weight matrix generated transformer shown right fig improve model prediction accuracy even run extensive experiment real datasets ass tcfkt performance order provide consistent experimental result evaluate model performance using area curve auc metric experiment ass reliability progression tfckt predicting whether student answer correctly next time based previous learning sequence figure depicts experimental result several observation follows figure experimental result full size image four datasets tcfkt model highest auc tcfkt model achieves significant improvement 3.96 average 0.7871 assist17 dataset compared akt 0.7571 kdd dataset tcfkt model improves 3.71 average 0.8125 compared 0.7834 achieved akt.on static dataset akt dkvmn comparable performance 0.8265 0.8204 respectively tcfkt akt 2.12 improvement dkvmn result assist15 dataset show significant improvement 1.94 average auc 0.7621 akt auc 0.7701 tcfkt significant improvement 1.94 average four real datasets tfckt achieves best prediction result result demonstrates tfckt perform higher fusion experimental data via cnn basis transformer improve model accuracy via forgetting factor conclusion future work propose transformer-based knowledge tracking model paper initially inspired transformer model basis added convolutional neural network solve problem large number repetition student problem record original model overcome context using convolutional neural network data processing also detect insensitive question order better discover potential connection various knowledge point basis forgetting factor added key factor forgetting factor simulates people forgetting behavior learning process output result remaining model combined better predict result next time point finally run large number experiment multiple real-world datasets result show model interpretable performant however according finding learning order student knowledge tracking critical factor position encoding included transformer model calculates position interaction using fixed sine cosine may result position associated knowledge point lost result intend use complex position encoding method following step position encoding contain feature information