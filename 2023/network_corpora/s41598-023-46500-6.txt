introduction speech one basic mean human interaction implicitly convey feeling speaker listener expansion technique human–machine interaction system also developed system must able process speech improving interaction analyzing emotion hidden speech feedback intelligent system improved information obtained used make output system understandable people sentiment extraction useful area online support lie detector system customer feedback analysis like reason achieving accurate emotion extraction system benefit various practical aspect nevertheless significant part research field sentiment analysis focused text processing speech analysis received attention reason higher complexity speech analysis system significant impact noise background sound system performance achieving accurate system recognizing speech emotion requires solving several challenge complexity emotional state one first challenge emotional state include combination several basic emotional state reason recognizing basic emotion speech considerable importance hand extraction speech feature performed efficiently relationship verbal pattern different emotional state interpreted based relatively large number emotional state another existing challenge cause learning technique unable correctly classify speech emotional feature paper hybrid solution presented solve main challenge problem speech emotion analysis difference current research previous similar work investigated feature extraction classification method modulation analysis deep learning technique extract speech feature present new classification model based combination ecoc overcome challenge large number target class explanation contribution current paper summarized following case proposed method article combination stm entropy feature speech signal describe emotional feature dimension feature reduced using cnn feature extracted cnn contain relevant feature emotional state paper new classification model based combination ecoc presented effective solving classification problem large number target class classification model includes several trained based ecoc matrix based current knowledge author two method used previous research considered new approach fill existing research gap rest paper organized follows section research background research background studied section research method description proposed method provided section result finding research discussed finally section discussion conclusion contain discussion summary research finding respectively research background speech emotion recognition done artificial neural network ann support vector machine svm since effect feature dimension reduction carefully evaluated effect dimension reduction two model compared feature casia chine emotional corpus dataset extracted deep neural network dnn classification method applied custom dataset recognize speech emotion mel frequency cepstral coefficient mfcc feature considered testing research using multiple linear regression mlr classification method combination mfcc feature seven emotion anger hatred happiness fear surprise sadness normal state reported recognition accuracy 82.41 mfcc feature widely used analyze speech signal employed recognize emotion speech set feature compared feature performed well speech emotion recognition system includes coefficient research long short-term memory lstm applied recognize emotion multifaceted self-care method proposed implement five convolution layer one attention layer using mfcc emotion recognition method applies functional descriptor output mfcc signal increase recognition accuracy combining feature based part model trained iemocap dataset considers four emotion happy sad angry neutral dnn hidden markov model hmm classification method using two mfcc epoch-based feature compared four emotional state happy sad angry normal iemocap dataset emotion recognition performed four datasets including shemo dataset feature used research mfcc formant prosodic feature lowest highest pitch pitch feature well combination three feature however paper recognition two emotional state neutral anger tested aggressiveness person speech state mobile phone voice discussed examines feeling anger fear cause stress speech available various datasets berlin emo-db recognizes stress person speech expression method mfcc-based feature functional feature low-level feature used describe feature speech signal svm used classify feature emotion recognition speech performed six datasets including shemo shown classification method made sensitive dependent small number phonetic label clustered k-means method feature component ignored specific phonetic component taken mfcc feature recognize speech emotion dnn svm classification method advantage reduction dimension reducing calculation execution time cost doe achieve high accuracy addition speech sentiment analysis done using type data example issue emotion evaluation interaction level blended learning discussed eeg brain signal used detect emotion method presented analyzed sentiment text also method presented discus topic emotion recognition long dialogue research method section describing specification datasets used research detail proposed method emotion analysis speech signal datasets presented data acquisition research speech signal available shemo berlin emotional datasets used input experiment following specification mentioned datasets described first obtained result analyzed data set used berlin dataset speech signal six emotional category subset including speech signal set used proposed method selected speech signal without background speaker include male female gender emotional state dataset anger sample hatred sample fear sample joy sample sadness sample average length speech signal dataset equal 3.051 also sample dataset recorded two speaker one male one female hand shemo dataset speech signal collected radio show dataset total sample length min dataset gathered persian speaker male female includes six basic emotion anger sample fear sample 3—happiness sample neutral state sample sadness sample surprise sample due shortness significant number sample dataset sample length one second ignored following process sample removed dataset result experiment conducted research performed sample shemo dataset experiment cross-validation technique used iteration iteration sample used training learning model remaining used testing proposed method emotion analysis speech model recognizing speech emotion generally include pre-processing feature extraction classification step process great importance resulting combination able efficiently extract emotional state highest possible accuracy proposed method recognition speech emotion done using following calculation step fig preprocessing speech signal feature description based spectro-temporal modulation entropy feature extracting feature using cnn classification based ecoc figure flowchart proposed method full size image according fig proposed method start preprocessing speech signal including converting mono-channel converting frequency sampling signal normalization second step feature description performed using two set feature entropy spectro-temporal modulation two technique process normalized signal independently extract feature describing signal feature extracted spectro-temporal modulation reduced using cnn third step proposed method extracted vector containing significant emotional feature speech vector merged entropy feature recognize emotion speech fourth step proposed method purpose detection model based combination ecoc used classification model includes set trained based ecoc code matrix rest section detail step proposed method described preprocessing speech signal proposed method start preprocessing input signal step includes three main step converting mono-channel converting signal frequency signal normalization purpose pre-processing step remove redundant data audio signal convert signal standard intermediate form prepare use next step purpose process audio signal normalization audio signal sampling transformation two-channel signal mono-channel vector form used way beginning pre-processing step two-channel nature input audio signal checked signal two -channel convert mono-channel signal since audio signal recorded different condition different device therefore frequency input signal may different signal reason second step pre-processing phase vector input signal converted frequency khz end pre-processing step signal vector converted vector zero mean unit variance special condition signal high low volume eliminated much possible feature description based spectro-temporal modulation entropy feature second step speech signal described entropy feature spectro-temporal modulation two technique process pre-processed signal independently extract descriptive feature signal described extraction entropy feature first feature set used describe speech signal feature entropy feature proposed method two type feature approximate entropy sample entropy used describe general characteristic speech signal approximate entropy describe speech signal based feature certainty chaos randomness high approximate entropy value indicates disorder high level chaos signal low approximate entropy value indicates repetitive change signal hand sample entropy modified version approximate entropy used evaluate complexity physiological speech time series signal compared approximate entropy criterion two advantage independence length data relative implementation without problem calculation entropy value explained therefore omitted deal detail calculation feature feature description based spectro-temporal modulation second set feature utilized describe audio signal feature spectro-temporal modulation technique also performs feature description process using auditory modeling includes two basic processing step modeling human auditory system generating temporal modulation based auditory spectrogram first step human auditory system modeled speech signal converted neural pattern i.e. time–frequency distribution along tonotopic axis logarithmic frequency obtained applying three stage transformation input signal second stage temporal modulation content obtained applying wavelet transformation line modeling human auditory system process modeling human auditory system consists three main step based initial stage sound processing human first step constant transformation applied input signal transformation done using filter bank filter ratio central frequency resolution always constant value proposed method overlapped filter used whose central frequency linearly uniformly distributed properly distribute filter logarithmic frequency axis divided following four octave range octave octave octave octave mentioned filter distributed logarithmic frequency axis way cover four octave consider logarithmic frequency filter bank impulse response filter expressed cochlea f\right considering impulse response caused filter input speech signal output cochlear filter calculated follows cochlea cochlea represents twist time domain way calculating output cochlear filter first stage modeling auditory system completed second step output obtained previous step i.e. cochlea converted auditory neural pattern hair cell using process cochlear output modeled intracellular pattern transformation implemented following step first output obtained filter derived respect high-pass f\right action work high-pass filter applying non-linear compress function output obtained previous step ion channel modeled compress function .\right defined follows gh_ 0.5 finally using low-pass filter c\left t\right 0\right output hair cell auditory system modeled filter frequency higher 4.5 khz passed filter three stage described second step modeling auditory system described following relationship gh_ cochlea f\right represents auditory neural pattern obtained speech signal processing next applying lateral inhibitory network lin discontinuity response along logarithmic frequency existing auditory neural pattern determined lin simulated first-order differential term logarithmic frequency using half-wave rectifier follows ylin\left max last step process modeling human auditory system integrate result relationship ylin\left f\right short range follows midbrain u\left represents unit step function defines short time constant range ms. explanation y\left f\right calculated follows y\left ylin\left midbrain described process modeling human auditory system shown diagram fig output matrix resulting step described example shown lower part fig figure auditory system modeling diagram full size image temporal modulation generation based higher level human central auditory system especially main cortex auditory system analysis performed estimating signal content order model human auditory system perception temporal modulation proposed method modulation dimension analysis used order provide detailed view spectro-temporal feature speech signal previous work show using logarithmic frequency vector along discriminator best mechanism achieved modeling human auditory system perception temporal modulation way applying continuous wavelet transformation line standard spectrum effect modeled efficient way proposed method instead using standard spectrogram used input modulation dimension analysis step modulation dimension analysis process consists two main step first given coefficient wavelet filter applied time row y\left applying output obtained cochlear channel filtered order reduce complexity increase computational efficiency wavelet filter simulated filter bank including set gabor filter filter adjusted different value spectro-temporal parameter low high speed based hertz modulation rate determined set gabor filter r=\ 2,4 8,16,32,64,128,256 noted output obtained step speed-time–frequency criterion analyze input signal way obtained depicted form three-dimensional spectrogram term speed time frequency mentioned filter applied row process shown fig figure applying gabor filter row spectrogram full size image last step temporal modulation generating process based integrating three-dimensional spectrogram respect time spectro-temporal modulation achieved process implemented integration member set f\right two-dimensional model obtained term rate frequency obtained two-dimensional model called auditory temporal modulation process generating temporal modulation based provided diagram fig short process gabor filter applied row integral taken member resulting set respect time extract temporal modulation figure diagram temporal modulation generation step based full size image fig shown upper part temporal modulation diagram extracted shown lower part figure feature extraction cnn proposed method feature extraction done cnn model noted step applied spectro-temporal modulation feature entropy feature extracted speech signal transferred next step without change cnn structure used proposed method extracting speech signal feature shown fig design cnn model simplest possible structure feature extraction used examining different architecture cnn model feature extraction determined highest performance achieved using model two convolution layer two maxpool layer two fully connected layer figure proposed cnn structure feature extraction full size image according fig proposed cnn model doe necessary layer feature classification output last fully connected layer model considered extracted feature sample modulation matrix obtained previous step dimension applied input layer input cnn model two convolution layer consisting filter dimension respectively responsible extracting input pattern finally extracted pattern converted vector using two consecutive fully connected layer dimension feature matrix reduced feature vector extracted cnn model combined entropy feature sample finally used input proposed classification model classification based ecoc last step proposed method combination ecoc used recognize emotion speech ecoc model presented comprehensive method solving complex problem complete structure solving multi-class problem combination several binary problem used solve problem face recognition emotion recognition etc framework ecocs consists two step encryption decryption coding stage code word assigned class problem emotional state code word contains string bit bit indicates belonging non-belonging class corresponding bit given binary classification combining code matrix called code matrix created whose row correspond code created target class ecoc model binary classifier assigned column code matrix trained based binary code corresponding column different way create code matrix proposed method dense random algorithm used form code matrix 10\times log number binary gamma classification trained based strategy training based column code matrix hamming distance criterion used recognize emotional state new sample purpose first feature training sample processed classifier creates binary code output combining output binary string created matching row code matrix emotional state input sample determined purpose hamming distance mentioned binary string line code matrix calculated thus sample belongs class smallest distance following mechanism proposed model explained supervised method whose name derived similarity operator generalized gamma operator operator receives two binary vector positive integer like input return vector otherwise gamma operator operator etc defined definition operator given set 0,1 0,1,3 alpha beta operator shown table table operator full size table definition operator let y\in input column vector output -dimensional vector whose component calculated follows definition operator considering binary pattern input operator generates following non-negative integer output calculated follows definition pruning operator let two binary vector pruned displayed binary n-dimensional vector whose component calculated follows y||_ gamma operator requires binary vector input deal real vector integer method represent vector binary form needed work modified johnson-mobius code used full detail algorithm discussed process explained paper definition gamma operator gamma similarity operator take two binary pattern n\le non-negative integer input generates binary output following two state case output calculated according following equation array if\ mod\,2 otherwise array mod indicates remainder division case output calculated using y|| instead according following equation array y||_ mod otherwise array considering operator operator explained following assuming basic pattern 1,2 cardinality test pattern described case classifies experimental sample following calculation step step convert training base set binary form using modified johnson-mobius code value calculated component n-dimensional vector base set follows step stopping parameter max j\right step test pattern also modified using johnson-mobius code coded parameter used code original set obtained greater corresponding j\right coded bit step indicator base pattern converted two indicator one class another position class step initial value parameter set zero step =0\ calculating checked whether fundamental pattern initial weighted addition basic pattern calculated follows unique maximum value equal class corresponding maximum value assigned test pattern such\ that\ step value i\omega calculated component basic pattern step value weighted sum class calculated i\omega relationship represents cardinality base set class step one maximum value among different value increase one unit step repeated one maximum value termination condition met step unique maximum value among different class corresponding experimental pattern calculated follows such\ that\ step otherwise pattern assigned class first maximum value result implementation proposed method done evaluate performance using matlab 2016a software test effectiveness proposed method investigated term accuracy classification quality result compared previous similar work also speech signal available shemo berlin emotional datasets used input experiment specification datasets described section introduction research method evaluation process done separately datasets using tenfold cross-validation figure show result accuracy proposed method comparison method extracting sentiment berlin shemo datasets iteration cross-validation graph performance proposed method compared case learning model use extracted feature cnn model figure accuracy different method extracting emotion two datasets berlin shemo iteration cross-validation full size image based fig proposed method perform process recognizing emotion speech accurately iteration said generally accurate compared method since difference compared method type classifier applied method input feature therefore improvement accuracy obtained proposed method attributed appropriate performance classification model proposed method combination ecoc used order recognize emotional state combination correctly recognize emotional state berlin shemo datasets average accuracy 93.33 85.73 respectively higher accuracy proposed method berlin dataset come two factor first data set used berlin dataset includes emotional state number emotional state shemo equal increase complexity problem secondly number speaker shemo dataset much berlin dataset add complexity problem speaker may convey emotional state specific pattern speech fig average accuracy value proposed method method compared berlin shemo datasets figure comparison accuracy different method extracting sentiment berlin shemo datasets full size image fig confirms proposed method berlin shemo datasets lead increased recognition accuracy mentioned improvement attributed performance proposed classification hand combination ecoc decision tree closest result proposed method direct result using ecoc model overcome high complexity multi-class problem inspecting fig show proposed method two advantage recognition process first proposed method recognize emotion accurately method second accuracy variation range proposed method limited compared method high accuracy time limitation variation range considered strong point recognition system feature show reliability output generated recognition system condition shown fig two datasets berlin shemo figure box plot different method accuracy two datasets berlin shemo iteration cross-validation full size image fig box represents variation range accuracy time cross-validation middle circle indicates median value accuracy iteration part box show one quartile accuracy change outlier also drawn point outside box based plot proposed method compact box higher level method confirms effectiveness proposed method accurately recognizing emotion speech fig show confusion matrix proposed method recognizing emotional state berlin shemo datasets numbering class confusion matrix based order class explained beginning section example number berlin dataset indicates state hatred tag shemo dataset describes emotional state fear row confusion matrix represent output learning model recognized emotional state real emotional state sample displayed column matrix way element main diameter matrix represent number sample emotional state correctly recognized element represent number error classification also state confusion matrix emotion recognition combination ecoc decision tree method closest performance proposed method presented fig figure confusion matrix proposed method recognizing type emotional state two datasets berlin shemo fold cross-validation full size image figure confusion matrix combination ecoc decision tree emotion recognition dataset berlin shemo fold cross-validation full size image comparison fig show proposed method recognize emotional state higher accuracy method inspecting emotional state seen berlin dataset proposed method recognize sample class higher accuracy hand shemo dataset proposed method doe perform well recognizing sample related emotional state surprise acceptable performance recognizing emotional state relatively small number surprise class sample main reason ineffectiveness proposed method recognizing sample class addition high similarity feature emotional state state sadness shemo dataset also one reason class sample wrongly classified emotional state sadness fig received operating characteristic roc curve different method recognizing emotional state two test base presented curve value true positive rate tpr displayed change false positive rate fpr goal emotion recognition method achieve higher tpr value time reduce fpr therefore emotion recognition system higher level roc curve better performance considering number target class emotional state research problem two therefore draw curve value tpr fpr calculated different class time current emotional state considered positive class class negative class figure roc curve speech emotion recognition berlin shemo datasets fold cross-validation full size image graph depicted fig show proposed method achieve higher tpr lower fpr value tested datasets result confirm proposed method superior recognizing emotional state separately class roc curve fig shemo dataset show proposed method lower tpr initial point curve combination ecoc decision tree low tpr result inappropriate performance proposed method recognizing emotional state surprise discussed fig however proposed method worked well accurately recognizing emotional state result achieving higher level roc curve better check effectiveness proposed method precision recall f-measure criterion used precision criterion demonstrate accuracy system recognizing sample target class separately also recall criterion show ratio sample belonging target class recognized correctly finally f-measure used describe classification efficiency system harmonic mean precision recall criterion formulated precision recall measure precision recall precision recall since criterion consider classification task binary problem calculated target class separately therefore calculating criterion target class mentioned class considered positive class assumed negative equation refers number correctly recognized sample belonging positive class also demonstrates number sample actually belong negative class classified positive finally refers number positive sample incorrectly labeled positive fig efficiency proposed method compared method term precision recall f-measure criterion also numerical result related criterion given table figure classification rate speech emotion recognition berlin shemo datasets full size image table comparing efficiency proposed method classification model full size table comparing precision recall f-measure value fig table show proposed method successful separating emotional state class result confirm claim made article regarding greater effectiveness combination ecoc accurate recognition emotion speech hand comparison criterion value table also confirms model classify emotional state accurately previous method research conducted used shemo dataset recognize emotional state method limited recognizing two emotional state anger neutral therefore order compare effectiveness proposed method result research target class ignored emotion recognition done based sample belonging two class anger neutral method perform two emotional state anger neutral without separating gender sample 90.97 accuracy meanwhile using proposed method two emotional state separated accuracy 93.23 result show using proposed method recognition accuracy increased 2.26 compared method figure show confusion matrix resulting recognition two mentioned emotional state proposed method also fig accuracy proposed method compared speech feature extraction method figure confusion matrix proposed method recognizing two emotional state anger neutral full size image figure accuracy comparison proposed method feature extraction method recognizing two emotional state anger neutral full size image result clearly show using combination spectro-temporal modulation entropy feature proposed method accuracy recognizing emotional state increased compared previous method discussion using experiment conducted research attempted identify advantage limitation proposed strategy different aspect regard two different datasets include various emotion used conduct experiment order prove generality proposed strategy evaluation comparison proposed method term identification accuracy fig variation range fig showed combination ecoc perform better compared classification combination ecoc achieved higher accuracy value tested data set addition range accuracy change reported proposed method fold narrower method confirms higher reliability output model nevertheless examination confusion matrix fig two datasets shemo berlin showed proposed method successful recognizing emotion classifier examining roc curve fig classification rate fig proved claim higher precision value show model accurate assigning label target category sample superiority recall criterion confirms higher efficiency proposed method recognizing sample emotional state overall result showed proposed method performs better method kerkeni liou berlin shemo datasets superiority attributed two main factor first combination ecoc led formation powerful classification model identifying emotional state used resist increased complexity caused increase number emotional state secondly use stm made possible represent emotional state speech signal efficient way extraction stm feature cnn combination entropy feature effective increasing accuracy model least 2.26 result presented fig prove claim based result combination stm entropy feature lead higher accuracy compared conventional feature description model mfcc formants prosodic one limitation proposed method need higher computing power learning model result use several ecoc model caused processing time proposed method train learning model conventional method although increase processing time occurs training phase problem solved using parallel processing technique future work use feature extraction technique describe emotional feature speech investigated also combining ecoc model existing classifier achieve accurate emotion recognition system topic research conclusion paper new method presented recognize emotion speech using machine learning technique proposed method set entropy feature spectro-temporal modulation describe speech feature feature extraction done cnn also new model based combination ecoc utilized classify feature recognize emotional state two technique make possible recognize emotional state speech higher accuracy efficiency comparison previous work performance proposed method tested two datasets berlin shemo result compared previous similar work obtained result showed proposed method accurate solution recognizing emotional state speech could recognize speech emotion berlin shemo datasets average accuracy 93.33 85.73 respectively improvement least 2.1 compared compared method hand comparing performance feature extraction technique recognizing emotional state showed combining spectro-temporal modulation entropy proposed method accuracy recognizing emotional state 2.26 higher compared method