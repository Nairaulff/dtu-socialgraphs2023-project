introduction brain complex network structurally connected functionally interacting neural unit network structure essential collective computational capability neural unit investigating neural mechanism structure computation critical neuroscience provide insight improving artificial neural network system reservoir computing type artificial recurrent neural network utilized study neural information processing brain network standard reservoir computing fixed randomly connected reservoir network creates complex dynamic induced input system output obtained readout network unit readout weight modified output produce target time series explore effect network structure reservoir computing reservoir network based brain-like topological feature human connectome constructed study reported network structure enhance learning performance reservoir computing recently kawai proposed new type reservoir computing called reservoir basal dynamic rebasics comprises multiple module small random neural network recurrent weight gain large random network large size number neural unit exhibit chaotic behavior leading learning failure orbital instability small network size reduces chaoticity network activity generates limit cycle oscillatory activity even gain large found limit cycle isolated small network module show wide frequency spectrum orthogonal creating orthogonal basis rebasics learns reproduce target time series linear summation limit cycle exhibit excelled learning performance orthogonal basis rebasics indicates computational role network modularity significant feature brain network however module rebasics completely isolated intermodular connectivity observed brain network seamless spatial connectivity therefore contribution network structure rebasics computation must explored study investigates role locality connection brain network known composed substantial number short-range local connection relatively long-range connection connectivity result small-world topology supposed reduce wiring cost kawai introduced small-world topology reservoir network study impact local connectivity computational performance study found local connectivity suppresses chaoticity allows learning even large recurrent weight gain would otherwise cause chaos randomly connected standard reservoir however neural activity generated local connectivity remains unclear neural activity exhibit stable orthogonal limit cycle employed rebasics computation local connectivity structure expected bolster biological plausibility rebasics thus deepening understanding computational role local connection brain local connectivity used improve learning performance reservoir computing instance rodan tino proposed cyclic reservoir neural unit arranged cycle connected one way neighboring unit cyclic manner topology allows input information stored cycle providing large memory capacity appeltant implemented reservoir computing single nonlinear unit delayed self-feedback considered cycle reservoir using virtual unit dale evaluated computing quality reservoir network ring lattice torus topology concluded topological constraint like ring often exhibit limited behavior approach assume neural unit driven external input activity asymptotically depend input time series called echo state property study local connectivity intended reduce chaoticity unit activity essence rebasics self-organization mutually orthogonal self-sustained neural oscillation conventional reservoir computer assume neural unit powered external input rebasics autonomously generate time series without continuous input previous study demonstrated effectiveness modular structure rebasics computation suggest similarly achieved local connection examine effect locally connected neural network network activity self-sustained limit cycle rebasics computation conducted computer simulation locally connected rebasics neural unit arranged one-dimensional ring two-dimensional lattice locally connected one another experiment demonstrated locally connected neural network generates diverse stable limit cycle moderately orthogonal subsequently evaluated learning performance locally connected rebasics motor timing task learning task lorenz time series study result indicate learn long time series ten second performance inferior existing modular rebasics nevertheless result suggest locality connectivity neural network may contribute neural computation beyond reducing wiring cost result locally connected rebasics locally connected rebasics comprises neural unit arranged one-dimensional ring see fig two-dimensional lattice pattern see fig hereafter one-dimensional two-dimensional locally connected rebasicss referred rebasicss respectively unit connection neighboring unit incoming connection unit randomly chosen unit destination unit rebasics neighbor parameter unit vicinity destination unit rebasics shown fig connection weight unit randomly chosen gaussian distribution zero mean standard deviation scaling coefficient recurrent weight local connectivity expected reduce chaoticity network activity generate limit-cycle time series figure one-dimensional locally connected reservoir basal dynamic rebasics neural unit arranged lattice neighboring unit readout output obtained linear sum unit activation weight trained recursive least square green arrow full size image figure local connection pattern rebasics green destination unit receives connection yellow neighboring source unit model three parameter setting examined experiment right panel show one end lattice connected opposite end full size image unit receive input signal unit emit output readout unit readout weight represented green arrow fig modulated using recursive least square minimize error readout target signal thus weighted summation various limit-cycle time series reproduce target time series limit cycle orthogonal constitute orthogonal basis resulting excellent time-series reproduction capability computational principle common fourier wavelet transformation utilize trigonometric function basis network dynamic fig show five instance unit output time series rebasics n=50,000\ 10\ 20\ trajectory display range limit cycle different frequency waveform learn target time series single short input pulse width introduced unit start signal yellow thick line input pulse synchronized phase unit output different run coping varying initial condition eliminated initial state dependency allowed network generate limit cycle trajectory across trial figure time series five sample unit output green black curve indicate two run different initial unit state yellow area -51\ indicates period input pulse given full size image analyzed orthogonality unit output fig show average absolute inner product function distance unit inner product output adjacent unit large approximately 0.7 indicating low orthogonality interestingly even short distance nonneighbor unit orthogonality output increased approximately 0.37 however even longer distance unit inner product approach zero figure averaged absolute inner product unit output versus distance unit distance adjacent unit considered full size image fig illustrates orthogonality network activity rebasics various value neighbor parameter scaling coefficient recurrent weight evidently two highly orthogonal region found low bottom left high top right respectively former region restricted term locality connection smaller weight latter region relatively global connection larger weight figure orthogonality unit output color indicate averaged absolute inner product unit output averaged network low value blue indicates high degree orthogonality full size image evaluate instability network activity computed local lyapunov exponent lle rebasics term fig found larger resulted higher instability implying network activity became chaotic conversely smaller caused lower instability indicating locality connectivity could suppress chaoticity network activity based finding orthogonality analysis fig inferred area top-right quadrant fig exhibited extensive global connection characterized high degree chaos despite highly orthogonal figure instability network activity color indicate local lyapunov exponent unit output averaged network high value yellow indicates high degree instability full size image synthesize arbitrary time series time series various frequency required thus calculated frequency spectrum unit output rebasics fast fourier transformation fig show histogram peak frequency power spectrum 1.0\ 1.2 20\ found lower resulted low frequency peak reducing frequency variation fig show frequency spectrum unit output various 1.2\ output unit large showed wide range power spectrum small show low power high frequency orthogonality analysis fig indicates area bottom-left quadrant small characterized low frequency band despite highly orthogonal figure histogram peak frequency power spectrum unit activity network 1.0 orange 1.2 cyan recurrent weight scaling coefficient neighbor parameter full size image figure one-sided power spectrum time series unit output computed using fast fourier transformation averaged power spectrum output unit network full size image motor timing task motor timing task commonly used ass learning ability reservoir computing self-sustained neural activity task input system single pulse -51\ desired output single gaussian pulse peak specific interval end input pulse owing interval period without input output previous study demonstrated standard reservoir computing including echo state network force complete task successfully example readout output rebasics test trial interval shown fig readout output successfully replicated gaussian pulse input pulse notably despite time constant neural unit 10\ capable learning interval one minute r^2=0.50\ two minute r^2=0.25\ figure example readout output locally connected rebasics learning interval full size image evaluated learning performance using coefficient determination squared pearson correlation coefficient r^2\ system output target signal shown fig varied interval parametrically plotted r^2\ performance rebasics modular rebasics indicated green black curve respectively although performance rebasics inferior modular rebasics capable learning even long interval one minute figure performance interval performance averaged network mean s.d. green yellow black gray curve indicate performance rebasicss respectively modular rebasics randomly connected echo state network esns respectively full size image conventional reservoir computer represented randomly connected echo state network esns depicted gray curve typically driven external input faltered timing task due interval without input esns network size 1,000 spectral radius 1.0 similar result observed network size increased finding aligns existing study fig show timing capacity defined area r^2\ curve rebasics term best timing capacity obtained 20\ 1.2\ conversely timing capacity decreased top right region large 1.0\ left column figure timing capacity rebasics averaged network full size image fig show performance rebasics various network size green curve 50,000\ fig curve doe differ curve 40,000\ indicating performance saturated 50,000\ reducing resulted significant decrease performance however small network size doe mean learning fails example 10,000\ performance 10-second interval r^2=0.88\ figure performance rebasics various network size averaged network mean s.d. full size image fig yellow curve depicts result rebasics figure show performance rebasics lower rebasics fig show timing capacity rebasics green black yellow best timing capacity obtained 1.2\ figure timing capacity rebasics averaged network full size image learning lorenz time series fig present instance learning outcome rebasics applied lorenz time series period similar previous experiment input consisted single pulse time yellow line fig following input self-sustained limit cycle able produce time series resembling lorenz time series performance lorenz system r^2 0.96 0.025\ 0.91 0.031\ 0.87 0.031\ mean s.d network respectively result indicates rebasics capable learning non-periodic complex time series lorenz time series figure learning chaotic trajectory lorenz system time series show example system output green target signal black input signal yellow rebasics learned lorenz system full size image discussion experimental result indicate local connectivity contributes generation diverse limit cycle chaotic behavior facilitates rebasics computation involves reproducing time-series weighted sum using limit cycle basis self-sustained nature limit cycle allows perform motor timing task long interval without requiring input maximize learning performance locally connected rebasics balance stability diversity unit activity crucial large value resulted chaotic behavior leading breakdown learning conversely smaller value caused variation frequency limit cycle unit activity therefore essential generate range self-sustained activity degree would prevent chaos asymptotically stable regime referred edge chaos reservoir computing study utilized concept oscillatory edge chaos found degree locality connection crucial parameter determining edge well weight gain additionally model necessitated significantly neural unit network size conventional reservoir computer reduced lower value performance markedly declined suggest sizable network essential produce diverse orthogonal neural oscillation learning performance locally connected rebasicss found inferior modular rebasics could attributed insufficient orthogonality unit activity although unit distant owing local connectivity activity weakly correlated completely separate local connectivity modular structure typical feature brain network combining feature rebasics expected exhibit stable high performance wide range parameter rebasics originally proposed general model either corticostriatal system cerebellum two hypothesis fact local connectivity enables rebasics computation hypothesis support view former model reservoir network readout learning interpreted cortical network dopamine modulation striatum respectively another study demonstrated timing learning could achieved even supervised learning rebasics readout replaced reward-modulated hebbian learning well known majority cortical network consist local connection small-world brain network however considering existence long-range connection cortical network necessary investigate whether rebasics connection work well latter cerebellar model reservoir network readout learning interpreted granular layer network granule cell interneurons purkinje cell plasticity respectively model based cerebellar reservoir hypothesis tokuda proposed biologically detained cerebellar model golgi cell interneurons granular layer mutually connected via gap junction owing connectivity local anatomical fact consistent structure locally connected rebasics locality connectivity neural network supposed due wiring economy shorter axon costly addition economic brain view emphasize computational role local connectivity neural network includes suppressing chaoticity network activity generating diverse stable oscillation learn arbitrary time series finding lead better understanding neural information processing invention better recurrent neural network conclusion study experiment conducted using locally connected neural network revealed local connectivity reduced chaoticity network activity produced stable limit cycle activity leveraging oscillatory activity proposed locally connected rebasics model integrated activity approximate target time series self-sustained nature activity enabled system learn timing within interval period without input additionally demonstrated locally connected rebasics capable accurately learning complex time series including lorenz system importantly found local connectivity modular structure critical rebasics neural computation prominent feature brain network work bridge gap function/structure computation neural network provides better understanding temporal processing motor learning brain method dynamic state x_i unit time represented firing rate model aligned x_i -x_i j=1 rec r_j w_i^ i_i^ noise aligned r_i x_i represents activity level unit denote network size time constant input signal respectively w_i^ i_i^ noise denote input weight noise term respectively drawn gaussian distribution zero mean s.d one s.d i_0\ denotes connection weight unit exists connection value randomly chosen gaussian distribution zero mean s.d. denote recurrent weight scaling coefficient number incoming connection respectively otherwise topology recurrent network random high-gain regime large 1.0\ make network activity self-sustained chaotic although modular rebasics set 1.0\ generate self-sustained activity using small network size enables small network module output stable orbit limit cycle similarly locally connected rebasics using local connectivity expected reduce chaoticity network activity even 1.0\ generate limit cycle orbital stability unit activity exhibit limit cycle converge fixed point activity unnecessary learning randomly select unit converge fixed point remain active regard output unit set index output unit specifically unit difference 0.01 maximum minimum activity learning end time considered active unit value threshold empirically determined standard instance rebasics depicted fig 21,895 unit disregarded converged number active unit dropped simulation restarted outset scenario observed experiment oscillatory activity output unit r_k converted readout output linear sum given aligned w_k^ r_k aligned w_k^ readout weight trained using recursive least square method let denote weight activity output unit respectively vector length updated time follows aligned aligned aligned aligned denotes target time series matrix corresponds running estimate inverse sample covariance matrix updated follows aligned aligned initial value set 1/\alpha denotes identity matrix constant experimental setting numerical solution obtained using euler method simulation step size recursive least square method applied every two step training period set simulation begin time -250\ -2050\ simulation fig initial state unit set uniform random value range experiment input single pulse magnitude five -51\ zero period single-pulse input used initialize phase limit cycle unit activity reduce dependency initial unit state see fig unless otherwise stated parameter value listed table used rebasics parameter empirically determined optimal value based experimental result presented fig value determined experimental result shown fig performance approximately saturated value valid comparison modular rebasics parameter i_0\ matched proposed method therefore modular rebasics module consisting unit module two output unit resulting total 1,000 output unit table parameter value full size table rebasics set 52,900 230\ lattice one end lattice connected opposite end considered three parameter value neighbor parameter see fig number connection set parameter value used rebasics evaluation experiment coefficient determination squared pearson correlation coefficient r^2\ system output target end task used evaluate learning performance value averaged test trial different network evaluate instability network activity used lle estimated similar manner reference small perturbation given unit time pert 10^ logarithm distance unperturbed perturbed trajectory computed time normalized initial distance ms. aligned dist pert pert aligned log distance dist averaged trial different initial unit state lle defined slope function lle system unstable larger lle unstable chaotic system evaluated orthogonality network activity average absolute inner product unit output temporal vector time series 1,000\ normalized unit vector absolute inner product combination vector unit computed inner product averaged combination different network low value average absolute inner product indicates high orthogonality entire network activity motor timing task input single pulse time desired output single gaussian pulse peak specific interval end input pulse magnitude standard deviation gaussian pulse respectively interval set experiment system trained time interval plus rebasicss trained trial trial run end training period learning performance r^2\ evaluated using untrained test trial defined timing capacity area r^2\ curve interval learning lorenz time series used lorenz system target signal evaluate learning performance complex unpredictable time series lorenz time series obtained interaction three variable aligned aligned aligned aligned aligned aligned 10\ 28\ 8/3\ set 0.1\ obtained numerical solution using fourth-order runge–kutta method step size 0.001 time series downsampled 1/5 length normalized magnitude resulting three-dimensional target signal regarded one step motor timing task input single pulse rebasics three readout corresponding variable lorenz time series readout weight modulated using corresponding target time series number trial training testing learning motor timing