introduction label noise training data detrimental supervised training deep neural network although training data without label noise desired accurate training often impractical impossible attain especially medical setting attaining high-quality label minimal error would require pool expert labeling data subsequently consensus reading address disagreement impractical extremely costly matter exacerbated deep neural network requiring large diverse set training data consequently overcoming limitation label noise training neural network active area research definition label noise may seem obvious instance e.g. image either correctly labeled may imply definition label noise always evident however class arbitrarily defined especially real-world medical setting example medical diagnosis often dichotomous follow spectrum consequence borderline case difficult label typically subject observer bias furthermore definition label noise becomes increasingly elusive region-level classification pixel-level classification classification task label noise refers incorrectly labeled structure may also refer decision subject inter- intra-observer variability example semantic segmentation gross segmentation may correct considerable variation definition outline segmented structure making label noise inherently present semantic segmentation task mitigating negative impact label noise training deep neural network achieved multiple way comprehensive review methodology refer reader paper song overview application medical imaging refer karimi briefly method focus creating label noise invariant network noise-tolerant loss-functions data-cleaning re-weighting however method either buckle extreme amount label noise require subset data free label noise focus single classification task image-level instance-classification generally applicable state-of-the-art method use multiple network applied sequence parallel rely principle although neural network memorize label noise prioritize learning general pattern first sequential model follow paradigm related knowledge distillation jiang proposed approach one network mentornet used train another network studentnet goal mentornet learn sample weighting scheme i.e curriculum identifies correct incorrect instance studentnet learns eventual task work based curriculum learning unlike curriculum learning doe rely manually defined curriculum easy hard training instance automatically defines one however method relies small validation set clean label method use network trained parallel focus identifying label noise training method related active learning boosting bootstrapping instead employing one network use multiple network exploit different decision boundary i.e epistemic uncertainty mitigate negative effect label noise i.e. aleatoric uncertainty example malach proposed method employ two network training instance update network disagree similar approach recently proposed voting scheme used select reject sample three network however method typically ignore easy training instance may still include erroneously labeled training instance han overcome limitation co-teaching co-teaching two model independently trained using instance mini-batch different selection model selects training instance model dependent loss specifically instance evaluated per model pre-specified amount instance highest loss rejected i.e. forgotten thereby selecting instance lowest loss training model update technique proposed showing combination previous technique keep decision boundary network converging each-other training compared conventional co-teaching approach achieved marginal consistent improvement several classification experiment however limitation co-teaching suboptimal forget-rate-parameter considerable negative impact performance effective deployment co-teaching requires clean validation dataset least knowledge amount label noise issue experiment label noise synthesized problematic real-world task priori knowledge label noise hardly available infeasible attain propose method introduces stochasticity co-teaching selection correctly labeled rejection incorrectly labeled instance instead using predetermined forget-rate hyperparameter used conventional co-teaching employ random selection threshold posterior probability training instance approach loosely inspired effectiveness random threshold extremely randomized tree approach overcome shortcoming conventional co-teaching hamper utilization similarly regular co-teaching stochastic co-teaching relies two deep neural model learn two model identical architecture trained data however given model independently initialized independently optimized develop distinct decision boundary stochastically determined threshold used include exclude instance update network based ground-truth-label posterior probability benefit stochastic co-teaching doe require assumption expected amount label noise present several key-contributions paper first propose stochastically chosen posterior probability threshold selecting rejecting training instance co-teaching second present in-depth evaluation hyperparameters stochastic co-teaching show method robust varying degree label noise third show stochastic co-teaching additionally provides estimate amount label noise fourth show superior performance stochastic co-teaching benchmark experiment using mnist cifar-10 cifar-100 data finally using multi-label ecg classification semantic segmentation cardiac mri show method generic readily applicable real-world medical task method stochastic co-teaching co-teaching revolves around two model jointly trained whereby model selects instance train model since model initialized differently model learns different decision boundary resulting different selection training instance conventional co-teaching depends predefined forget rate tuned towards label-noise rate reject fixed number instance based largest loss severely limit application task amount label noise known beforehand instead propose stochastic approach doe reject fixed number instance exploit posterior probability select reject training instance analogous lowest highest loss used conventional co-teaching likely low-probability training instance labeled erroneously instance high posterior probability however unclear impact significant factor low probability associated challenging correctly labeled training instance question remains threshold optimally separate incorrect correct instance since assume prior knowledge exact ground truth therefore knowledge noise-rate either want exclude low-probability example hence use stochastic approach instance selected based threshold randomly chosen beta distribution algorithm show pseudo-code method stochastic threshold beta distribution threshold determine instance included randomly chosen beta distribution beta distribution defined interval aligned beta x|\alpha =\frac 1-x aligned beta function aligned =\int 1-t aligned ensures total probability parameter determine characteristic distribution non-zero positive number table list detailed property different value experiment chose parameter distribution uniform uni-modal note increasing value result narrower distribution experiment reveal approach benefit resulting narrow left-tailed distribution threshold randomly chosen distribution result high value allow selecting lower threshold thereby offering chance randomly select training instance low posterior probability may represent difficult incorrect case table characteristic beta-distribution defined parameter interval full size table numerical stability benefit selecting rejecting arbitrary number training instance mini-batches may come cost numerical instability instance mini-batch would rejected empty training set would generated would cause numerical instability combat employ two procedure first clamp randomly selected threshold 0.01 0.99 ensures rejection case posterior probability low ensures selection case posterior probability high note threshold would result inclusion instance threshold would result rejection instance thus empty set second monitor fraction selected training instance impose following selection criterion 10\ instance mini-batch selected generate new selection threshold five consecutive threshold satisfy selection criterion mini-batch may exclusively consist instance label noise new mini-batch sampled similarly gradually introduce stochastic co-teaching via schedule schedule introduces selection threshold gradually aligned beta aligned multiplication factor aligned n-n_0 aligned current time-step n_0\ time-delay start introduction gradually step rejection rate contrast conventional co-teaching stochastic co-teaching reject arbitrary number training instance calculating rejection-rate per epoch training stability monitored instability appears majority instance rejected training furthermore assuming training instance incorrect label rejected rejection-rate provides estimate label noise note rejection-rate equivalent term forget-rate used conventional co-teaching clarity use two term distinctly experiment result comparison previously published method performed baseline experiment synthesized label noise mnist cifar-10 cifar-100 datasets generate label noise employed two type noise transition matrix visualized fig bias noise also referred label-flipping noise mimic observer-bias substituting ground-truth label label neighboring class uniform noise achieved replacing ground-truth label randomly selected label direct comparison related method experiment performed similar i.e experiment high level label noise bias noise noise rate uniform noise noise rate note experiment bias noise exceptionally difficult marginal majority correct sample noise rate bias noise would flip majority instance incorrect label implemented two cnn architecture standard four-layer cnn mnist experiment nine-layer cnn cifar experiment former model specifically designed mnist highly efficient fast latter used experiment weak supervision noisy label efficient therefore slower table list architecture network network trained epoch mini-batches using stochastic gradient descent adam learning rate 0.001 regular co-teaching performed using optimal setting reported experiment stochastic co-teaching introduced ten epoch mnist experiment without delay cifar experiment delay ten epoch stated otherwise reported result corresponding standard deviation determined last ten epoch experiment experiment implemented using pytorch performed accordance relevant guideline regulation figure noise transition matrix used synthetic experiment transition matrix equivalent confusion matrix visualize noise distribution among class noise label-flipping noise mimic observer-bias substituting true label label neighboring class uniform noise achieved replacing true label randomly selected label note bias noise always ensure majority instance remain correct full size image table two network architecture used experiment mnist model relu activation function cifar model used batch-normalization convolution layer leaky-relu activation negative slope 0.01 full size table hyperparameter stability propose stochastic co-teaching approach randomly chosen threshold used select reject training instance based posterior label-probability performed extensive experiment investigate influence hyperparameters i.e hyperparameters determine different beta distribution sample instance-selection threshold figure show parameter used experiment corresponding beta distribution probability density show wide variety shape ranging uniform parabolic bell-shaped distribution symmetric right- left-tailed selection threshold stochastically sampled distribution used experiment figure variation used experiment result varying beta probability distribution full size image figure hyperparameter-sweep experiment showing accuracy average last epoch epoch figure show experiment performed using corresponding variation beta-distributions shown fig colorbar scaled specific range result full size image figure show effect different hyperparameters stochastic co-teaching different beta distribution different impact dominant hyperparameters pair achieve optimal result nearly experiment general distribution diagonal i.e right-tailed distribution show suboptimal result distribution diagonal i.e left-tailed distribution show optimal result right-tailed distribution sample lower threshold average meaning increased chance selecting instance low posterior probability left-tailed distribution sample higher threshold average meaning increased chance rejecting instance low posterior probability leaving mainly high posterior probability instance hyperparameters perform equally experiment cifar-10 cifar-100 image classification task generally considered challenging application narrower sweet spot hyperparameters left-tailed somewhat wider bell-shaped distribution appear optimal =32 optimum application nevertheless hyperparameters show relatively wide sweet spot different hyperparameters limited impact accuracy rejection rate stochastic co-teaching estimate amount label noise present data monitoring rejection rate figure show development rejection rate training several application figure observe accuracy converges rejection rate converges towards noise-rate cifar classification rejection rate overshoot challenging task higher noise rate however faulty estimation inferred deteriorating test validation accuracy figure stochastic co-teaching provides estimate label noise rate rejection rate test/validation performance converge rejection rate test accuracy shown image classification mnist cifar-10 cifar-100 classification experiment used beta-distributions parameter 32\ real noise-rates provided dashed horizontal line matching color full size image comparison method label noise-rate known conventional co-teaching outperforms competing method synthetic task reported han result experiment shown table one experiment stochastic co-teaching outperforms conventional counterpart benefit stochastic co-teaching method allowed parameter setting 32\ experiment addition rejection rate stochastic co-teaching provides insight label noise rate table comparison accuracy achieved stochastic co-teaching previous method fair comparison report result han well replication experiment report result related method decoupling mentornet report result standardly trained neural network standard co-teaching cot stochastic co-teaching stocot stochastic co-teaching chose left-tailed beta distribution parameter =32\ =2\ note performance increase cot experiment cifar experiment figure show random initialization impact ill-tuned forget rate full size table replication conventional co-teaching experiment achieved higher accuracy reported han inspect cause studied effect forget-rate hyperparameter effect different random seed used parameter initialization mini-batch sampling experiment used forget rate step repeated mnist experiment ten time different random initialization cifar experiment repeated experiment five time training network time consuming 0.5 vs. result shown fig reveal conventional co-teaching sensitive forget-rate hyperparameter noise-rate known forget-rate chosen equal suboptimal accuracy achieved concurring finding reported han additionally result demonstrate impact overestimating forget-rate larger impact underestimating particularly experiment cifar-10 cifar-100 furthermore different random seed resulted large range achieved accuracy specifically experiment mnist data figure accuracy conventional co-teaching using different setting forget-rate hyperparameter solid line show average accuracy dashed line show range accuracy calculated experiment per forget rate mnist experiment per forget-rate cifar cifar result demonstrate impact ill-chosen forget-rate different random initialization conventional co-teaching full size image real-world medical task show applicability method medical data employed stochastic co-teaching multi-label classification medical signal namely ecg semantic segmentation medical image namely cardiac cine mri like medical task data inherently contains label noise caused e.g. inter- intra-observer variability ecg classification ecg primary tool cardiologist ass cardiac condition patient typical ecg exam acquires second data using lead ecg characteristic sometimes automatically detected ecg thereafter manually assessed diagnosis manual interpretation task cumbersome often non-trivial presence pathology automatic interpretation ecg using deep neural network currently subject intensive research training network non-trivial ecg interpretation complex due label noise observer bias experiment apply stochastic co-teaching ecg classification using ptb-xl dataset full description data found briefly dataset consists 21,837 clinically acquired 12-lead ecg second bit 18,885 patient data divided ten fold equal size patient-level different feature diagnosis aggregated different class task posed non-exclusive multi-label classification following class normal conduction disturbance myocardial infarction hypertrophy st/t change divide fold training validation test data proposed training data label scrutinized expert therefore training data contains label noise validation test data checked expert considered contain minimal level label noise performed experiment using resnet adapted time-series top-scoring neural network ptb-xl data table present baseline result reported strodthoff replication baseline method result proposed stochastic co-teaching result show similar auc implementation baseline method stochastic co-teaching 0.913 0.917 respectively overlapping confidence interval note difference result reported strodthoff replication likely caused implementation difference implementation trained baseline network achieved accuracy 0.618 stochastic co-teaching accuracy increase 0.640 shown fig different hyperparameter setting show similar pattern performance shown fig higher performance diagonal table macro-averaged auc super-diagnosis i.e course-grained classification ecg ptb-xl dataset following strodthoff indicate confidence interval bracket bootstrapping test set first row show baseline result strodthoff second row show replication experiment last row show result experiment stochastic co-teaching stochastic co-teaching hyperparameter setting =32\ =2\ used full size table figure performance proposed stochastic co-teaching real-world medical task different hyperparameter setting horizontal axis vertical axis accuracy multi-label ecg classification ptb-xl dice score left ventricle myocardium segmentation cardiac mri cmr baseline ecg classification network trained without stochastic co-teaching achieved accuracy 0.618 baseline cmr segmentation network dice score 0.71 full size image cardiac mri segmentation experiment evaluate stochastic co-teaching left ventricle segmentation cardiac cine mri image image typically acquired evaluate cardiac function one primary indicator cardiac function ejection fraction ejection fraction fraction blood leaf heart contract calculated annotation endocardium inner wall left-ventricle two time-points end-diastole maximum expansion end-systole maximum contraction segmentation may seem trivial endocardium contains many papillary structure i.e protruding muscle tissue make task prone high intra- inter-observer variability moreover papillary muscle quite large may affect measurement inconsistently segmented however segmenting papillary muscle cumbersome experiment use publicly available mri image sunnybrook challenge dataset consists short-axis cardiac cine mri patient multiple slice acquired encompass heart image resolution .25 in-plane slice time-series frame visualizing one heart-beat dataset three structure annotated end-diastole end-systole approximately half image two largest papillary muscle annotated separate class included experiment divided image training set image test set image patient level test set doe contain image patient training set mimic segmentation error assigning papillary muscle blood pool image slice training set modify test-set performed segmentation experiment u-net one used architecture medical image segmentation network trained epoch using mini-batches containing randomly selected image patch pixel original image size augmentation random flipping random rotation around step degree implemented stochastic co-teaching select reject individual voxels training visualizing selection rejection voxels mask valuable qualitative information training revealed area label error area observer variability shown fig selection mask ignore border segmentation logical considering outline quite arbitrary furthermore selection mask include inner voxels papillary muscle correctly labeled exclude labeled meaning stochastic co-teaching effectively ignored incorrect label note single threshold might used select pixel-instances mini-batch chose generate selection threshold pixel however given generating random parameter time-consuming generate one 16-map per training image patch tile patch size figure show several qualitative example u-net trained without stochastic co-teaching result show method using stochastic co-teaching achieves consistent output method consistently segment papillary muscle result also show outline myocardium consistently quantitative result listed table demonstrate dice distance metric improve stochastic co-teaching applied compared standard training finally fig demonstrates robustness stochastic co-teaching towards segmentation task varying setting hyperparameters figure three cardiac mri training image first column original reference label second column synthetic label noise applied third column fourth sixth column show different selection mask generated stochastic co-teaching left ventricle myocardium indicated yellow blood pool red selection mask indicate selection white rejection black pixel training top row training example annotated papillary muscle hence synthetic label noise applied note pixel rejected exclusively along segment border pixel center papillary muscle selected middle row bottom row two training example synthetic label noise synthetic label noise added including pixel representing papillary muscle blood pool note selection map middle bottom row pixel representing papillary muscle rejected training example show method reject noisily labeled papillary muscle preventing overfitting correctly labeled papillary muscle first example see table quantitative result full size image figure example showing segmentation performance test-set standard cnn similar cnn stochastic co-teaching co-teaching show sensitivity towards segmentation papillary muscle even though segmentation excluded training data see table quantitative result full size image table cardiac mri segmentation left ventricle blood pool myocardium compared standardly trained u-net standard stochastic co-teaching stocot result higher dice lower distance metric owing robustness label noise star indicate p-values determined one-sided wilcoxon signed-rank test 0.01\ 0.001\ see fig qualitative result full size table discussion stochastic co-teaching employ two neural network jointly trained network selects mini-batch example using stochastically determined threshold posterior probability approach doe require priori knowledge label noise result eliminates need meticulous parameter tuning especially useful real-world task stochastic co-teaching achieves excellent result outperforms state-of-the-art approach variety classification task extreme unknown level label noise robust varying level label noise used estimate level label noise monitoring rejection rate furthermore demonstrated applicability method two real-world medical task classification ecg signal semantic segmentation cardiac mri image incorrect estimation noise rate may detrimental model performance fig show noise rate incorrectly estimated uniform biased noise case cifar experiment caused complexity problem poorly chosen however demonstrated priori unknown noise rate stochastic co-teaching requires relatively little hyperparameter tuning compared conventional co-teaching conventional co-teaching requires extensive tuning forget rate stochastic co-teaching forget rate automatically determined hyperparameters stochastic co-teaching pertain shape sampling distribution defined hyperparameter grid shown fig suggest performance consistent wide range setting problem similarly result cardiac mri segmentation problem demonstrate relatively homogeneous performance across different hyperparameter setting ecg classification result ptb-xl dataset show slightly lower fault tolerance respect choosing agree general trend beneficial choose observed mnist cifar note distribution could potentially chosen threshold sampling requirement distribution defined exclusively unimodal beta distribution satisfies additionally left-tailed distribution preferred like -distribution however result may specific experiment performed softmax output cross-entropy since different loss differently calibrated might show response stochastic co-teaching replication baseline experiment conventional co-teaching resulted higher accuracy reported han difference result may ascribed difference random initialization namely experiment shown different random initialization conventional co-teaching resulted highly variable outcome case extreme label noise mini-batches might sampled consisting predominantly training-instances label noise situation difficult conventional co-teaching predetermined number training instance selected stochastic co-teaching handle situation better since reject arbitrary amount label noise however observed mini-batches containing instance label noise led numerical instability instance rejected addressed generating new selection threshold resampling new mini-batches stochasticity additional effect enforcing different decision boundary model training although study effect presume stochastic co-teaching allows training model initialized identically meaning stochastic co-teaching might readily applicable pre-trained network similar conventional co-teaching employed schedule introduces selection threshold stochastic co-teaching complex task found delay several epoch benefited performance delay effectively utilizes tendency deep neural network learn general pattern first perform in-depth study effect different schedule different classification task network architecture loss function observed limited effect preliminary experiment stochastic co-teaching ptb-xl ecg classification task lead improved accuracy whereas auc remains similar baseline may indicate improvement stochastic co-teaching caused improved classification majority class difference baseline experiment ptb-xl ecg classification dataset strodthoff may attributed difference randomness-based operation weight initialization data sampling drawback co-teaching training two network imposes increased computational demand approximately twice amount compute memory compared supervised training efficient alternative bootstrapping one network selection rejection sample bootstrapping handle label noise well outperformed co-teaching indicates beneficial train two predictor decision boundary alternative method including two network also proposed however approach increasingly impact hardware demand especially semantic segmentation task require high resolution output alternative direction might training bayesian network synthetically increasing number network however method would require non-trivial voting scheme furthermore consensus voting selecting instance training render method similar decoupling error propagation strongly coupled among network contrasting core benefit co-teaching network decoupled thus decision boundary develop individually although evaluated stochastic co-teaching classification task using cross-entropy loss loss could applied however threshold selection re-evaluated effect hyperparameters might different experiment additionally co-teaching could recasted application regression problem decision selection rejection distance metric e.g norm inclusion threshold sampled unbounded distribution e.g gamma chi-squared distribution conclusion presented method stochastic co-teaching method employ training two network network selects training instance network training instance selected based posterior probability network selection threshold sampled left-tailed beta distribution method doe require a-priori knowledge level label noise applied variety classification problem including medical task classification medical signal semantic segmentation medical image