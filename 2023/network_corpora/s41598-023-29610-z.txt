introduction field scientific machine learning sciml burgeoning innovative method combine machine learning existing scientifically-derived differential equation model computational physic scheme part many realistic dynamical model complex often truncated coarsened aggregated due computational cost constraint machine learning used learn represent neglected unresolved term data-driven fashion technique express missing dynamic function modeled state variable parameter referred closure model closure model sciml result general however often limited interpretability black-box model generalization different computational grid resolution initial condition boundary condition domain geometry physical problem-specific parameter addressing challenge interpretability generalization imperative justify cost training sciml model using data set obtained expensive measurement generated solving complex dynamical model first place goal present study simultaneously address challenge learn closure model generalizable interpretable need closure modeling arises variety reason ranging computational cost consideration preference simpler model complex one due overparameterization lack scientific understanding process variable involved system interest simpler known model often referred low-fidelity model complex counterpart either model observation referred high-fidelity model reality real-world data low-fidelity model categorized three category reduced-order model original high-dimensional dynamical system projected solved reduced space computationally cheaper solve low-dimensional system model quickly accumulate error due missing interaction truncated dimension coarse-resolution model resolve scale interest case neglected unresolved scale along interaction resolved one lead unintended unacceptable effect global scale iii simplistic speculative model incomplete representation understanding process interaction occurs thus uncertainty model formulation even relevant state variable lead gross incorrect approximation real-world phenomenon neural closure model ncms developed low-fidelity model using neural delay differential equation nddes data high-fidelity simulation need time delay closure parameterizations rooted presence inherent delay real-world system theoretically justifed mori–zwanzig formulation using nddes closure modeling number advantage allow use smaller architecture account accumulation numerical time-stepping error presence neural network nns training additionally nddes agnostic time-integration scheme handle unevenly-spaced training data good performance prediction period much longer training validation period however highly-desirable property mentioned fundamental question neural closure include interpretable lead analytical expression achieve generalization many condition variable physics-based model combined seamlessly classic numerical scheme number recent approach aimed address question however challenge remain especially partial differential equation pdes often nns used discretized ordinary differential equation ode form corresponding pdes make inherently difficult generalize change boundary condition domain geometry computational grid recently study taken step addressing drawback sirignano augment underlying pde neural network however learn markovian closure input neural network include state spatial derivative fixed number neighboring grid point also provide accompanying discrete adjoint pde efficient training saha use radial-basis-functions-based collocation method allow mesh-free embedding nns however resulting nns also learn markovian closure account accumulation time-integration error lack interpretability present study develop unified neural partial delay differential equation npddes augment existing/low-fidelity model pde form markovian non-markovian closure parameterized deep-nns neural closure term contain instantaneous delayed contribution input consist modeled state spatial derivative combination derivative problem-specific variable parameter melding low-fidelity model deep-nns continuous spatiotemporal space automatically allows generalizability computational grid resolution boundary condition initial condition design closure term also provide analytical expression missing term thus leading interpretability resulting npddes discretized using numerical method relevant dynamical system studied provide adjoint pde derivation continuous form thus allowing one implement across differentiable non-differentiable computational physic code also different machine learning framework derivation implementation done considering deep-nn architecture thus automatically encompassing linear- shallow-nns providing user subject-matter-expert user flexibility choosing architectural complexity accord prior information available refer new methodology generalized neural closure model ncm series experiment demonstrate flexibility ncms learn closure either interpretable fashion black-box fashion simultaneously using prior scientific knowledge problem hand ncms eliminate erroneous redundant input term combine achieve increased accuracy also demonstrate generalizability learned closure change physical parameter grid resolution initial condition boundary condition first class simulation experiment nonlinear wave advecting shock problem governed kdv-burgers classic burger pdes learned ncm find missing term discovers leading truncation error correction non-linear advection term find training data corresponding combination grid resolution reynolds number sufficient ensure learned closure generalizable range grid resolution reynolds number combination initial boundary condition also outperform popular smagorinsky subgrid-scale closure model second class experiment based ocean acidification model learn functional form biological process compensate lack complexity simpler model obtained aggregation component simplification process parameterizations finally comment computational advantage new ncm framework follows first develop theory methodology gncms application result discussion showcase generalization interpretability property gncms experiment nonlinear wave advecting shock ocean acidification discus computational advantage finally conclusion provided theory methodology functional form closure model representing missing dynamic derived mori–zwanzig formulation prof dependent time-lagged state dynamic many system modeled assuming smooth field state variable governed advection-diffusion-reaction pdes pdes implicitly assume local information state variable exchanged instantaneously spatial location reality however time delay occur several reason first reaction change population non-negligible time scale time delay captured complex model modeling intermediate state variable time response lower-complexity model thus approximate high-complexity model explicitly introducing delay second time delay arise due missing subgrid-scale process and/or truncated mode reduced-order model reason memory-based term thus non-markovian closure term needed augment low-fidelity model general low-fidelity model also outright missing markovian term due truncation coarse resolution incomplete uncertain functional form model term therefore use markovian non-markovian term close low-fidelity model pde form lead partial delay differential equation pddes widely used ecology control theory biology climate dynamic name application area study markovian non-markovian closure term modeled using deep-nns achieve full interpretability learned weight nns closure model time consider single-layer linear-nns closure term general depend state variable spatial derivative combination belonging function library presence discrete delay seen special case distributed delay non-markovian term assumed contain distributed delay maximum finite time-delay given continuous state vector comprising n_s\ different state n_s thus consider dynamical system belonging domain following form aligned aligned x^2 ... low-fidelity~/~known~model x^2 ... markovian~closure~term t-\tau x^2 ... non-markovian~closure~term x\in -\tau aligned aligned nonlinear function parameterized respectively problem-specific parameter associated physical/biological/chemical phenomenon interest weight compared pdes pddes require history function -\tau initialization t=0\ operator represents appropriate boundary condition dirichlet neumann etc needed solve system uniquely furthermore ease notation assumed one-dimensional domain however method directly extends domain neural partial delay differential equation obtain scheme learn pddes parameterized using deep-nns referred neural partial delay differential equation pddes without loss generality brevity limit pddes markovian term non-markovian term distributed delay low-fidelity model considered absorbed markovian closure term hence pdde form aligned aligned x^2 ... x^d t-\tau x^2 ... x^d x\in -\tau aligned aligned two deep-nns instantaneous delayed remain parameterized generality considered function arbitrary number spatial derivative highest order defined ^+\ rewrite equivalent system coupled pddes discrete delay aligned aligned x^2 ... x^d x\in x^2 ... x^d t-\tau t-\tau t-\tau x^2 ... t-\tau x^d t-\tau x\in -\tau -\tau x^2 ... x^d aligned aligned let assume high-fidelity data available discrete time t_1 ... t_m t_i spatial location t_i ... t_i time thus define scalar loss function i=1 t_i k=1 t_i t_i t_i _0^t i=1 t_i k=1 t_i t_i t_i dxdt _0^t i=1 t_i dxdt\ scalar loss function mean-absolute-error mae kronecker delta function order derive adjoint pdes start lagrangian corresponding system aligned aligned _0^t dxdt _0^t t-\tau dxdt -\tau x^2 ... x^d dt\right aligned aligned lagrangian variable find gradient w.r.t first solve following adjoint pdes brevity denote aligned aligned k=1 t_k i=1 i+1 x^i x^i i=1 i+1 x^i x^i t+\tau i=1 i+1 x^i t+\tau x^i -\lambda aligned aligned initial condition =\mu boundary condition derived based forward pdde satisfy aligned aligned j=0 d-i-1 j+1 j+i+1 +\sum ^d\sum j=0 d-i-1 j+1 j+i+1 ^d\sum j=0 d-i-1 j+1 t+\tau j+i+1 aligned aligned detail derivation adjoint pdes supplementary information sect si-1 solving lagrangian variable compute required gradient aligned aligned _0^t dxdt _0^t t-\tau dxdt -\tau _0^t dxdt aligned aligned finally using stochastic gradient descent algorithm find optimal value weight generalized neural closure model property ncm framework schematized fig next discus property variation figure overview generalized neural closure model ncm framework block labeled dnn represent deep-neural-network architecture block labeled symbolizes time-integration scheme dde stand delay differential equation full size image interpretability interpretability—especially markovian closure term—we use simple architecture hidden layer linear activation nonlinearity still introduced input feature nonlinear combination state derivative belonging function library result linear combination nonlinear input feature along l_1\ regularization weight pruning threshold help promote sparsity thus allowing redundancy input function library practice one include many input test function computationally efficient scientifically meaningful adaptively augment prune library data-driven learning process similar demonstrated although approach similarity sindy significantly different sindy requires training data rich enough allow computation temporal spatial derivative solves regression problem discover governing dynamical system successor sindy circumvent need calculating spatio-temporal derivative training data utilizing weak form nns map coordinate problem state variable ncm method also doe require using training data compute temporal spatial derivative account accumulation time-integration error training numerically solving pde augmented markovian closure term corresponding adjoint pde compared model discovery method ncm seamlessly incorporates simultaneously learns non-markovian closure term without simplifying assumption use informative function library along simple architecture hidden layer linear activation also applicable non-markovian term enhanced interpretability fact derivation prior section framework implementation keep possibility using general deep-nn architecture markovian non-markovian closure term allows one introduce arbitrary amount nonlinearity especially case prior information available functional form missing dynamic use deep-nns come cost full interpretability however even case insight obtained example examining weight input layer learned deep-nn determine relative importance different input feature showcased experiment advecting shock—model discovery generalization learned non-markovian closure term generalizability forward model adjoint pdes discretized integrated using numerical scheme finite difference finite volume collocation method etc new approach augment pdes nn-based markovian non-markovian closure first numerical discretization ensures burden generalization boundary condition domain geometry computational grid resolution along computing relevant spatial derivative handled numerical scheme learned deep-nns also automatically make learning dependent local feature affine equivariant similar numerical scheme backpropagation adjoint equation adjoint method adjoint pdes solved backward time one would require access original neural ode proposed adjoint method forgets forward-time trajectory instead remembers state final time solves reverse-time along adjoint pdes approach known suffer inaccuracy numerical instability thus current implementation create continuously update interpolation function using obtained every time step solve forward model memory efficiency one could also use method checkpointing interpolated reverse dynamic method irdm along using adaptive time-integration scheme lead stable accurate solution forward adjoint pdes especially stiff dynamical system inherent inaccuracy instability using continuous adjoint equation followed discretization remain open question well-known issue data assimilation work found combination continuous adjoint method followed discretization adaptive time-integration scheme successful another challenge occur feasibility derivation continuous adjoint pdes followed discretization especially known realistic low-fidelity model highly complex nonlinear case discrete adjoint method i.e. approach deriving adjoint equation discrete forward model might useful make easier utilize vast array tool developed automatic differentiation community last several decade specifically source-code-transformation source-to-source method finally reduced-space adjoints well ensemble approach used estimate gradient application result discussion using four set experiment showcase evaluate capability new closure modeling framework ncm term generalizability grid resolution boundary initial condition problem-specific parameter also demonstrate interpretability learned closure within pdes first second set experiment consider problem based advecting nonlinear wave shock pdes find ncms discriminate discover process dispersion leading truncation error term correction nonlinear advection term interpretable fashion learned markovian neural closure using markovian non-markovian neural closure term demonstrate generalization ncm grid resolution reynolds number initial boundary condition along superior performance compared popular smagorinsky closure third fourth set experiment consider problem based coupled physical-biological-carbonate pdes used study threat ocean acidification utilize ncms discriminate discover functional form uncertain term interpretability augment simpler model obtained aggregation component simplification process parameterizations becomes accurate complex model training evaluation protocol similar use performance validation period past period high-fidelity data snapshot used training fine-tune various training-related hyperparameters final evaluation based continuous evolution training validation period followed longer-term future prediction also compare learned closure known true model figure table section reference prefixed si- direct reader supplementary information experiment nonlinear waves—interpretable model discrimination first set experiment consider advecting nonlinear wave pdes discover missing/uncertain physical process dispersion interpretable fashion using learned markovian neural closure setup true model data generation low-fidelity model model advecting shock nonlinear wave backbone various physical phenomenon korteweg vries kdv –burgers pde often used describe weak effect dispersion dissipation non-linearity wave propagation considering spatial domain select kdv–burgers pde high-fidelity model truth aligned aligned -6u\frac x^3 aligned aligned data generated two solitary wave colliding exact solution initial boundary condition given aligned aligned _1^2 sech x_1 _2^2 sech x_2 x^2 aligned aligned x_1\ location _1^2\ amplitude 1/\eta _1\ width first soliton wave whereas x_2\ location _2^2\ amplitude 1/\eta _2\ width second soliton wave initially parametric analytical solution system given aligned aligned _1^2 _2^2 _1^2 _2^2 aligned aligned _2\ _1\ _2\ given aligned aligned x-x_1 _1^2 x-x_2 _2^2 aligned aligned choose l=10\ maximum time 1.5\ 1.2\ 0.8\ x_1 -6.0\ x_2 -2.0\ closure learning experiment assume prior knowledge existence advection term low-fidelity model thus aligned aligned -u\frac aligned aligned effect unknown need discovered assume unknown effect mainly markovian nature modeled using linear combination library nonlinear function comprising term 3rd order spatial derivative x^2 x^3 u\frac u^2 compared true model library contains two superfluous redundant term x^2 u^2 course doe contain repetitive term numerics numerical solution low-fidelity model augmented ncm obtained using finite difference scheme advection term order accurate upwind used spatial term derivative discretized order accurate central-difference time-marching vode scheme adaptive time-stepping used finally employ fine grid n_x 200\ number grid point x-\ direction order keep low discretization truncation error comparison lf-hf fig compare numerical solution low-fidelity model analytical solution high-fidelity model solution two model initial condition however evolution drastically different high-fidelity model two soliton interact elastically i.e. amplitude shape unchanged interaction however experience phase shift position low-fidelity model however two soliton even come close interacting figure comparison numerical solution kdv–burgers equation advection term low-fidelity model middle plot analytical solution corresponding equation stronger advection 3rd order derivative term high-fidelity model left plot low-fidelity model solved grid n_x=200\ grid point absolute difference two solution provided right panel full size image training architecture data loss function ncm consider markovian term simple neural network hidden layer linear activation output layer in-effect equivalent linear combination input training data consists analytical solution sampled time interval 0.01 time t=1.0\ validation period 1.0\le 1.25\ experiment use _1\ _2\ regularization weight neural network prune value drop certain threshold weightage _1\ regularization non-zero order promote sparsity set tuned hyperparameters used generate result presented next provided supplementary information sect si-2.2 given analytical solution data true t_i i=1 ^m\ loss function based time space averaged mean-absolute-error mae i=1 pred t_i true t_i dx\ m=100\ number high-fidelity solution state different time available training learning result perform six repeat experiment exactly set hyperparameters learned model mean standard deviation weight follows aligned aligned -u\frac 4.9680 0.0008 u\frac 1.0105 0.0002 x^3 aligned aligned true coefficient corresponding learned u\frac x^3 term -5.0\ -1.0\ respectively learned closure able recover true model slight discrepancy learned coefficient compensate small discretization truncation error illustrate compare root-mean-square-error rmse i=1 j=1 n_x n_x pred x_j t_i true x_j t_i learned closure true model solved using numerical scheme rmse mean standard deviation obtained learned closure true model solved numerically 0.0063 0.0014\ 0.0251 respectively thus average learned closure lead smaller rmse error numerically-solved true model note excellent accuracy coefficient recovered learned model compared true model similar observed sindy variant kdv pde sensitivity learning sensitive batch-time higher value especially detrimental convergence behavior general observed error low- high-fidelity model large e.g. low-fidelity model using smaller batch size regularization weight lead slightly different value learned coefficient especially noted u^2\frac term whose weight tends towards non-zero value small magnitude study impact different hyperparameters encountered specifically ncm framework sciml general training refer current set experiment learning framework able recover known true model due additionally focus demonstrating generalization initial condition boundary condition grid resolution experiment advecting shock—model discovery generalization second set experiment employ advecting shock pde model first ncm discovers leading truncation term correction nonlinear advection term interpreting learned markovian neural closure second utilize markovian non-markovian ncm term trained data corresponding combination grid resolution reynolds number demonstrate generalization learned closure model grid resolution reynolds number initial boundary condition along superior performance compared popular smagorinsky closure model interpret learned closure analysing weight learned neural network find closure independent reynolds number despite one functional input setup true model data generation low-fidelity model consider classic form burger equation governing high-fidelity model aligned aligned -u\frac x^2 aligned aligned diffusion coefficient data generated analytical solution burger equation initial boundary condition aligned aligned t_0 re\frac x^2 x=l aligned aligned reynolds number 1/\nu\ t_0 re/8 solution given aligned aligned t+1 t+1 t_0 re\frac x^2 aligned aligned however discrete version solved numerically truncation round-off error occur numerical solution incurs discretization error numerics solve burger equation numerically following scheme 1st order accurate upwind advection term 2nd order accurate central-difference diffusion term vode scheme adaptive time-stepping thus leading order truncation error term given -\frac x^2 x^2 uniform grid-spacing term x^2 contain spatial derivative order comparison lf-hf comparison analytical numerical solution burger equation provided fig one clearly notice effect numerical diffusion error location shock peak later time due truncation error figure comparison numerical solution burger equation re=1000\ low-resolution grid low-fidelity model middle plot corresponding analytical solution high-fidelity model left plot low-fidelity model solved grid n_x=50\ grid point absolute difference two solution provided right plot also provide pair time-averaged error specifically root-mean-squared-error rmse rmse considering grid point error least maximum velocity value denoted rmse full size image learning interpretable truncation error nonlinear flux correction training architecture data loss function first consider markovian closure term based library composed second-degree combination x^2 library explicitly omits u\frac already known part governing equation u^2\ part truncation error due absence derivative hence markovian closure term assumed linear combination x^3 x^2 x^2 x^2 x^2 fourth term true unknown leading order truncation error term term informed still expected redundant term multiplied appropriate power closure term dimensionally consistent term burger equation 4th order accurate central upwind finite-difference scheme used compute spatial derivative markovian closure eliminate additional source truncation error analysis training data consists analytical solution t=4.0\ solved domain length l=1.25\ saved every 0.01 time-intervals three different combination n_x\ number grid point x-\ direction chosen n_x\ pair ~\text -\frac x^2 term really leading source error every epoch parse training data pair selected random order sampling without replacement tune hyperparameters based performance training period 0.0 4.0\ validation period 4.0 6.0\ provided sect si-2.2 markovian closure model simple neural network hidden layer linear activation output layer in-effect equivalent linear combination input given analytical solution true t_i i=1 ^m\ loss function time space averaged mean-absolute-error mae i=1 pred t_i true t_i dx\ m=400\ number high-fidelity solution state different time available training learning result perform eight repeat experiment tuned hyperparameters resulting learned model mean standard deviation coefficient follows aligned x^3 x^2 x^2 x^2 x^2 0.133 0.017 0.009 0.023 x^3 x^2 0.323 0.022 x^2 aligned evaluation first compare performance learned ncm w.r.t using true leading truncation error term -\frac x^2 closure case evolve burger equation respective closure term t=8.0\ beyond training validation time-periods n_x pair domain spanned n_x 200\ 1500\ fig provide rmse error see fig description true leading truncation error term used closure find increasing lowering n_x\ value lead instability solution cause explode contrary learned ncm case even though shown training data high low n_x\ regime still provides stable solution average performs better counterpart region n_x domain interpret learned closure rewrite substituting u\frac x^2 aligned x^3 x^2 x^2 x^2 x^2 0.133 0.017 u\frac 0.009 0.023 x^3 x^2 0.456 0.012 x^2 aligned thus learned ncm contains x^2 term coefficient correct sign slightly smaller value—in absolute value—in comparison true leading truncation error term along significant term u\frac corresponds first-order taylor series correction nonlinear advection term help mitigating resolution error highlighted earlier finally remarkable important u\frac term missing input feature however surprise still accounted indirectly learned closure utilizing redundant term present input feature library highlight noteworthy learning capability ncm figure performance four closure model burger equation evaluated various n_x pair domain spanned n_x 200\ 1500\ error provided rmse\ see fig description computed w.r.t corresponding analytical solution 0.0\le 8.0\ domain length 1.25\ leading truncation error term -\frac x^2 closure white region top-left denotes unconverged numerical solution learned ncm markovian term three marking n_x re\ pair used training data smagorinsky model c_s 1.0\ learned ncm markovian non-markovian closure term four marking n_x re\ pair used training data full size image learning generalizable interpretable closure training architecture data loss function keeping markovian closure term formulation sect si-2.2 add non-markovian closure term input x^2 discretized using order finite-difference scheme deep-nn architecture given table si-1 utilize fully-connected deep-nn four hidden-layers non-linear swish activation output multiplied ensure contribution non-markovian closure term zero right-hand part domain shock yet reach non-markovian closure term nonlinear explicitly make input dimensionally consistent term burger equation overall training evaluation setup learning interpretable truncation error nonlinear flux correction however time four pair n_x used four combination high low n_x\ contained training data chosen pair tuned set hyperparameters provided sect si-2.2 time-delay 0.075\ based optimal-time delay established burger equation experiment learning result perform seven repeat experiment exactly set tuned hyperparameters learned coefficient markovian term different due presence non-markovian term however weightage given ^2\ x^2 term upon inspection weight input layer deep-nn non-markovian term multiplied consistently found particularly small 10^ indicating learned closure independent one experiment run show fig performance n_x pair domain spanned n_x 200\ 1500\ compare popular smagorinsky model used subgrid-scale turbulence closure large eddy simulation burger equation model introduces dynamic turbulent eddy viscosity _e\ resulting aligned aligned -u\frac x^2 _e\frac aligned aligned c_s c_s\ smagorinsky constant rectangle formed training n_x pair subset rectangle evaluate learned closure testing interpolation extrapolation performance w.r.t changing physical parameter governing model grid resolution find learned ncm clearly outperforms smagorinsky model noted fig bottom-right corner low high n_x\ region inherently small error even without presence closure amount error low-fidelity high-fidelity solution different four training data n_x combination example coarsest resolution higher incur error thus notice differential impact learned ncm reducing error around different training data n_x pair claimed earlier expect learned ncm also generalizable different boundary condition tested modifying boundary condition analytical solution used training corresponded neumann boundary condition right edge domain changed zero dirichlet boundary condition furthermore length domain decreased l=1\ n_x 50\ number equally-spaced grid point used low-fidelity model re=1000\ since closed-form analytical solution exists dirichlet boundary condition case solve system n_x 1000\ grid point use true solution comparing performance learned closure fig find learned ncm able keep error remarkably low throughout time period encompassing training testing prediction figure solution burger equation without learned generalized neural closure model ncm 1000\ low-resolution grid n_x 50\ zero dirichlet boundary condition right edge case also provide pair time-averaged error see fig description full size image sensitivity general quality learning sensitive batch-time hyperparameter however higher value led interpretable closure using lower-order finite-difference scheme closure input compromise performance learned closure however lead decrease interpretability sensitivity hyperparameters similar observed experiments-1a experiment ocean acidification—interpretable model discrimination third set experiment consider coupled physical–biological–carbonate pde model used study ocean acidification utilize markovian neural closure model interpretably discriminate candidate functional form uncertain zooplankton mortality term setup true model data generation low-fidelity model model used study predict essential carbonate chemistry biological production cycle interplay global warming plethora biogeochemical model proposed differ complexity ability resolve different biological process set parameter value functional form might work particular ocean region may apply anywhere else additional source complexity may seasonal variability functional form set experiment high-fidelity model similar hadley centre ocean carbon cycle hadocc model biological part modified version four component nutrient phytoplankton zooplankton detritus developed gulf maine along dissolved inorganic carbon dic total alkalinity carbonate part npzd model aligned aligned -u_p g_z u_p g_z m_p g_z m_z 1-\gamma -\lambda g_z m_p m_z aligned aligned u_p\ phytoplankton growth regulated nitrogen limitation based michaelis–menten kinetics photosynthetically active radiation g_z\ zooplankton grazing given aligned aligned u_p max n+k_n 1-\exp max -\beta max i_0 -k_w g_z max zp^2 p^2 k_p^2 aligned aligned m_z to-be-learned zooplankton mortality equation concentration biological variable measured nitrogen mmol depth parameter max maximum growth rate phytoplankton k_n\ half-saturation constant light-growth slope inhibition coefficient i_0\ photosynthetically active radiation par sea surface k_w\ attenuation coefficient water max zooplankton maximum grazing rate k_p\ half-saturation constant zooplankton grazing assimilation coefficient m_z\ zooplankton mortality coefficient m_p\ phytoplankton mortality coefficient active respiration zooplankton expressed fraction grazing remineralization rate detritus carbon system coupled nitrogen fixed carbon–nitrogen ratio c_p\ c_z\ c_d\ aligned aligned dic -c_p c_z c_d c_p u_p -\frac 2\gamma c_p u_p aligned aligned neither dic effect biology phytoplankton growth carbon limited last term dic equation represents precipitation calcium carbonate form shell hard body part subsequently sink euphotic zone also known hard flux flux modeled proportional additional uptake carbon primary production chemistry dictate decrease total alkalinity two molar equivalent mole carbonate precipitated general since measured mmol mol divide right-hand-side equation density sea-water _w\ moreover unit dic concentration mmol biological carbonate model often coupled physical model introduce spatial temporal component experiment use 1-d diffusion-reaction pde vertical eddy mixing parameterized operator k_z k_z\ dynamic eddy diffusion coefficient mixed layer varying depth used physical input model thus biological carbonate state variable governed following non-autonomous pde aligned s^b k_z aligned aligned k_z z_b z_0 z_b -\gamma -\gamma d_z -\gamma -\gamma d_z aligned z_b z_0 diffusion bottom surface respectively _t\ thermocline sharpness d_z\ total depth 1-d model parameterizations adapted simulate seasonal variability upwelling sunlight biomass vertical profile dynamic mixed layer depth surface photosynthetically-available radiation i_0 biomass field shown fig radiation i_0 total biomass concentration bio affect s^b\ initial condition generate data first initialize state depth-varying total biomass concentration state zero concentration run one-month spin-off npzd model without diffusion term constant sea-surface solar radiation order determine stable equilibrium biological state equilibrium state form initial condition respective state npzd-oa model initialize dic multiply equilibrium state nitrogen-to-carbon ratio considered nearly equal value c_p\ often assumed dependence salinity biological process contribution salinity psu modeled using linear relationship optimized gulf maine array 198.10 61.75 32.34 744.41 44.86 32.34 array dr. p.j haley jr. pers comm biological impact given assume stationary salinity profile described using sigmoid function -bz 1/\nu 31.4~psu\ 32.8~psu\ 1.0\ 0.5\ 0.25\ 2.0\ thus initialize based salinity evolve using coupled low-fidelity model assume prior knowledge existence linear zooplankton mortality term i.e. m_z m_z high-fidelity model however true zooplankton mortality contains additional quadratic dependence i.e. m_z m_z z^2 numerics use 2nd order central difference scheme spatial discretization n_z 20\ dopri5 scheme time integration adaptive time-stepping comparison lf-hf fig -left- -mid-columns provide year-long simulation npzd-oa model quadratic truth linear prior mortality term respectively notice low concentration enhanced bloom former case figure -right-column provides absolute difference two case value model parameter provided sect si-2 figure solution column concentration profile mmol dic mmol mmol vs. time day model used experiments-2a corresponding different functional form zooplankton mortality term left-column top panel show yearly variation solar radiation subsequent panel depict state npzd-oa model m_z m_z z^2 ground truth overlaid dynamic mixed layer depth dashed red line middle-column state npzd-oa model m_z m_z low-fidelity right-column absolute difference corresponding state left- middle- column case also provide pair time-averaged error see fig description full size image training architecture data loss function ncm—we consider markovian term—belonging linear combination library popular mortality function z^2 z^2 1+z compared true zooplankton mortality term library contains three superfluous redundant term z^2 1+z noting term already part low-fidelity model completely known markovian term use simple hidden layer linear activation output layer using weight constraint output layer enforce biomass conservation equation couple dic equation known system architectural detail given table si-1 tuned set training hyperparameters sect si-2.2 training data consists true/high-fidelity model solution sampled time interval 0.1 day t=\ day true t_i b\in dic ta\ i=1 ^m\ i.e. m=300\ high-fidelity solution state use mae-based loss function i=1 _0^ dic ta\ pred t_i true t_i dz\ _b\ hyperparameters scale importance different state variable based magnitude multiple hyperparameter tuning experiment value ~\sigma 0.25 ~\sigma ~\sigma ~\sigma dic ~\sigma 0.1\ found aid learning learning result seven repeat experiment exactly hyperparameters learned model consisted contribution closure equation dic equation contribution found—with mean standard deviation—to -0.02996 0.00014 z^2\ 0.03001 0.00013 z^2\ -0.05603 0.00136 z^2\ respectively reference true contribution zooplankton quadratic mortality term dic equation given -0.02998 z^2\ 0.02998 z^2\ -0.05621 z^2\ respectively sensitivity multiple experiment done study effect hyperparameters batch-time batch-size regularization factor etc. convergence true model severely compromised increasing batch-time changing loss-scaling individual state variable experiment ocean acidification—model complexity last set experiment consider coupled physical–biological–carbonate pde model however time utilize full generalized neural closure model augment simpler model obtained aggregation component simplification process parameterizations becomes accurate complex model simultaneously also discriminate candidate functional form uncertain zooplankton mortality term interpretable fashion setup true model data generation low-fidelity model high-fidelity model data used experiments-2a model intermediate state detritus thus capturing process remineralization quadratic zooplankton mortality i.e. m_z m_z z^2 low-fidelity model complex three-component npz model aligned aligned -u_p 1-\gamma g_z m_p m_z u_p g_z m_p g_z m_z aligned aligned coupled carbonate system using fixed carbon-nitrogen ratio c_p\ c_z\ aligned aligned dic -c_p c_z c_p u_p -\frac 2\gamma c_p u_p aligned aligned 1-d diffusion-reaction pde goal experiment use ncm simultaneously learn functional form zooplankton mortality term using markovian closure term account missing intermediate state detritus non-markovian closure term numerics numerical scheme used experiments-2a comparison lf-hf since high-fidelity npzd-oa model resolve process concentration aggregated state dic differ significantly dic low-fidelity npz-oa model shown fig figure comparison model used experiments-2b without closure model parameter value concentration unit fig ncm training period day validation period day future prediction period day left-column top panel show yearly variation solar radiation subsequent panel depict aggregated state npzd-oa model m_z m_z z^2 ground truth overlaid dynamic mixed layer depth dashed red line middle-column absolute difference corresponding state npz-oa model m_z m_z low-fidelity left-column high-fidelity ground truth right-column absolute difference corresponding state low-fidelity model augmented learned ncm ground truth case also provide pair time-averaged error see fig description full size image training architecture data loss function markovian closure consists linear combination library popular mortality function z^2 z^2 1+z compared true zooplankton mortality term library contains three redundant term z^2 1+z term already part low-fidelity model completely known additionally use deep-nn non-markovian closure term input inclusion photosynthetically active radiation make closure term non-autonomous architecture fully-connected deep-nn used non-markovian closure term provided table si-1 consists two hidden-layers non-linear swish activation include state dic among input order preserve one-way coupling biological carbonate system along biomass conservation coupling carbonate system nitrogen conversion maintained non-markovian closure term manipulating channel output layer hand markovian layer constraint imposed constraining weight output layer help learning impose condition contribution markovian closure term equation exactly equal zero see table si-1 implementational detail constraint training data consists solving npzd-oa model m_z z^2 solution sampled time interval 0.1 day 60~days\ true t_i b\in n+d dic ta\ i=1 ^m\ i.e. m=600\ high-fidelity solution state different time performance learned model validation interval day day used tune hyperparameters provided sect si-2.2 use mae based loss function i=1 _0^ dic ta\ pred t_i true t_i dz\ ~\sigma 0.25 ~\sigma ~\sigma dic ~\sigma 0.1\ similar used experiments-2a time delay 2.5~days\ used non-markovian closure term based optimal delay value study performed learning result nine repeat experiment exactly set hyperparameters mean standard deviation learned contribution markovian closure term equation given -0.03000 0.00067 z^2\ reference true contribution quadratic mortality term equation -0.02998 z^2\ due weight constraint contribution markovian closure term equation exactly zero evaluate performance learned neural closure model long prediction spanning year day comparison true/high-fidelity data one experiment provided fig overall learned closure keep error low throughout 1-year time period apart slight increase observed state 200~days\ sensitivity multiple experiment done study effect hyperparameters batch-time batch-size regularization factor etc. effect similar observed previous experiment however using larger neural network architecture non-markovian term led high variability learned coefficient markovian term repeat experiment set hyperparameters probably increased expressive power non-markovian term overshadows significance learned markovian term remark discussion computational advantage flop-count analysis proved additional computational cost due presence neural closure model similar lower complexity existing low-fidelity model however current generalized framework additional computational advantage first size neural network architecture completely independent number discretized state variable dictated number local feature used input ncm term second neural network applied locally every grid point directly possible use batch size number grid point reported larger batch size could lead performance speed-ups forward pas neural network inference stage estimating leading flop-count order training non-trivial due presence number operation ranging time-integration forward model adjoint pdes automatic differentiation neural network creation use interpolation function integral compute final derivative gradient descent step etc operation lead training cost non-negligible however generalizability interpretability learned ncms boundary condition initial condition domain problem-specific parameter etc. help justify one-time training cost lack prior knowledge showcased prior experiment summarized corresponding sensitivity study lack prior knowledge missing dynamic could manifest many different way includes known low-fidelity model dynamic known low-fidelity model different high-fidelity model/data knowledge potential candidate term create input function library even information relevant state variable allow compensation lack prior knowledge ncm framework derived implemented deep-neural-network dnn architecture markovian non-markovian closure term fig flexible modeling framework provides full autonomy design unknown closure term using linear- shallow- deep-nns selecting span function library using either markovian non-markovian closure term decision made subject matter expert/user depending problem hand example experiment fully-connected deep-nns utilized non-markovian closure term general prior knowledge available framework could extended allow adaptive increase input function library example using algorithm proposed non-markovian term current derivation ncm framework due mathematical construct non-markovian term doe account possibility memory decay contribution function integral w.r.t t-s\ however memory decay variation desired property problem allow one split integral contiguous part multiply different weight alternate option consider discrete delay utilizing recurrent neural network discrete-nddes implicitly incorporate desired memory decay general need non-markovian closure term given problem determined subject matter expert however many case anticipate need non-markovian closure term imperative especially high-fidelity model/data account intermediate state variable modeled known low-fidelity model experiment ocean acidification—model complexity finally also desirable allow learning adaptive optimal delay instead treating hyperparameter possibility refer appendix derive theory learning optimal delay ncm framework dde counterpart ncm conclusion present study develop neural closure model generalizable computational grid resolution boundary initial condition domain geometry problem parameter also provide interpretability generalized neural closure model ncms based neural partial delay differential equation npddes augment existing/low-fidelity model pde form markovian non-markovian closure parameterized deep-nns melding continuous spatiotemporal space followed numerical discretization ensures burden generalization along computing relevant spatial derivative carried numerical scheme learned nns space-time continuous form ncms also make easy interpret learned closure efficient training derive adjoint pdes continuous form discretize adaptive time-integration scheme employ interpolation function constructed forward integration increase numerical stability accuracy enables implementation across differentiable non-differentiable computational physic code different machine learning framework agnostic numerical method remove requirement availability regularly spaced training data space time also account error time-evolution state presence nns training finally derivation implementation consider deep-nn architecture markovian non-markovian term thus automatically encompassing linear- shallow-nns providing user subject-matter-expert flexibility choosing architectural complexity accord prior knowledge series four set experiment demonstrate interpretability generalizability learned ncms first two set simulation experiment based advecting nonlinear wave shock governed kdv-burgers classic burger pdes low-fidelity model either missing term contain error due unresolved subgrid-scale process presented function library containing term spatial derivative different order combination grid-resolution reynolds number input closure term learned ncms eliminate redundant term discover missing physic leading truncation error term correction nonlinear advection interpretable fashion correction nonlinear advection term despite absent input function library still accounted learned indirectly analyzing deep-nn weight also notice learned closure term independent reynolds number find training data corresponding 3–4 combination number grid point reynolds number sufficient ensure learned closure generalizable large range grid resolution reynolds number initial boundary condition also outperform popular smagorinsky closure model last two set experiment based one-dimensional non-autonomous ocean acidification pde model couple physical biological carbonate state process interaction experiment low-fidelity model uncertainty functional form certain biological process lack complexity due missing intermediate state variable learned ncms simultaneously discriminate candidate functional form uncertain zooplankton mortality term markovian part closure account missing intermediate state process non-markovian part term computational advantage new framework naturally lends batching across computational grid point forward pas nns closure term thus leading potential performance speed-ups ncms allow learning markovian non-markovian closure parameterization deep-nns pde level thus addressing issue generalizability interpretability often bottleneck come using machine learning computational science engineering problem generalizability interpretability property also make easier justify often computationally expensive training stage thus enabling wider adoption