introduction development microelectronic technology provides new opportunity improve maintenance production equipment technical managerial perspective establish improvement production necessary focus important step failure analysis process technical procedure studying material product fail important understand component fails longer performs intended function main goal failure analysis find underlying root cause failure ideally view removing identifying way prevent happening objective failure analysis number good outcome obtaining information database put good use preventing future failure enhancing quality extending life product service making economic aspect meet principal fundamental challenge digital world important build information database describe failure conclusion allowing ensure increasingly complex electronic system operate reliably securely many organization use failure reporting analysis corrective action system fracas keep track product problem fracas technique main task recording capturing information failure problem provide new information support future reliability analysis provide report summary incident count provide failure dataset metric measure quality parameter developing novel technique based artificial intelligence swiftly ass discover fault development manufacture electronic component system using final report generated fracas one key difficulty facing digital world incorporation multi-structured data source critical success data-driven maintenance ai-enhanced technique introduced integrated reliability-centered maintenance analysis complex production system failure rate reduced availability improved text mining artificial intelligence technique applies natural language processing nlp convert unstructured text document database normalized structured data analyzed used train machine learning algorithm text mining also technique extracting information unstructured document identifying novel previously unknown pattern next step feature attribute selection step focus deleting element significant mining process addition step several advantage reduce computational complexity get fewer noise decision space reduce dimension consistent homogeneous dataset study textual dataset consists description failure analysis failure conclusion microelectronic technology product objective construct model able predict failure conclusion feature failure analysis description feature however textual information valuable construction predictive model use limited number priori feature may tricky feature selection reduces dimensionality selecting subset original input textual variable word textual variable selection strategy decrease dimension textual feature may relevant specific phenomenon identifying best minimum subset without transforming data new set order achieve complicated model prediction classification algorithm implement selection relevant textual variable excluding non-informative variable various mathematical technique used select optimal subset variable successive projection algorithm backward/forward selection algorithm reweighted adaptive competitive sampling importance variable projection elimination non-informative variable interval partial least square regression monte carlo-elimination non-informative variable particle swarm optimization deep learning approach feature learning enhanced convolutional neural network fle-cnn competitive adaptive reweighted sampling partial least square etc however technique well suited textual datasets hand application method lead loss lot information analysis genetic algorithm belongs research technique emulate principle natural selection performs search complex large multimode landscape provides near-optimal solution objective fitness function optimization problem however cost computational time high long string representation evolves high dimensional space typical textual data genetic algorithm bottom-up strategy chooses best feature subset based survival fittest principle chromosome competing others quality chromosome assessed using predetermined fitness function fitness function arguably important part role measure quality chromosome population according given optimization objective supervised learning method used derive new fitness function transform textual data much lower-dimensional subspace adequately regarding specific application different type supervised method exist literature best-known decision tree model support vector machine model svm study conducted demonstrate combination genetic algorithm support vector machine method improves textual classification accuracy spam dataset another study show efficiency feature selection based information gain genetic algorithm reduce dimension text vector improve accuracy text classification recent paper proposes genetic algorithm-oriented latent semantic feature methodology achieve better representation document text classification therefore based one summarize motivation combining supervised learning method following combination genetic algorithm supervised learning method popular topic research field machine learning optimization example study fern√°ndez author used optimize parameter support vector machine svm classification task showed combination two approach led improved performance compared using svm another study liu proposed ga-based approach feature selection conjunction decision tree classifier showing combination two method outperformed individual method several benchmark datasets addition parameter optimization gas also used search optimal network architecture deep learning instance real proposed method called large-scale evolution image classifier used evolve architecture convolutional neural network cnns showed evolved architecture outperformed manually designed one cifar-10 cifar-100 image classification benchmark study demonstrate potential combining supervised learning method improving performance various application highlight need research area hand research gap challenge overcoming point summarized following challenging issue study likely related task developing predictive model accurately predict failure conclusion based failure description challenging task requires model learn relationship input feature target output difficult due presence noisy irrelevant feature imbalanced class distribution non-linear relationship feature target proposed method address challenge combining genetic algorithm decision tree classifier referred ga-dt used search subset discriminative feature failure description used input decision tree classifier help overcome issue noisy irrelevant feature selects informative feature classifier use additionally decision tree known able handle imbalanced class distribution non-linear relationship making suitable choice task effectiveness proposed ga-dt model demonstrated experiment show improved performance compared using decision tree classifier genetic algorithm highlight contribution proposed method combine strength decision tree classifier improve accuracy predictive model main goal study build advanced predictive model capable predicting failure outcome significantly using failure analysis description another objective investigate potential using supervised variable selection technique using genetic algorithm identify informative useful text feature text dataset contains large number word show whether feature selected proposed method significantly improve performance predictive model failure conclusion feature failure analysis description feature propose methodology based association genetic algorithm supervised model decision tree support vector machine evaluated score fitness function identification discriminating variable applied failure analysis textual data function allows calculating accuracy predictive model applied unbalanced dataset suggested algorithm called ga-svm ga-dt article structured follows second part present feature selection associated algorithm detail operating principle population-based metaheuristic algorithm focus particularly genetic algorithm detailed operation allows selection relevant feature part work present machine-learning algorithm used calculate fitness value metaheuristic algorithm deep description supervised method support vector machine svm decision tree third part present result obtained applying metaheuristic-machine learning algorithm combination failure conclusion feature failure analysis description feature show result observed allow pick valid model ga-dt confirmed different metric bleu score cosine similarity division 70\ training set 30\ testing set finally discussing result close general conclusion interest combination feature selection algorithm machine learning method capability performance dimension reduction possibility implementing tool belonging metaheuristic algorithm improve accuracy rate mathematical method supervised learning modelling multiclass error-correcting output code ecoc model using support vector machine svm error-correcting output code ecoc framework basic yet effective method dealing multi-class categorization problem based embedding binary classifier classifier consists multiple binary learner support vector machine svms ecoc model classifier allow store training data parameter value prior probability coding matrix classifier aim perform task predicting label posterior probability new data multi-class ecoc model using svm method consist three major component encoding binary classifier learning decoding step coding procedure coding matrix usually first determined several class row coding matrix represents specific class group independent binary classifier formed based different partition original data according column coding matrix finally new data predicted specific class decoding procedure based output learned binary classifier coding matrix let x=\ j=1 training set sample observed variable d-dimensional vector represents sample let unobserved random variable denoting class membership denoting number class class svm problem class separated remaining class binary svm classifier combined together make final multi-class classifier remaining mean data point class combined form one class optimal hyperplane separate data point class combined class found using standard svm approach denote optimal separating hyperplane discriminating class combined class aligned aligned weight vector bias mapping function project training data suitable feature space allow nonlinear decision surface parameter decision function determined following minimization aligned j=1 aligned subject aligned aligned scalar +1\ denoting class label regularization constant denote slack variable introduced relax separability constraint decision rule assigns vector class given aligned sign aligned main difficulty approach output classifier binary value usual way handle problem ignore sign-operator finding optimal hyperplanes given say class largest value decision function given aligned argmax aligned approach index largest component discriminant function assigned data point error rate svm svm classifier defined aligned svm j=1 aligned belongs class estimated method classifier class indicator function defined aligned array class class array aligned decision tree classifier decision tree classifier non-parametric classifier doe require priori statistical assumption made regarding underlying distribution data basic structure decision tree however consists one root node number internal node finally set terminal node node subset predictor used determine split non-terminal node parent node node split two child node growing tree consists selecting optimal split determine non-terminal node assignment terminal node class data recursively divided decision tree according defined classification framework class simply assigned terminal node observing class mostly commonly observed region tree thus challenge optimally choose best variable split variable maximize purity similarity among response impurity parent node denoted zero observation class split determined selecting best predictor split value optimizes highest reduction purity aligned b=1 aligned denotes child node proportion observation assigned number branch splitting two common impurity function entropy criterion aligned k=1 aligned gini index criterion aligned k=1 aligned proportion observation class pruning based upon successive step removing lower branch lead improved classification rate final tree determined natural evaluate predictive performance comparing observed class predicted class observation terminal node representing region observation let aligned j=1 aligned denote proportion class observation terminal node classify observation node class aligned argmax aligned misclassification error rate simply proportion observation node member majority class node aligned dtc j=1 aligned supervised learning modelling genetic algorithm type evolutionary optimization computation became popular work holland algorithm based concept natural selection solution copying main principle solution may considered population element represented form chromosome selected textual feature positioned gene step reproduce various evolutionary operation crossover mutation allowing select generation best chromosome identify end optimal chromosome respect optimization criterion defined fitness function figure show step informative feature selection procedure using figure synoptic representation proposed methodology full size image applied data matrix x=\ j=1 set textual feature failure description dataset procedure give one case optimal chromosome textual feature form number variable chosen select optimal chromosome allows extracting new sub-data matrix j=1 under-dimensioned data apply method data analysis step briefly described thereafter detailed article initialization initial parameter chromosome size number gene corresponding number feature selected case population size number chromosome per generation number elite chromosome best fitness value current generation guaranteed survive next generation fraction crossover number chromosome selected perform crossover n-n_ stop parameter maximal number iteration tolerance fitness function first step creation starting population chromosome generated randomly selecting variable size chromosome aligned i=1 aligned initial population wavenumbers variable chosen randomly set uniformly distributed variable ranging maximum minimum limit aligned min max aligned signifies initial variable population min max minimum maximum limit decision variable min max signifies uniform random variable ranging min max computation done generation generation obtain population chromosome i=1 step thereafter give another population chromosome t+1 i=1 evaluation chromosome rated fitness function assigns value smaller value corresponding chromosome chance selected role fitness function measure quality chromosome population according given optimization objective since want create predictive model failure description dataset failure conclusion dataset propose use supervised model chromosome decision tree support vector machine svm calculate score model built fitness function ass quality predictive model obtained score supervised learning method one simplest method used classical fitness function evaluate accuracy predictive model fitness function defined follows aligned model model 1-f_ model aligned aligned model model model model model aligned model score defined harmonic mean precision recall model positive predictive value precision model sensitivity recall predictive model svm function score useful dealing unbalanced class issue problem one class dominate data set fitness function value ordered ascending order best chromosome selected based ordering surviving chromosome copied unchanged next population selection used choosing parent population crossing step may implemented different way rank stochastic roulette wheel stochastic universal sampling selection etc chosen stochastic universal sampling selection since method zero-biased deviation expected reproduction rate algorithmic sampling frequency minimum spread selection performed probabilistically individual selection probability proportional individual fitness first compute probability selecting chromosome z_i\ cumulative probability aligned p_i= f_i i=1 f_i aligned aligned q_i= i=k p_k aligned next generate uniform random number q_1\ select first chromosome z_1\ otherwise select chromosome z_i\ i-1 q_i\ ascending ordered value allows selecting chromosome guaranteed survive next generation 2n_ parent chromosome crossover crossover step attempt extract gene selected chromosome recombines potentially superior child chosen uniform crossover since give good result majority case gene randomly selected either first second parent crossover operation give child explain uniform crossover parent chromosome child chromosome gene array popular crossover variant real number uniform crossover gene situated position child chromosome z_i\ calculated follows random vector real number uniformly distributed size p_1\ p_2\ o_1\ o_2\ 0,1 child copied parent crossover obtained aligned 0.5 =p_ aligned aligned 0.5 =p_ aligned mutation genetic operator used modification value gene maintain genetic diversity one generation population next one chosen gaussian operator since produce best result fitness function operator add unit gaussian distributed random value 2n_ chromosome new value gene rounded nearest integer standard deviation distribution parameter call scale equal one first generation parameter controlled next generation another parameter shrink standard deviation tth generation coordinate parent chromosome given recursive formula aligned t-1 shrink aligned number generation low value shrink produce small decrease amplitude mutation index gene position step repeated maximal number iteration reached converged i.e average relative change fitness function value tolerance procedure give optimal chromosome depends fitness function initial value proposed choice step found optimal chromosome found whatever initial value chromosome used computing similarity document bleu score bilingual evaluation understudy bleu scoring algorithm ass similarity predictive document collection reference document ass quality document translation summarization model use bleu score n-gram count clipped n-gram count modified n-gram precision score brevity penalty used calculate bleu score necessary clipped n-gram count function countclip truncates n-gram gram count doe exceed highest count found one reference n-gram clipped count function defined follows aligned count clip n-gram count n-gram maxref n-gram aligned count n-gram represent n-gram count maxref n-gram highest n-gram count observed single reference document n-gram updated n-gram precision score calculated follows aligned predictive document n-gram count clip n-gram predictive document n-gram count clip n-gram aligned length n-gram predictive document set sentence predictive document predictive document given n-gram weight vector bleu score formulation given aligned bleu score n=1 aligned greatest length n-grams geometric mean modified n-gram precision brevity penalty defined aligned array 1-\frac array aligned bleu score returned scalar value range bleu score close zero indicates low similarity predictive document reference bleu score close one indicates strong similarity predictive document identical one reference document score one cosine similarity similarity two vector inner product space measured cosine similarity determines whether two vector pointing general direction measuring cosine angle text analysis frequently used determine document similarity let see document corpus related one another let two vector represent topic association document respectively number term connected subject respectively cosine similarity used calculate measure document similarity aligned =\frac aligned denotes norm vector cosine similarity score indicate scalar value range cosine similarity close zero indicates poor similarity predictive document reference cosine similarity close one indicates strong similarity proposed methodology evaluation predictive model failure analysis figure synoptic representation proposed methodology full size image section present methodology proposed selection variable genetic algorithm combined decision tree ga-dt support vector machine ga-svm model applied textual data figure show step failure analysis modeling methodology extracting best textual feature using supervised variable selection technique representing predictive model failure description conclusion failure analyzed data proposed methodology consists three main phase first perform pipeline preprocessing failure analysis description conclusion failure important time consuming part textual data failure clean prepare data could compromise predictive model phase show application word2vec vectorization method preprocessed textual data obtain numerical data phase show application variable selection method combined decision tree support vector machine supervised learning vectorized preprocessed data quantify accuracy selected predictive model discriminate textual feature compute different metric like bleu score cosine similarity finally compare predicted textual conclusion original textual conclusion confirm similarity application result data treatment performed using matlab-r2022b environment script available upon request data description structure data description analysis important phase precedes modeling accurate representation data necessary define parameter model textual dataset failure analysis microelectronics production original dataset provided stmicroelectronics dated consists two part first description failure analysis source failure request property sample detail failure second dataset conclusion analysis conclusion success rate cycle time table contains list different feature short description data transformed vertical stacking analysis horizontal stacking mean description objective context etc well conclusion failure represent observation transformation reduces data size 12,300 observation keep preprocessed feature date getting clean processed data using preprocessing pipeline introduced vectorize using word2vec genism word2vec setting kept except vocabulary size set minimum word three formalizing approach use following notation let i=1 j=1 represents input space given dataset number sample number feature i=1 j=1 represents output space conclusion failure dataset number feature table textual feature description present data set conclusion failure analysis full size table table textual feature description present data set description failure analysis full size table preprocessing pipeline eliminating noise removing whitespace punctuation correcting spelling error deleting duplicate instance converting text lowercase removing stop word word three letter example preprocessing text start stage preparation pipeline symbol alphanumeric removal technique remove word text add intelligence pattern analytic sample symbol occasionally alphanumeric word stop word inflexion used emphasize meaning thus removed tokenization thresholding tokenizing change break sentence token using separator thresholding term used remove word certain length paper set threshold two stemmatization lemmatization process removing affix prefix suffix textual feature abbreviation abbreviation common fracas hence need replace original meaning created dictionary abbreviation alleviate challenge optimization parameter critical phase right choice parameter order ensure convergence algorithm optimal solution parameter initialized follows number elite fraction crossover 0.8\ maximum number iteration t=100\ population size 100\ tolerance =10^ value used several implementation since give good result similar data identify optimal value evaluated different size chromosome algorithm converged tolerance reached maximum number iteration value chromosome size give maximize value fitness function chosen optimal value aligned aligned best accuracy ga-svm ga-dt evaluated different size chromosome l=3 figure show fitness value ga-dt ga-svm algorithm respectively found give highest fitness value method indicates need four failure description feature build best predictive model analysis conclusion failure figure value ga-dt fitness function different size chromosome optimal value highest score full size image figure value ga-svm fitness function different size chromosome optimal value highest score full size image table value accuracy bleu score cosine similarity ga-svm ga-dt algorithm full size table table example textual sample analysis conclusion three best prediction ga-dt algorithm full size table result discussion proposed methodology prediction conclusion failure proposed methodology applied two different fitness function svm selecting variable ga-svm ga-dt algorithm calculated accuracy evaluate performance predictive model bleu score cosine similarity metric order quantify result prediction conclusion failure accuracy intuitive performance measure simply ratio correctly predicted document total document aligned accuracy aligned true positive true negative false positive false negative value occur actual document contradicts predicted document value bleu score cosine similarity accuracy presented table confirm ga-dt allows better predictive model textual sample predict failure conclusion feature compared algorithm ga-svm see first four feature give good precision good value bleu score cosine similarity ga-dt method except last textual feature conclusion analysis sample recorded variable large textual paragraph latter feature say metric calculated accuracy bleu 0.32 cosine 0.30 good compared study textual dataset one also find application variable selection genetic algorithm improves accuracy model result showed table table present example result obtained application genetic algorithm decision tree ga-dt display three best prediction failure analysis conclusion text sample calculate bleu score quantify similarity predicted sample original sample one find value bleu score close one indicates strong similarity predicted sample reference one conclusion proposed methodology based association genetic algorithm supervised classifier method identification discriminant textual feature study best predictive model failure conclusion using feature failure description implementation genetic algorithm decision tree classifier fitness function led identification interesting feature blue score cosine similarity used evaluate similarity predictive document set reference document obtained interesting value indicate strong similarity predictive document reference also found application variable selection genetic algorithm improve accuracy metric model obtained svm method shown discriminating feature selected proposed ga-dt method provide best predictive model failure conclusion according description failure process compared ga-svm model direct application decision tree support vector machine applied feature description failure i.e. without preselection method perspective working towards addressing following challenge improving performance model applying generative sequence-to-sequence language model failure conclusion generation given failure description propose methodology based genetic algorithm decision tree select important input variable best predicts conclusion root cause failure analysis variable used train transformer model failure conclusion generation gpt2 transformer model etc