introduction skeleton-based action recognition aim recognize human action category given complete action execution become research hotspot due comprehensive application scenario human-computer interaction autonomous driving video surveillance early stage motion data based rgb video depth map optical flow information used primary representation human motion compared representation human skeleton data advantage free interference video background information eliminating influence external factor high degree aggregation sufficient data make become mainstream data format human action recognition maturity pose estimation algorithm optimization depth camera technology calibration skeleton data become accurate accuracy researcher using skeleton data action recognition analysis also improved lot human skeleton natural graph structure joint point graph bone edge generally gcns-based model try make contribution graph adjacency matrix higher prediction performance yan applied graph convolutional network gcns human skeleton-based action recognition used graph adjacency matrix aggregate feature human body connected joint extend gcns st-gcn capture flexible spatio-temporal relationship motion dependency proposed as-gcn based st-gcn compared limitation st-gcn refers adjacent joint reference joint extended several adjacent joint aggregate information larger receptive field shi presented 2s-agcn transform gcns multiplication addition establishing latent relationship non-adjacent joint ms-gcn module designed liu solves problem biased weight multi-scale adjacency matrix improvement limited inherent connection structure joint adjacency matrix a\in n\times represents structural relationship indicates i\textrm joint j\textrm joint adjacent otherwise unreasonable alternative setting method make joint show strong correlation characteristic irrelevant characteristic training process whole model also gcn-based sota model suffers over-smoothing issue increasing depth chen put forward movement human body different activity change correlation joint action clapping correlation joint left right hand high act waving correlation two weaken hence expressing correlation joint adjacency matrix inappropriate need new method represent correlation joint recently success self-attention natural language processing shown modeling attention help improve recognition ability effectively motivates develop attention network based skeleton data human action recognition human skeleton data correlation joint obtained inner product eigenvectors joint obviously correlation represented obliquely symmetric matrix unified adjacency matrix gcns term physical meaning mathematical expression element adjacency matrix correlation matrix arbitrary value abundantly characterize correlation joint based observation integrate gcns method self-attention design amanet includes spatial feature extraction module sfem temporal feature extraction module tfem spatio-temporal fusion feature extraction module stfem sfem extract spatial feature sequence according correlation joint frame tfem extract temporal feature series according correlation frame stfem aim extract spatio-temporal characteristic adjacent multi-frames simultaneously furthermore extract richer feature limited data design data preprocessing module module obtain three independent spatio-temporal feature sequence original skeleton data including skeleton velocity sequence skeleton acceleration sequence relative position sequence entire system adopt multi-scale dual-stream ms-ds architecture inspired dual-stream network multi-scale final softmax score obtained training joint bone data different scale weighted added obtain final recognition score main contribution paper summarized follows extract richer feature limited data data pre-processing module designed include four independent spatio-temporal feature sequence namely original skeleton sequence skeleton velocity sequence skeleton acceleration sequence relative position sequence unifying gcns self-attention propose adaptively multi-correlations aggregation network amanet capture dynamic correlation human joint considering multi-scale feature aggregation amanet propose three key module learn rich motion representation spatial feature extraction module sfem capture motion dependency spatial-wise channel temporal feature extraction module tfem exploit temporal information motion joint spatio-temporal feature extraction module stfem comprehensively model motion dynamic conduct extensive experiment amanet three public datasets including ntu-rgb+d ntu-rgb+d kinetics-skeleton verify method highly competitive compared existing sota model rest paper organized follows section related work show recent work related work section method describes explicitly several vital module proposed architecture section experiment illustrates detail experiment section conclusion concludes work related work early human action recognition key issue human action recognition research extracting distinguishable rich feature limited data deep learning method applied field dense trajectory algorithm improved version idt algorithm excellent among traditional human action recognition algorithm method collect feature point video frame extract feature point trajectory time dimension time combine motion descriptor feature point supplement description optical flow feature maintains excellent performance robustness prolonged calculating optical flow feature optical flow independent branch vision often formulated problem estimating projection natural motion world result show optical flow feature informative correlation edge slight displacement action recognition relatively large cai recently used optical flow information skeleton information input dual-stream network architecture obtained fairly good result deep learning method applied field traditional design based artificial prior knowledge quickly eliminated advantage learning effective representation video data without manual feature selection identifying complex high-level activity work use cnn extract feature based idt algorithm extend motion trajectory feature method greatly simplifies feature extraction process successfully applies cnn feature extraction human action extended traditional cnn 3d-cnn temporal information performed feature computation video data temporal spatial dimension connected feature map convolution process data multiple consecutive frame multi-resolution cnn divide input video two independent data stream low resolution original resolution perform feature extraction two set data stream respectively finally combine result obtained two set data softmax year dual-stream structure fused spatiotemporal network proposed take deep learning significant step forward action recognition structure divide video information spatial temporal data space refers surface single frame time refers optical flow characteristic frame cnn implement two different stream finally result fused action recognition based gcns method method almost process rgb channel information video rgb channel information inevitably introduce much noise redundant data harming recognition result speed weaken influence part researcher try extract skeleton data human body model input skeleton sequence doe exist form traditional euclidean space tree-like topology unreasonable use cnn process data structure yan applied gcn network human skeleton-based action recognition first time st-gcn network constructed realizes dynamic modeling bone based time series human joint position extending gcn st-gcn capture spatiotemporal change still selection convolution kernel aggregate local feature attention mechanism work good finding association non-connected joint proposed as-gcn network based classic encoder-decoder framework divide skeletal data information actional link a-links s-links structurals link a-links represent connection specified joint related joint body compared limitation st-gcn refers adjacent joint s-links extend referenced joint several adjacent joint aggregate information larger receptive field 2s-agcn network proposed shi transforms attention mechanism gcn multiplication addition successfully establishes connection non-adjacent joint based previous work liu proposed feature extractor named ms-g3d successfully solved problem biased weight multi-scale adjacency matrix using adjacency matrix single frame define connection relationship joint adjacent multi-frames complex spatiotemporal joint feature skeleton sequence captured proposed ctr-gcn achieves breakthrough gcns field skeleton-based action recognition optimizing skeleton topology information different input channel skeleton data advantage identifying simple action completed single double individual appears weak identifying group activity due lack background information idea gcn still serve guiding ideology identifying group activity liu proposed visual semantic graph neural network complete action recognition task group activity first divide image data multiple token defines connection relationship token gcn aggregate connection different token realize group activity recognition action recognition based self-attention mechanism self-attention first proposed model established notice keywords text analysis nlp field principle calculate inner product feature vector generate correlation matrix vector network pay attention feature high correlation popular field recent year due good portability superiority field human action recognition based skeleton data improve recognition ability model standard method enable model accurately find spatiotemporal dependence characteristic joint human body movement process optimization adjacency matrix section action recognition based gcns method purpose plizzari applied self-attention skeleton-based human action recognition first time proposed st-tr network framework accurately reflect spatiotemporal dependency joint gcn-based method method overestimate relationship joint lead confusion overall model parameter reducing accuracy network liu proposed ka-agtn network structure basis structure multi-head self-attention simulate high-order spatial dependency joint adjacency matrix revise dependency improves problem overestimating relationship joint st-tr proposed tka module structure enhances correlation joint temporal dimension whole framework obtains competitive result zhao utilized attentive graph convolutional network achieve geometry-aware facial expression recognition alleviate physical design defect convolution kernel zhao connected attention template adaptive learning position achieve feature extraction large receptive field gcns focus original physical connection joint method based certain prior knowledge self-attention focus action connection joint adaptive relationship joint model focus important joint making difficult disentangle lightweight relationship remaining entangled joint proposed method take account two connection relationship mentioned defines relationship two form hyperparameters method preliminary input action sequence series multi-frame skeleton joint coordinate let t\times j\times input skeleton sequence t\times t\times t\times denote frame joint coordinate respectively represents number input channel previous work people used method multi-scale gcn extract spatial feature skeleton sequence remarkably work improved weight bias problem adjacency matrix disentangling neighborhood improving feature extraction ability gcn resulting multi-scale gcn formula aligned l+1 =\sigma k=0 -\frac -\frac aligned -\frac -\frac normalized k-adjacency represents feature layer frame denotes learnable weight matrix layer network activation function relu difficult find effectiveness multi-scale gcn cost parameter redundancy adjacency matrix represent one connection relationship obtain dependency one joint others must expand value nturgbd dataset kinetics skeleton dataset value respectively time still problem gcns processing method artificially assign exact distance joint unified value example value corresponding joint spine relative forehead knee mean dependency unreasonable reduce ability gcns extract feature empirically dependency joint different element value adjacency matrix different gcns achieve condition however self-attention solve problem well figure show advantage disadvantage two method formula self-attention aligned attention softmax aligned data obtained input feature linear layer constant designed keep gradient value model stable training process usually taking number channel softmax normalize matrix generated ^t.\ figure consider multi-scale feature aggregation skeleton-based graph structure node red left unbiased weight k-hop different k-hops cause indiscriminate feature aggregation addition massive value exist matrix obtained splicing adjacency matrix different hop resulting parameter redundancy right work propose adjust multi-scale feature aggregation exploiting attention-based strategy adaptive processing used generate adaptive correlation matrix making sure every node correlation matrix get different correlation different k-hops different color denote node correlation full size image explain effectiveness attention mechanism feature extraction perspective vector skeleton sequence input model matrix-vector three set data c\times t\times c\times t\times c\times t\times obtained three linear layer fact linear mapping input data reason directly used replace improve fitting ability model consider channel dimension ^t\ obtain j\times matrix element value square matrix physically represent degree correlation row vector represents degree correlation m\textrm joint n\textrm joint matrix achieve function adjacency matrix gcns normalized spatial dependency joint determined unlike adjacency matrix element correlation matrix arbitrary value indicating dependency joint different multiplied achieve purpose aggregating feature algorithm calculate alien relative coordinate full size image data preprocessing key human action recognition extract rich highly recognizable feature limited data therefore considerable enrich original data artificially work preprocessing module data input main frame consists four part original data information velocity information acceleration information alien relative coordinate information according content section preliminary original human skeleton sequence expressed t\times j\times calculate speed information sequence difference operation subtracting previous frame original motion data corresponding calculation formula aligned vel aligned ... t-1\ noted speed sequence t-1\ frame facilitate subsequent calculation keep first frame original data similarly calculate acceleration information sequence differential operation performed based velocity information corresponding calculation formula aligned acc vel vel aligned ... t-1\ also keep first two frame velocity sequence later processing visualization part shown fig usually relative coordinate human skeleton expressed rel i=1 aligned ,\textrm ,\textrm aligned represents index central joint human body shown fig however analyzing dataset found relative position joint central joint hardly change movement physically considered joint perform rigid body motion simply subtracting coordinate joint coordinate center joint body produce little useful information figure acquisition velocity information acceleration information skeleton sequence original series differentiate velocity information first frame original sequence taken first frame velocity information speed information distinguishes acceleration information first two frame regarded first full size image figure schematic diagram acquisition method two different relative coordinate left conventional comparative coordinate acquisition method joint center joint differentiated right human body divided five part part center coordinate coordinate system brown joint origin piece full size image enrich data feature reduce parameter redundancy caused time-invariant feature design processing module alien relative coordinate specifically divide human body five part determine five joint whose relative position central joint almost unchanged movement shown fig order solve problem lack change rotation scaling mathematically use central joint part origin construct coordinate system benefit application moving frame differential geometry taking part left arm example artificially set bone neck left shoulder axis set coordinate joint neck hyperparameter represents scaling relationship coordinate transformation transformation relationship coordinate system aligned o_i =\textrm +\mathrm r_i aligned o_i represent joint coordinate o_i\ coordinate system respectively represents rotation relationship two coordinate system r_i\ represents translation relationship two coordinate system rotation relationship o_i\ coordinate system obtained aligned o_i -\mathrm r_i pinv aligned pinv represents pseudo-inverse operation matrix calculated substituting coordinate point coordinate system o_i\ coordinate system coordinate rest joint part o_i\ coordinate system calculated algorithm flow method shown alg represents coordinate origin part coordinate system connect four set information channel dimension represent entire data preprocessing module aligned =bn concat rel vel acc aligned rel _v\ acc represent output data original data alien relative coordinate information velocity information acceleration information respectively represents batchnorm layer spatial feature extraction module sfem model competitive result present ms-g3d multi-scale gcn method represent non-connected relationship process skeleton action according content section preliminary two problem processing method need improved amount computation time single-scale gcn represents number scale scale joint correlation considering two problem integrate gcns self-attention mechanism improve illustrated fig figure spatial feature extraction module pink block represent linear layer blue block represent convolutional layer yellow block represent activation layer mean input data linear layer size original data represents way obtain inner product vector red red dotted line represents corresponding addition element value adjacency matrix correlation matrix obtained self-attention hyperparameter represents weighting coefficient adjacency matrix correlation matrix full size image channel dimension feature used feature vector joint original self-attention mechanism inner product feature vector used obtain correlation matrix size j\times correlation matrix mean frame corresponding spatial connection relationship instead using one correlation matrix represent connection frame line physical meaning correlation matrix multiplied input feature spatial feature skeleton sequence obtained finally trainable convolutional layer used correct extracted spatial feature mathematical description follows aligned l+1 =\sigma softmax aligned represent feature layer frame difficult find self-attention well express action connection relationship joint process model training distance doe limit correlation joint value method improve problem caused multi-scale gcn time since physical connection feature vector way specifically consider structural connection joint take account inherent structural connection joint use broadcasting mechanism copy adjacency matrix time add adjacency matrix obtained correlation matrix obtain new correlation matrix finally input data multiplied new correlation matrix going linear layer residual structure added obtain final spatial feature entire spatial feature extraction module represented aligned l+1 =\sigma softmax +\alpha aligned -\frac -\frac hyperparameter represents weighted relationship correlation matrix obtained self-attention adjacency matrix inherent gcn denotes learnable weight matrix layer network activation function residual connection module realized temporal feature extraction module tfem frame human skeleton sequence characteristic linear arrangement allows researcher directly use convolution method aggregate information frame extract temporal feature skeleton sequence commonly used method tcn convolution kernel dimension r\times aggregate information one joint frame way doe ability model capture characteristic change joint long period necessary preprocess feature relationship frame alleviate problem performing local information aggregation naturally analogize attention mechanism used section spatial feature extraction module sfem shown fig figure temporal feature extraction module composed frame feature joint feature ms-tcn series stride first block stride rest frame feature linear represents linear layer view represents dimension transformation function data concat indicates data connected time dimension joint feature identity represents placeholder function conv x\times represents two-dimensional convolutional layer convolution kernel x\times hyperparameters full size image cause obtain feature change joint different frame according correlation matrix size t\times calculated first matrix represents correlation joint different frame multiply correlation matrix original sequence get characteristic joint long-term distance based feature use convolutional layer kernel dimension r\times extract feature joint short temporal distance shown fig mathematical description part follows aligned l+1 =\sigma softmax res\right aligned represent feature layer j\textrm joint dimension softmax 0,1 ... j−1 feature extraction considered point view joint generally human follow principle first whole part recognizing action take frame whole find relatively important frame first process joint information frame previous work ignore necessity holistic consideration module use attention mechanism realize overall consideration temporal feature shown fig since feature frame expressed form matrix size j\times matrix calculated inner product need converted one-dimensional vector slice matrix joint dimension splice vector obtained slice obtain one-dimensional vector dimension jc\times let vector feature vector frame inner product calculation obtain correlation matrix frame since information based frame whole relatively general inevitably cause redundancy information reduce accuracy model correlation frame calculated part let frame group calculate correlation frame hyperparameter result group spliced together extracted temporal feature mathematical expression temporal feature extraction taking frame whole shown aligned l+1 =\sigma concat softmax res\right aligned dimension t/g-1\ concat indicates result group concatenated time dimension whole temporal feature extraction module composed frame feature joint feature tcn series shown fig spatio-temporal feature extraction module stfem sfem tfem extract spatial temporal feature skeleton sequence respectively principle analysis section spatial feature extraction module sfem temporal feature extraction module tfem seen two module separated capture complex spatio-temporal association empirically spatio-temporal feature joint provide lot adequate information human action recognition task g3d author artificially determined connection relationship joint consecutive frame designing spatio-temporal adjacency matrix shown realizes spatio-temporal feature modeling joint aligned _\tau =\left array ccc array aligned however single frame still determines connection joint multi-frame due limitation adjacency matrix joint generalized modeling achieved original paper author used multi-scale method alleviate problem achieved specific result also increased computational load model inspired work principle section spatial feature extraction module sfem use self-attention construct correlation matrix across space time complete modeling spatio-temporal feature skeleton sequence module first calculate correlation matrix joint frame concatenate adjacency matrix representing joint connection relationship single frame add calculated correlation matrix concatenated adjacency matrix addition hyperparameter also used control proportion adjacency matrix entire spatio-temporal feature extraction module represented aligned i\_ l+1 _t=\sigma softmax +\beta _\tau +res\right aligned represents joint relationship consecutive frame _t\ _t\ _t\ represent feature frame layer linear layer t=0 ... t-1\ data whose index exceeds filled overall framework inspired dual-stream network idea multi-scale overall framework paper multi-scale dual-stream network shown fig specifically dual-stream network representation joint bone data train separate model architecture respectively weight softmax score joints/bones get final prediction score bone feature obtained differentiating adjacent joint far center body noted ensure bone joint feature use architecture add zero bone vector center human body obtain bone join adjacency matrix used define connection relationship figure overall stream frame represents data preprocessing module appears original data stream indicates number basic block stream r=3\ gap represent global average pooling fully connected respectively i=1,2,3,4 indicates proportion result different stream overall framework represents addition corresponding element value matrix full size image found experiment using original joint calibration method make data lack generalizing information beneficial task human motion recognition use multi-scale perform fusion operation original data shown fig barycenter multiple adjacent joint data taken generalized information mean value part input model training finally softmax score softmax score obtained original scale weighted summed obtain final score experiment conduct experiment verify effect proposed model three large-scale skeleton action recognition datasets ntu-rgb+d ntu-rgb+d kinetics-skeleton datasets ntu-rgb+d ntu-rgb+d large-scale motion data set open-sourced rose laboratory nanyang technological university singapore dataset contains type action category daily behavior action category health-related action remaining category two-person interactive action action performed individual joint data includes three-dimensional space coordinate joint point two division criterion dividing training set test set cross-subject x-sub training set test set divided according person training set 40,320 sample test set 16,560 sample cross-view x-view training set test set divided according camera sample collected camera used test set collected camera training set ntu-rgb+d ntu-rgb+d extends category based ntu-rgb+d expanded data set total sample ntu-rgb+d erroneous sample ntu-rgb+d dataset effective number sample excluding author replaces cross-view ntu-rgb+d crosssetup x-set dataset sample collected half camera used training set sample total rest used test set cross-subject half sample collected volunteer selected training set total 63,026 sample rest used test set kinetics skeleton kinetics skeleton dataset extracted based kinetics video dataset contains type skeleton sample sample training set sample test set sample includes joint feature joint composed space coordinate openpose prediction confidence score meanwhile upper limit number skeleton frame time implementation unless otherwise stated model trained sgd momentum 0.9 loss function cross entropy batch size per worker initial learning rate 0.05 linearly scale batch size epoch step decay factor 0.1 epoch ntu-rgb+d kinetics skeleton respectively corresponding interaction 1260/1184 sub/view 1602/1968 sub/set respectively weight decay set 0.0005 final model adjusted accordingly component study skeleton sequence padded frame replaying action model implemented using pytorch deep learning framework experiment conducted two rtx 24gb gpus comparison state art compare proposed amanet current sota model ntu60 ntu120 kinetics-skeleton datasets result comparison shown table classify sota model three type namely early traditional method gcn-based method self-attention-based method according experiment result three observation follows compared current sota model traditional method absolute disadvantage hcn 86.5 91.1 x-sub x-view ntu st-lstm 55.7 57.9 ntu far lower accuracy gcn-based method self-attention-based method gcn-based model outperform attention-based model classic model-based gcn accuracy ms-g3d reach 91.5 96.2 ntu 86.9 88.4 ntu contrast performance self-attention method-based model promising example accuracy ka-agtn 90.4 96.1 ntu 86.1 88.0 ntu method show best result compared work amanet get accuracy 92.1 96.6 ntu 86.7 88.6 ntu kinetics model 0.2 0.1 higher ms-g3d ka-agtn standard top1 0.3 0.1 higher standard top5 reach level sota dataset table comparison recognition accuracy state-of-the-art method ntu-rgb+d ntu-rgb+d kinetics skeleton datasets full size table according defect adjacency matrix original gcn express explicit relationship joint researcher improved adjacency matrix aggregate implicit relationship joint certain extent improvement often accompanied parameter redundancy method nlp field text data processed self-attention explicit implicit difference congenital disadvantage migrating skeleton data however denied high research value field based analysis aim integrate gcns self-attention proposed amanet accuracy outperform ms-g3d ka-agtn margin 0.6 0.4 1.7 0.5 ntu 0.2 0.2 0.6 0.6 ntu respectively believe main reason amanet effective x-sub ntu120 dataset amanet modified basis framework built gcn integration attention framework fully reflect ability amanet extract feature doe affect feasibility fusion method action recognition task ablation study section verify effectiveness various part proposed section method including sfem tfem staf ms-ds baseline selected performance ms-g3d cross-subject setting ntu-rgb+d dataset illustrate effectiveness generalization ability module directly add module basis baseline shown table based original data velocity acceleration information added baseline improved 0.6 coupled general relative coordinate information model achieve 0.8 improvement general relative coordinate information replaced alien relative coordinate information designed section data preprocessing model achieve 1.1 improvement obtaining relative coordinate information hyperparameter represents scaling relationship original data relative coordinate information greatly affect accuracy model compare accuracy model different value fig seen result figure accuracy model show approximately symmetrical distribution change accuracy model highest 1.2 also input general relative coordinate information relative coordinate information model time result show result method 0.3 lower adding relative coordinate information indicating conflict two kind information interfere training model table comparison recognition accuracy model obtained adding different input information data preprocessing module mean adding information module full size table figure visual comparison different value data preprocessing block whether compared reproduced result theoretical result paper baseline adding module achieve higher accuracy full size image sfem two baseline part ablation experiment one make spatial scale ms-g3d experimental result shown table prove correlation matrix better reflect connection joint initial adjacency matrix replace ms-gcn stem model accuracy improved 0.4 0.8 0.3 lower baseline indicating multi-scale method still competitive add sfem directly front ms-gcn accuracy model reach 89.5 0.4 higher using ms-gcn model increase parameter 0.1 indicating self-attention make model pay attention feature ignored gcns method visualization result upper fig also support conclusion table replace ms-gcn baseline sfem full size table table compare accuracy model gcn ms-gcn sfem used spatial feature extractor respectively full size table figure visualized validation upper bottom represent clapping watching time respectively normalized adjacency matrix skeleton graph corresponding nturgbd dataset trained correlation matrix outputted last layer amanet =0\ matrix fusion adjacent correlation matrix =0.8\ fusion matrix =1.5\ fusion matrix =0.8\ full size image tfem order demonstrate effectiveness module temporal feature extraction module tfem conducted experiment shown table baseline time feature extraction module composed two ms-tcns series first replace one joint-based feature extraction module improves accuracy model 0.2 indicating effective conduct preliminary attention analysis entire skeleton sequence short-range feature extraction joint replace one ms-tcns frame-based feature extraction module improves accuracy model 0.3 indicating grouping operation avoid information interference long-distance frame joint frame treated whole finally form fig concatenate three module whole improves accuracy model 0.5 three hyperparameters module mean use convolution dimension r\times extract time characteristic consecutive frame skeleton sequence tfem utilize ms-tcn complete work therefore r=1\ principle analysis section temporal feature extraction module tfem divide skeleton sequence group extract feature g-frames data group since information frame unit relatively vague feature short-term distance extracted naturally take value hyperparameter represents number frame per group hyperparameter represents number aggregated short-term distance frame g=r\ according result table g=5\ accuracy model reach 89.6 0.5 higher baseline number parameter model increased 0.6m table replace ms-tcn baseline tfem full size table stfem experimental result part shown table like sfem two baseline part ablation experiment one set spatiotemporal scale k=1 g3d first order prove correlation matrix better reflect spatiotemporal joint relationship skeleton sequence adjacency matrix replace g3d module baseline stfem hyperparameter set 0.8 accuracy model improved 0.2 0.4 lower baseline time hyperparameter small 0.1 large 1.5 accuracy model low indicating importance inherent connection relationship joint relationship obtained correlation matrix relatively balanced added stfem directly front g3d module model accuracy 89.3 0.2 higher using g3d alone indicating relationship skeleton sequence obtained using correlation matrix complement joint relationship represented adjacency matrix table compare accuracy model using g3d ms-g3d stfem feature extractor spatio-temporal fusion respectively full size table table replace ms-g3d baseline stfem full size table ms-ds shown fig dual-stream architecture add softmax result obtained different data correspondingly score change previous non-first first accuracy improved seen table result obtained joint data bone data 89.2 90.4 respectively weighted summation obtain accuracy 91.7 coupled result obtained barycenter data proposed section overall framework accuracy entire model reach 92.1 highly competitive current sota figure visualization accuracy top1−top5 various stream x-sub ntu60 dataset j-f b-f represent fused barycenter data joint bone respectively full size image table compare accuracy different stream model x-sub ntu60 dataset full size table order ablation amanet three different module extract spatial sfem temporal tfem spatiotemporal entangled feature stfem among sfem tfem sequential mentioned woo module different function order may affect overall performance changed order connection relationship sfem tfem obtain overall performance result table show extracting spatial feature first temporal feature overall model various indicator advanced reverse table impact different order sfem tfem model accuracy full size table limitation analysis sfem exploit adjacency matrix gcn correlation matrix self-attention characterize physical connection motion connection joint respectively utilize hyperparameter determine weight relationship two matrix intuitively make model adaptively fit optimal value taking trainable parameter found ablation study treated hyperparameter value 0.8 model achieve better result weight physical motion connection different joint variable action clap hand focus model motion connection hand joint weight physical connection relatively weak joint still require physical connection maintain basic skeleton structure value also variable different action watch time addition action connection hand joint fine-grained movement head neck joint equally important selecting -value clapping action inevitable model ignore strengthen proportion relevant feature merely dataset currently used suitable situation 0.8 comparison upper bottom fig characterizes limitation conclusion paper present adaptively multi-correlations aggregation network amanet improving skeleton-based action recognition firstly data preprocessing module designed skeleton data enhancement design three key module spatial feature extraction module sfem capture motion dependency spatial-wise channel temporal feature extraction module tfem exploit temporal information motion joint spatio-temporal feature extraction module stfem comprehensively model motion dynamic extensive experiment proven effectiveness amanet however integration self-attention brought significant improvement amanet since doe fully utilize powerful feature extraction ability self-attention future work seek build framework conforms mechanism according fusion method attention-based method gcns-based method stimulate better performance improvement