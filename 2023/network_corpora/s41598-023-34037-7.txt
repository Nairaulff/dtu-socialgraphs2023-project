introduction possible thanks advance sequencing technology genetic material explore gene expression profile individual cell higher coverage greater resolution dynamic nature transcriptome single-cell rna sequencing scrna-seq provides unprecedented opportunity extracting underlying biological information large amount data consequently caused exponential increase number cell analyzed triggered need efficient computational method effective computational analysis approach developed recent year involving several step quality control mapping quantification dimensionality reduction clustering finding trajectory identifying differentially expressed gene hand scrna-seq technology suffers crucial technical challenge high dimensionality sparse representation true transcriptome single cell high dimensionality scrna-seq data prevents visual exploration hamper downstream analysis clustering lineage inference dimensionality reduction method employed alleviate problem creating faithful lower dimensional representation data so-called embeddings commonly used algorithm scrna-seq include principal component analysis pca t-distributed stochastic neighbor embedding t-sne uniform manifold approximation projection umap well variation three among algorithm t-sne stood one top performer term accuracy computing cost despite crucial role scrna-seq analysis impressive performance t-sne might always produce precisely accurate representation containing erroneously embedded sample could lead inaccurate interpretation research finding generally ranking-based metric human judgment may biased depending level expertise used evaluate quality embeddings ranking-based metric focus retaining local neighborhood ranking high low dimension instead considering preservation ground truth target label label-based also label-based error detection confidence estimation method developed specifically t-sne embeddings similar way many domain medical image registration stereo matching make label-based confidence estimation algorithm unique generates confidence score every sample t-sne embedding supervised random forest regression algorithm based target class label six different distance measure utilized feature regressor chosen domain-specifically rather common one argue based previous study choice distance measure strong influence overall performance algorithm first contribution explored contribution different distance measure predict sample-based confidence score t-sne embeddings found best possible distance measure used estimating confidence score embedding particularly scrna-seq data t-sne one key element downstream analysis scrna-seq data clustering although providing impressive result term detecting unreliable sample embeddings score shown effective clustering second contribution showed confidence score used increase performance k-means density-based spatial clustering application noise dbscan clustering algorithm result seven top performing distance measure revealed based performance first task find performance different distance measure predict sample-based confidence score t-sne embeddings distance measure presented table accordance trained regressor training set amb18 baron human datasets using distance measure individually feature next evaluate performance distance measure intra-dataset experiment predicted confidence score sample embedding sorted descending order calculated score indicating number error last sample lowest confidence score neighbor preservation ratio npr score baron human amb18 datasets shown table considering individual performance distance measure datasets also proven success high-dimensional space seven best distance measure braycurtis correlation cosine kullback-leibler pearson wiad chosen table number erroneous sample correctly detected lowest npr score score obtained training baron human amb18 testing test set individual distance measure distance measure perform well jointly datasets marked bold full size table chosen distance measure detected erroneous sample embedding better previous model npr trained regressor using grid search fine-tuning two different training set amb18 baron human together generated confidence score used algorithm predict confidence score datasets inter- intra-datasets study examined well suggested model performed model created two distinct domain different distance measure yigin kept split train test set ratio 80/20 shared datasets experiment fair comparison kept performance evaluation method systematically ass performance confidence estimation algorithm number successfully detected erroneous sample lowest npr-scored rf-scored sample calculated summarized table since number show successfully-revealed erroneous sample embedding higher better performance table total number erroneous sample dataset also provided since ratio correctly detected erroneous sample could useful interpreting performance difference datasets case prediction generally showed higher concordance ground truth erroneous sample comparison yigin npr score noteworthy success detecting erroneous sample increased much higher rate particularly lowest rf-scored sample even occasionally success attained result signify importance chosen distance measure predicting confidence measure accurately addition quantitative result fig show t-sne visualization segertsolpe baron mouse datasets respectively cell color-coded based cell-type annotation fig red circle represent erroneously embedded sample green star reflect correctly detected erroneous sample confidence estimation algorithm predicted erroneous sample confidence estimation algorithm calculated sample lowest confidence score predicted erroneous sample match ground truth erroneous sample marked star figure show heat map distribution confidence score t-sne distribution also perceptually apparent erroneous sample indicated fig relatively lower confidence score sample figure t-sne embeddings segerstolpe top baron mouse bottom datasets erroneous sample marked red circle correctly detected erroneous sample marked green star heat map show confidence score sample embeddings full size image clustering performance k-means dbscan algorithm improved elimination low-confident sample although providing exciting result confidence measure embeddings yet used downstream analysis contribution yet shown particular clustering investigate using confidence score improve performance downstream analysis scrna-seq data employed two clustering method k-means dbscan eliminating low-confident sample corresponding 0.4 data explained section investigation impact confidence score clustering algorithm subsequently applied four version implementation collection scrna-seq datasets compared performance using evaluation measure including ari nmi acc result k-means dbscan clustering algorithm elimination rate 0.4 shown fig respectively evaluation result elimination rate summarized fig fig clustering algorithm supplementary material result various elimination rate show best performance improvement obtained elimination rate 0.4 may appear prudent select elimination rate 0.4 study leave choose elimination rate user choice hyperparameter set accordance sensitivity targeted study case clustering algorithm inclusion confidence score substantially increased clustering performance figure evaluation k-means clustering performance whole dataset without confidence score dataset high-confident sample dataset eliminated sample positioned existing cluster respectively elimination rate chosen 0.4 elimination rate corresponds threshold value confidence score 0.985 0.824 0.946 0.859 0.825 0.873 datasets respectively full size image figure evaluation dbscan clustering performance whole dataset without confidence score dataset high-confident sample dataset eliminated sample positioned existing cluster respectively elimination rate chosen 0.4 elimination rate corresponds threshold value confidence score 0.985 0.824 0.946 0.859 0.825 0.873 datasets respectively full size image discussion ultra-high throughput scrna-seq technique resulted various new computational challenge normalization dimensionality reduction clustering differential expression analysis given large impact assessment dimensionality reduction downstream analysis meaningful biological discovery important find quantification method applicable kind embedding work presented rf-based confidence estimation algorithm predicting confidence score sample embeddings demonstrated confidence score could utilized enhance performance clustering scrna-seq data using variety distance measure line study literature demonstrated choice similarity measure significant impact confidence estimation comparison distance measure specifically scrna-seq data correlation-based measure correlation pearson etc outperformed distance-based measure euclidean manhattan chebyshev etc line result experimental result table show proposed method detects erroneous sample embedding better previous model npr-scored result main reason result previous study performed worse proposed algorithm choice distance measure confidence estimation made domain-specifically resulting improvement clearly demonstrates significance appropriate distance measure selection although proposed algorithm performs better datasets compared npr score seen proposed algorithm slightly lower number correctly detected erroneous sample amb18 cellbench muraro datasets inherently lower number erroneous sample embeddings table number erroneous sample lowest confidence score based top seven distance measure score npr score score obtained training amb18 baron human datasets tested test set bold number indicate best result test set top performance indicated bold full size table seen fig proposed confidence estimation algorithm tends find erroneous sample inter-cluster transition region rather cluster center hand datasets small number erroneous sample cause problem imbalanced data confidence estimation algorithm tends produce higher confidence score situation prevents selection gold standard elimination rate elimination low-confident sample performance improvement obtained confidence score integrated dbscan algorithm higher k-means method seen fig observed eliminate low-confident sample facilitates determination optimal value parameter dbscan algorithm therefore may help produce better cluster high nmi ari acc value argue better performance increase dbscan compared k-means due difference number parameter need adjusted two hyperparameters dbscan algorithm minpts set effectively confident sample parameter k-means algorithm furthermore performance increase difference among datasets related sample distribution datasets instance segerstolpe dataset imbalanced class distribution cellbench dataset result elimination minority class sample consequence clustering performance improvement significantly lower method t-sne algorithm proposed maaten used obtain lower-dimensional representation high-dimensional datasets utilized t-sne implementation scikit-learn default value perplexity number component definition erroneous/correct sample confidence score according local neighborhood using approach yigin looking local neighborhood embedding checked whether sample embedding shared label majority sample nearest neighborhood k=20\ selected sample considered correctly embedded label least case neighbor erroneously embedded different label majority similarly obtain ground truth confidence score calculating ratio neighbor label sample confidence value thus generated range denoting lowest possible confidence representing highest possible confidence neighborhood preservation ratio npr npr metric comparing performance dimensionality reduction outcome also utilized maaten npr quantifies extent nearest-neighbor distance original space correctly preserved low-dimensional space based euclidean distance used npr score baseline compare confidence estimation calculate npr similar calculated intersection amount closest neighbor original low-dimensional space point selected lowest euclidean distance low-dimensional space n_d x_i,1 k+1 original space n_d x_i,1 k+1 npr ratio number preserved neighborhood aligned npr x_i n_d x_i,1 k+1 n_d x_i,1 k+1 aligned chosen number nearest neighbor n_d x_i,1 k+1 n_d x_i,1 k+1 calculates number co-existing point original low-dimensional space extraction distance-based feature investigation impact confidence estimation estimation confidence distance neighbor original low-dimensional space identified different distance measure several study conducted analyze performance algorithm affected choice distance measure k-nearest neighbor knn classifier image recognition clustering algorithm study conclude choice distance measure substantial impact performance algorithm since found considerable variation result different distance also confirm single distance measure optimized datasets appropriate distance measure given study determined specifically domain containing much similar data possible common distance measure namely euclidean cosine correlation chebyshev canberra braycurtis used extract feature datasets different domain prediction confidence score however yet examine effect different measure performance confidence estimation algorithm study attempted bridge gap examining wide range distance measure particularly scrna-seq datasets order investigate distance measure yield best confidence estimation result review presented abu alfeilat performance knn classifier using different distance measure analyzed different datasets.these distance measure classified part eight major distance family distance measure used study chosen among distance measure used review according performance accuracy value 0.75 typically used selection criterion exceptional decision made excluding measure similar including highest performer category addition mutual information similarity measure included review also included study comparison high performance various task comprehensive information distance measure found table supplementary material contains list distance measurement employed work along explanation selection initially evaluated individual performance distance measure amb18 baron human datasets order determine best-performing distance measure datasets using one distance measure time feature model trained evaluated confidence estimation algorithm extracted feature neighborhood around sample choosing used approach yigin calculate distance neighbor original low-dimensional space first sorted nearest neighbor space according euclidean distance distance calculated separately selection distance measure algorithm based performance also proven success high-dimensional space demonstrated numerous study literature since calculation distance measure constituted computationally expensive part algorithm aimed limit number used distance measure many study demonstrated functionality cosine similarity kullback-leibler divergence clustering effectively measure dis similarity cluster especially dealing high-dimensional data natural language processing application sc-rna seq analysis therefore despite availability distance measure demonstrate comparable performance jaccard motyka two measure specifically included study selecting joint best performer distance measure amb18 baron human datasets concatenated seven distance measure order feed input feature algorithm joint best distance measure braycurtis correlation cosine dice kullback-leibler pearson whittaker index association distance wiad rf-based confidence estimation algorithm trained regressor predict confidence score using distance measure feature ground-truth confidence score target decision tree framework ensemble learning technique combined regressor produce many randomly selected decision tree data averaged produce new result frequently yield accurate prediction thus preventing overfitting utilized sklearn module train regression model training set amb18 baron human datasets separately evaluated model test set dataset intra-dataset test set datasets inter-dataset kept model structure performed 3-fold cross-validation conducted grid search hyperparameter tuning adhering hyperparameter grid order obtain result comparable yigin investigation impact confidence score clustering algorithm performed clustering experiment datasets used confidence estimation task first reduced dimensionality data pca apply clustering algorithm experiment ranked estimated confidence value obtain four different version datasets eliminating least reliable sample 40\ 30\ 20\ 10\ observed difference performance k-means density-based spatial clustering application noise dbscan clustering algorithm frequently used clustering algorithm scrna-seq analysis measured clustering quality original dataset potential deviation may occur obtained confidence value sample removed low-confident sample according certain elimination rate measured clustering performance data confident sample label fig cluster center determined high-confident sample eliminated sample assigned cluster according distance cluster center confidence score label fig way able compare quality cluster entire dataset without using confidence score dataset consists high-confident sample elimination low-confident sample data positioning eliminated low-confident sample existing cluster clustering result vary considerably depending value hyperparameters employed couple criterion provide clustering consistency among comparison essential hyperparameter k-means algorithm number cluster called determine optimal value applied elbow method widely adopted clustering analysis depicts sum squared error function search elbow point employ kneelocator function kneed package proposed satopaa find location optimal cluster number essential hyperparameters dbscan algorithm epsilon minimum number point minpts value determine optimal value dbscan use method proposed rahmah method first average distance point nearest neighbor calculated plotted ascending order optimal value determined finding point maximum curvature ascending curve graph kneelocator function hand way determine value minpts automatically value general guideline choosing minpts value sander suggested selecting minpts dimension dataset suggested minpts value increased size dataset therefore simply selected minpts round ns/1000 dimension dataset number sample dataset automatically set hyperparameters clustering algorithm observe clustering performance elimination low-confident sample used three index cluster quality assessment adjusted rank index ari normalized mutual information nmi clustering accuracy acc ari nmi commonly used index measurement partition diversity quality calculated ari nmi score using sklearn.metrics.adjusted_rand_score sklearn.metrics.normalized_mutual_info_score scikit-learn library respectively necessary modify accuracy formula used classification method clustering since clustering algorithm doe provide relationship predicted cluster label ground truth class label calculate accuracy clustering result confusion matrix random order generated accuracy calculated reordering row column confusion matrix using hungarian algorithm sum diagonal value maximal use accuracy function coclust.evaluation module adopts approach calculate accuracy value datasets study analyzed five scrna-seq datasets include mouse brain cell human mouse pancreatic cell datasets used study provided abdelaal available zenodo repository http except amb18 provided michielsen brief description dataset shown table experiment used amb18 baron human datasets training testing splitting 20\ dataset testing rest training datasets used testing datasets preprocessed abdelaal explained following methodology cpm count per million read count normalization log2 count+1 transformation applied data clustering experiment table scrna-seq datasets used study full size table conclusion advancement sequencing technology enables produce ever growing data set containing rna expression level thousand gene million cell common approach downstream analysis pipeline scrna-seq data dimensionality reduction typically performed using t-sne visualising data two dimension although generally work well revealing local structure high-dimensional data prone error algorithm may result potentially misleading interpretation develop model assigns confidence score sample embedding order prevent misleading interpretation well make subsequent analysis step reliable study showed domain-specific selection appropriate distance measure confidence estimation algorithm substantially improve success capturing erroneously embedded sample t-sne embeddings furthermore examined clustering algorithm one downstream analysis step possible application confidence score showed confidence score information may improve clustering performance well study contributed current research filling gap use confidence score specifically downstream analysis scrna-seq analysis concentrated single-cell transcriptomics data study confidence estimation algorithm broadly applicable dataset domain looking suitable domain-specific distance measure although concentrated t-sne embeddings study approach developed potential successful different dimensionality reduction algorithm proposed approach used develop novel adaptive clustering algorithm make use confidence score feedback generate cluster furthermore proposed approach optimized exploring advanced machine learning method overall believe approach provides valuable contribution field scrna-seq data analysis potential broader application domain