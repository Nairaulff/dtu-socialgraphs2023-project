introduction diabetes mellitus significant global health issue approximately one adult affected worldwide case type diabetes mellitus t2dm diabetes-associated mortality rate increased across age group reported world health organization diabetes complication leading cause disability mortality early diagnosis crucial effective disease management improving life quality patient accordingly several trial conducted predict development diabetes accurately despite multifactorial nature diabetes development study focused predicting diabetes using multivariate multi-instance time series data irregular visit pattern patient varying frequency stay length diversity individual pathology make collecting organizing time series data challenging statistical method time series regression dimension reduction traditionally used disease prediction however recent development deep learning algorithm enabled application deep neural architecture diverse research task including medical data high correlation dimension instance first investigation diabetes multivariate time series prediction using deep learning model introduced possibility applying long short-term memory lstm gated-recurrent unit gru clinical data pima indian dataset pid provided national institute diabetes digestive kidney disease niddk one widely used datasets diabetes prediction dataset high prevalence diabetic outbreak includes several important feature various approach including artificial neural network ann naive bayes decision tree deep learning explored provide effective prognostic tool healthcare professional recent approach successfully incorporated cnn cnn-lstm cnn-bilstm significantly enhancing metric large scale study korean genome epidemiology study koges dataset focused identifying correlation certain factor diabetes development using statistical method study demonstrated correlation diabetes development factor waist circumference prehypertension hypertension glycated hemoglobin level however application time series prediction diabetes using deep learning model based koges dataset thus far limited vanilla lstm model doe reflect characteristic data structure additionally need novel method enhance disease prediction imbalanced datasets significantly larger number feature instance therefore developed sophisticated deep learning framework detect dynamic temporal pattern feature combination label property enhancing diabetes development prediction performance framework includes adequate data preprocessing method feature selection data imputation using koges ansan ansung dataset aimed improve diabetes prediction using multivariate multi-instance time series data time series analysis commonly used technique various field collected data temporal dimension case time series classification frequently benefit enhancement convolutional neural network cnns unlike conventional statistical method relying variance distribution correlation analysis deep learning algorithm developed applied successfully time series analysis recent advance artificial intelligence disease prediction domain time series analysis applied targeting specific feature overall condition patient active research related chronic disease including diabetes hypertension conducted due growing size affected population social interest condition example recent study addressed application deep neural network dnns hypertension well use lstm multi-layer perceptrons mlps heart disease furthermore recent corona virus covid-19 epidemic led various time series prediction task including use deep learning algorithm lstm gru bi-lstm since many researcher continue focus data engineering optimization existing model attempt develop sophisticated deep learning framework novel insight meanwhile recent research time series prediction medical domain focused self-supervised algorithm overcome problem associated inadequately labeled incompletely collected data algorithm aim capture temporal dynamic enable early intervention patient however inherent difficulty detecting progression associated feature still pose problem including multiple covariates progression heterogeneity data storage issue despite challenge time series prediction research continues advance medical field potential significant improvement disease diagnosis management transfer learning methodology used convey information across data neural network three major approach developing model algorithm scheme transfer selection information boundary target task knowledge transfer tuning method pruning layer freezing determining transfer learning performance therefore careful consideration detail transfer learning implementation necessary one method transfer learning involves model weight initialization knowledge acquired source domain transferred target domain initializing model weight alleviating performance target task recent research transfer learning investigates diverse form datasets including time series data image data text data time series prediction study focused multimodal data multitask learning self-supervised approach informative fusion available datasets providing appropriate task result additionally researcher explored selection appropriate source domain among diverse datasets overcome problem frequently encountered time series datasets missing label however shortage large general datasets remain challenge future study human perceive time series data sequentially also whole motivated idea application transfer learning self-supervised learning study focus ongoing temporal self-data addition implementation idea model structure data feature space approach expected improve performance time series prediction model various application present novel approach time series disease prediction adjusting time series data modification time window time resolution manner similar data masking self-supervised learning proposed model framework transfer information including unseen pattern variable temporal property label predict diabetes development individual also apply ensemble technique calibrate multiple learner demonstrating potential application tool early prediction diabetes contribution include introduction novel progressive self-transfer framework time series disease prediction effectively teach dynamic temporal pattern via downstream classifier introduction efficacious method process discrete time series data shifting rolling window modifying time resolution total number model training learn important representative feature increased extensive training evaluation method using large dataset multivariate multi-instance time series given ability handle diverse datasets beyond current study approach potential extensibility overall proposed approach significant potential improve accuracy diabetes prediction may broader implication time series prediction task medical domain result result presented order experimental complexity order conducted begin presenting result baseline experiment serve reference comparison proposed downstream classifier following present result single progressive self-transfer network function downstream classifier finally introduce ensemble result combine multiple classifier non-progressive self-transfer network non-progressive self-transfer network used baseline model comparison estimating performance progressive self-transfer network study introduce four downstream classifier called submodels four non-progressive self-transfer network used predict whether patient distinguished code experience diabetes last time-step dataset despite data every time-step preprocessed data dimension match final prediction task four single progressive self-transfer network description four non-progressive baseline prediction output last time-step dataset serve baseline output four progressive self-transfer network shown table evaluation baseline model revealed four five evaluation category accuracy auc precision score best performance observed data used training however also observed best recall obtained two time step used indicating rest baseline model show significant performance loss table ten iteration four non-progressive self-transfer model result name time-step every two year data collected label predicted auc area curve data reported mean full size table single progressive self-transfer network stage study applied progressive self-transfer learning model task sequence network first task designed based time series data preprocessing method first task served foundation sequential learning task followed sequential stage knowledge previous learning step transferred next learning step weight initialization approach allowed transfer information progressive manner time resulting call progressive self-transfer network table display result single progressive self-transfer network progression approach implemented using two method shifting rolling window modifying time resolution first two model designed using shifting rolling window method next two model preprocessed using method modifying time resolution found result submodel significantly better model four five evaluation metric however evaluation performance model also showed similar superiority additionally comparing result table observed progression performance evaluation metric especially accuracy area curve auc recall table ten iteration four single progressive self-transfer model result submodel submodel submodel submodel downstream classifier best performance evaluation metric among single progressive self-transfer network indicated bold data reported mean full size table multiple progressive self-transfer ensemble network final stage study involved application ensemble technique integrate result single progressive self-transfer network four different network submodel 1â€“4 experimented every possible combination ensemble network identify trend result submodel submodel downstream classifier processed time series data using shifting rolling window method submodel submodel downstream classifier processed time series data doubling tripling time resolution table show ensemble result every combination four model among single progressive self-transfer network first two row display result ensemble two downstream classifier experiment conducted submodel submodel submodel submodel based time series data processing method used next four row show ensemble result three downstream classifier auc recall score indicated improved performance ensemble result compared single downstream classifier result described table last row display ensemble result four downstream classifier four five evaluation metric outperformed result four downstream classifier except precision furthermore fig illustrates schema experimental case described table figure focus auc served criterion determining best epoch stage table ten iteration ensemble result result multiple progressive self-transfer network consists downstream classifier described best auc performance among ensemble result indicated bold data reported mean full size table figure trend evaluation metric particularly auc depicted experiment described full size image found combination submodel submodel submodel produced best performance term auc metric overall thus conducted additional experiment thoroughly inspect validity combination table show ensemble result every possible combination among submodel submodel submodel best performance auc obtained combination submodel submodel submodel outperformed ensemble result four downstream classifier table table ten iteration ensemble result combination submodels best auc performance among ensemble result indicated bold data reported mean full size table furthermore experimental result compared baseline model lstm gru rnn table display performance metric obtained case first three row present metric baseline model utilized dataset hyperparameter setting proposed model comparison last row showcase metric proposed model referring combination submodel submodel submodel exhibited best auc performance shown table overall proposed model outperformed baseline model term auc recall considering highly imbalanced label ratio observe improvement ability detect diabetes patient larger scale additionally given proposed model based multilayered lstm demonstrates best auc among baseline model also see model exhibit similar pattern accuracy precision score compared baseline lstm model table ten iteration baseline model comparison proposed model best performance evaluation metric among single progressive self-transfer network indicated bold data reported mean full size table performance proposed model presented fig gradual improvement model performance noted across evaluation metric highlighting effectiveness proposed approach furthermore interaction single progressive self-transfer network also observed instance accuracy submodel improved significantly surpassing submodels ensemble approach enhanced accuracy four submodels particularly submodel additionally increase recall crucial medical domain prevents missing potential patient proposed model showed promising result improving recall figure trend evaluation metric accuracy auc precision recall score non-progressive self-transfer model single progressive self-transfer network ensemble result submodel submodel submodel descripted full size image discussion given complexity disease unknown interaction related factor accurate prediction diabetes development crucial study proposed progressive self-transfer network incorporates time series data preprocessing method shifting rolling window modifying time resolution reflect feature representation multivariate multi-instance time series analysis proposed method also account dynamic temporal pattern including temporal imbalance label common medical data gradual improvement metric performance shown fig indicates progressive self-transfer network followed ensemble method efficiently integrates employ information added time enabling downstream classifier interpret dataset perspective result demonstrate proposed method effectively recognize previously unseen data pattern transfer acquired knowledge background information sequential task therefore model detect patient earlier enabling early diagnosis intervention furthermore study contributes field utilizing deep learning method time series prediction task koges dataset best knowledge limited study dataset using deep learning technique previous study used conventional algorithm primarily focused identifying association single factor development diabetes contrast approach considers multiple relevant feature predict diabetes development providing comprehensive understanding disease study offer significant contribution field diabetes prediction using time series analysis deep learning method order optimize lstm model used research conducted series experiment adjusting various parameter experiment performed across stage including non-progressive self-transfer network single progressive self-transfer network ensemble application tested lstm model different number layer found model five layer performed slightly better four-layer model metric significant increase standard deviation also manually adjusted dropout rate input unit size additionally optimized method used modify time series data including initial window size expansion time interval since limited number discrete time step chose double triple time resolution allowed finalize structure input output data important note study may potential bias human error issue arise discriminative supervised model one potential source human error labeling process influenced misreported survey response bias introduced clinical measurement moreover study discriminate type diabetes mellitus t1dm t2dm limit understanding participant diabetes mellitus development address limitation future study could focus disease-specific data collection improve reliability label allow discrimination t1dm t2dm additionally incorporating genetic information single nucleotide polymorphism snp could enhance model background knowledge enable personalized patient care snp combination lifestyle habit could serve key factor diabetes development support effective patient intervention conclusion conclusion study present novel progressive self-transfer learning network integrates information time predict diabetes development target time-step remarkable performance method several advantage including mitigating problem accumulated error recursive neural network predicting multiple timepoints utilizing result sequence improving learning important representative feature modification time series data finding implication field digital healthcare chronic disease particularly potential improve clinical efficiency aid early diagnosis intervention disease like diabetes mellitus indeed proposed model help detect patient improved metric finding significantly enhance opportunity early detection promising result method contribute development digital healthcare preventative medicine enhancing expertise healthcare provider improving health outcome patient method korean genome epidemiology study korean genome epidemiology study koges ansan ansung dataset used research koges consortium aim investigate geneticâ€“environmental factor interaction common complex disease korean study ongoing community-based cohort study conducted korea disease control prevention agency kdca ministry health welfare dataset contains biannual medical checkup survey data participant 40â€“69 year old residing either urban ansan rural ansung area cohort baseline 10,030 participant established 2001â€“2002 participant attended last time-step 2017â€“2018 objective study predict whether participant would develop diabetes last time-step american diabetes association ada guideline followed participant considered diabetes fulfilled least one criterion listed table word label time-step generated considering inspection feature based ada criterion survey feature strongly indicate whether participant diagnosed diabetes preceding year label creation based assumption data specific time step influence subsequent diabetes case two year aligning interval koges dataset table criterion diabetes development ogtt oral glucose tolerance test full size table table present label used study time-step label indicates diabetic participant label indicates non-diabetic participant label generated based specific criterion described table inferred data collected time-step participant participated every time-step included proposed network analysis excluding already diabetes first time-step table provides detailed description data used study notably ratio diabetic label non-diabetic label participant show gradual increase table quadrupling first last time-step furthermore proposed framework utilized n-1 data generate label data early prediction next time-step used latest six time-step set data input proposed framework table label information every time-step label refers diabetes development information time-step instance column list label information inferred data 2005â€“2006 column 2007â€“2008 number except last row indicate number people full size table lasso feature selection feature selection crucial step network training ensure optimal model performance efficiency study employed least absolute shrinkage selection operator lasso feature selection identify relevant feature proposed classification task first sorted variable participant every time-step resulting dataset participant feature selected continuous feature missing value time-step resulting final dataset participant feature missing value dataset imputed using bidirectional recurrent imputation time series brit lasso feature selection applied final dataset select relevant feature network training process performing basic preprocessing step lasso adapted feature selection lasso type regularized linear regression control penalty strength shrink insignificant coefficient zero reducing dimensionality minimizing number feature relevant label simultaneously grid search applied find suitable penalty coefficient resulting selection feature positive negative correlation label data coefficient selected feature displayed fig creatinine blood serum hemoglobin whole blood body fat body mass index bmi subscapular measurement identified top five feature largest coefficient magnitude larger coefficient magnitude effective explanatory variable additionally sign coefficient indicates positive negative correlation label data total demographic information selected feature described descriptive statistic table detailed information feature lasso coefficient shown table figure diagram lasso coefficient selected feature coefficient sorted descending order based absolute value full size image brit imputation brit powerful algorithm leverage bidirectional recurrent neural network brnn impute missing value multivariate time series data taking advantage feature mean standard deviation trend notably brit perform imputation classification regression task concurrently acting versatile multi-task learning algorithm study utilized brit imputation twice first applied brit imputation lasso feature selection step mitigate potential bias ensure proper selection appropriate feature model second applied brit imputation training proposed network selected feature instead entire feature set brit algorithm treat missing value variable within bidirectional rnn graph performing missing value imputation classification/regression application simultaneously consequently combination variable impact accuracy imputation thus second application brit imputation help exclude important feature process allowing relevant feature incorporated progressive self-transfer architecture figure present schematic diagram brit application described figure schematic diagram brit application full size image progressive self-transfer framework proposed progressive self-transfer framework implemented using pytorch library trained linux intel core i7-9700 cpu nvidia geforce rtx super gpu environment network trained tested batch size dropout rate 0.2 binary cross entropy loss sigmoid function loss function experiment repeated time obtain mean standard deviation result figure illustrates proposed framework includes multiple submodels utilizes ensemble method soft vote classification score sub-classifiers final prediction submodels multilayered lstm-based self-transfer network train test sequential manner introduction multilayered lstm aim fully capture address dynamic temporal pattern data forget input output gate lstm control unit cell effectively resolving vanishing gradient short-term memory issue traditional rnns forget gate decides information previous cell state discarded input gate determines new information added cell state finally output gate identifies significant information current cell state determine value next hidden state purpose submodels capture dynamic characteristic time series data varying label imbalance among time step framework designed process training testing data sequential manner human perceive time series data sequence also whole simultaneously progressive self-transfer learning scheme allows former learning step provide information latter learning process sequential manner meanwhile baseline model introduced verify explain performance proposed model fully consider uniqueness submodel track improvement evaluation metric non-progressive self-transfer network used baseline model four progressive self-transfer network additionally compared experimental result three well-known recurrent neural network lstm gru rnn model capture temporal pattern sequential data rnn traditional recurrent model output former step becomes input current stage especially suited data relatively short time-steps however sequence length data increase issue like gradient vanishing exploding may occur time gru similar lstm described however consists two gate update gate reset gate update gate determines much information previous time step updated current state reset gate decides information disregard figure overall architecture progressive self-transfer network submodels 1â€“4 example downstream classifier full size image figure illustrates example progressive self-transfer submodel used study network input dimension selected lasso feature selection step model consists sequential task first task predict whether participant develop diabetes time step using data time step second task predict time step using data time step sequential progress achieved gradually adding time step best epoch weight transferred consequent task via weight initialization preprocessing method shifting rolling window modifying time resolution discussed next section figure example progressive self-transfer submodel model trained tested sequential flow lstm box dotted line represent learning step weight transferred former learning step full size image self-supervised learning transfer learning key concept behind proposed network self-supervised learning technique label acquired data data partially used predict part data pretext task task designed improve performance target prediction study pretext task binary prediction diabetes formulated according two time series data processing method self-supervised sequence carried using transfer learning efficiently improved performance pretext task time finetuning necessary without change model architecture furthermore proposed framework increased total number model training iteration compared single whole-data training enabling model better perceive self-data word progressive self-transfer network transferred knowledge sequence capture high-level information self-data time series data preprocessing shifting rolling window following feature-level data preprocessing performed data preprocessing time-step level enable sequential input proposed model framework koges ansan ansung study provides discrete time series data explicit repeated time interval inspection based time window interval two year adopted modification technique involves changing window size rolling time window gradually first method used shifting rolling window approach applied method submodels shown schematic diagram fig submodel length time window gradually increased task proceeded training began window size six year refers first three time step since length single window two year submodel length time window fixed rolled task proceeded consequently sequential input progressively provided time series data following formula defines model input sequence formed shifting rolling time window original data submodel submodel figure schematic illustration submodels employed proposed progressive self-transfer network submodel consists sequential task modified time series data used input network specific order submodel submodel submodel submodel full size image input data model represented defined consists pair feature label data participant appropriate format training sequence pair data composed represent data time step sequential training performed time generalized expression however since time series data preprocessing method submodel submodel different expression diversified shown represents number total time step data represents width initial time window increased rolled training sequence time series data preprocessing modifying time resolution modifying time resolution another method creating sequential input time series data preprocessing approach involves zooming data inspect information different perspective study data collected every two year minimum time resolution two year reflect micro macro aspect data doubled tripled time resolution dataset submodel fig double time resolution simulating window interval four year creating four sequential input submodel fig triple time resolution simulating window interval six year creating three task input following formula used define sequential input developed varying time resolution original data structure input follows formula mentioned earlier however variable dependent skipping series time window interval shown skipping series refers input dataset configured exclude data specific time window accordance change time resolution instance set skipping interval consequently skipping series created omitting every data point starting end moving backwards first time-step reached value specific submodel submodel using skipping interval submodel using skipping interval value determined based total number time step data ensemble final step proposed framework involves ensembling classification score submodels step aim compensate imbalanced label variable feature property prevent overfitting prediction result since submodel unique learning strategy perspective data concatenate perspective via soft voting calculate mean probability weight perspective considered equal find best combination multiple learner ensemble diverse combination submodels submodel 1â€“4 find best evaluation performance evaluation study five-fold cross-validation used validate model prevent overfitting dataset participant divided five fold based binary label last time step fold used validation set consecutive order mean metric validation set used evaluate model performance accuracy auc recall precision score considered overall binary classification evaluation model including non-progressive single self-transfer network ensemble result compared based five metric submodel performance evaluation auc considered important metric determine best epoch continue training sequence reflects classification performance class weight best epoch transferred following similar task based auc