introduction artificial intelligence revolutionizing patient care medical research algorithm rapidly introduced integrated medical practice enhancing human capacity efficiency large language model llm specifically chatgpt gpt-4 gained large scientific attention increasing number publication evaluating model performance medicine llm proven proficient diverse task performance area requiring empathy human judgment remains relatively unexplored united state medical licensing examination usmle pivotal series examination ass wide range skill indispensable practice medicine measure candidate cognitive acuity medical knowledge also ability navigate complex interpersonal system-based scenario uphold patient safety exercise professional legal ethical judgment soft skill critical effective empathetic practice medicine enabling physician resonate patient contribute positively healthcare system march step2 clinical skill exam standard assessing communication interpersonal skill however suspension due covid-19 pandemic underscored need effective validation vital skill pandemic led discontinuation step2 core element clinical communication integrated step usmle remains official benchmark assessing physician communication ability notably usmle step2ck score found predictive performance across various residency performance domain including patient care teamwork professionalism communication artificial cognitive empathy ability mimic human empathy emerging area interest accurate perception response patient emotional state vital effective healthcare delivery understanding capacity particularly relevant telemedicine patient-centered care thus aim study evaluate performance chatgpt gpt-4 responding usmle-style question test empathy human judgment soft skill method large language model study assessed performance model chatgpt gpt-3.5-turbo gpt-4 developed openai http language model designed process generate text mirror human-like conversation achieve predicting next word token come next given input allowing generate coherent contextually relevant response chatgpt earlier version widely used variety application due ability generate coherent contextually relevant response gpt-4 newer paid version similar design much larger scale meaning trained data understand generate text even greater accuracy medical question datasets ass chatgpt gpt-4 used set multiple-choice question designed mirror requirement usmle examination question set compiled two reputable source first set sample test question step1 step2ck step3 released june march available official usmle website http screened example test question selected question require scientific medical knowledge required communication interpersonal skill professionalism legal ethical issue cultural competence organizational behavior leadership second source ambo widely recognized question bank medical practitioner student http selected additional question chosen question include step1 step2ck step3-type question dealing ethical scenario comparable question chosen usmle sample test question ambo provides performance statistic past user allowing comparative analysis llm performance medical student physician chatgpt gpt-4 tasked responding question included study response performance subsequently compared prompting line standard methodological practice assessing language model formatted prompt structure included question text followed multiple-choice answer separated new line following model response asked follow-up question sure allowed evaluate consistency stability model elicit potential reevaluation initial response model change answer may indicate posse level 'uncertainty initial response tracking often circumstance model change answer provide valuable insight model capacity self-revision important aspect learning decision-making system prompt example shown supplementary item result overall performance study total multiple-choice usmle soft-skills question included different subject detailed table fig chatgpt accuracy usmle sample test ambo question 66.6 respectively overall 62.5 accuracy gpt-4 demonstrated superior performance accuracy 86.4 usmle sample test ambo question respectively overall accuracy result also shown table fig table accuracy chatgpt gpt4 usmle-type soft skill question full size table figure performance chatgpt gpt-4 usmle sample exam ambo question full size image consistency table show chatgpt gpt-4 response follow-up question sure initial answer either correct incorrect gpt-4 exhibited change rate offered opportunity revise response regardless whether initial response correct incorrect contrast chatgpt demonstrated significantly higher rate self-revision altering original answer 82.5 time given chance chatgpt revised incorrect original response found model rectified initial error produced correct answer 53.8 case result detailed table table consistency chatgpt gpt-4 full size table llm versus human comparing performance human performance ambo user statistic reported average correct response rate user question utilized study chatgpt showed lower accuracy human user gpt-4 higher accuracy rate 86.4 discussion study evaluated chatgpt gpt-4 performance usmle multiple-choice question assessing soft-skills empathy ethic judgment several key finding chatgpt gpt-4 answered correctly majority question gpt-4 outperformed chatgpt answering question correctly compared chatgpt 62.5 additionally gpt-4 demonstrated complete confidence response unlike chatgpt displayed confidence 17.5 answer previous study demonstrated ability llm chatgpt gpt-4 successfully pas usmle significantly better performance gpt4 chatgpt shown 41â€“65 accuracy across step1 step2ck step3 question whereas gpt4 average score previous study evaluated model across spectrum usmle question without specific focus topic distinguishing medical knowledge-based query soft skill question study focused question require human quality like empathy communication professionalism ethical judgment showed llm demonstrate impressive result question test soft skill required physician superiority gpt-4 confirmed study model correctly answering soft skill question contrast chatgpt accuracy 62.5 result indicate gpt-4 display greater capacity effectively tackle medical knowledge-based question also requiring degree empathy ethical judgment professionalism regarding specific question datasets used study previous study tested performance chatgpt subset step1 step2ck question ambo reported accuracy respectively lower accuracy rate accuracy chatgpt demonstrated study difference may arise use different question ambo large database might also focused specific type question include whole span topic covered usmle study also tested chatgpt usmle example test question selected question study found chatgpt accurate 64.4 step1 57.8 step2 another study tested chatgpt usmle example test question reported accuracy 55.8 59.1 61.3 step1 2ck respectively study chatgpt showed similar slightly higher result accuracy 66.6 beyond question type also noteworthy address behavioral tendency two model chatgpt inclination adjust initial response may suggest design emphasis adaptability flexibility possibly favoring diverse interaction conversational context hand gpt-4 consistency could point robust training sampling mechanism predisposed stability distinction significant highlight difference model inherent adaptability dynamic setting versus consistent output stable context comparing performance human performance ambo user statistic provide invaluable benchmark ambo reported average correct response rate user question utilized study chatgpt showed lower accuracy human user implying shortage relevant soft skill compared medical student doctor gpt-4 surpassed human performance metric suggesting capability solving complicated ethical dilemma demonstrating empathy managing patient family professional manner physician required exhibit potential display empathetic response topic increasing interest notable recent study compared response chatgpt physician patient inquiry social medium platform found chatgpt response viewed empathetic emphasizing potential emulate human-like empathy several limitation study first question pool used study limited including multiple-choice question two different source potential introduction selection bias may accurately reflect actual usmle question may encompass aspect 'soft skill essential medical practice addition consistency level two model assessed based opportunity revise answer however mechanism potential reevaluation might translate human cognition understanding uncertainty model operate based calculated probability output rather human-like confidence simplification potentially limit depth understanding model decision-making process conclude finding underscore potential role llm augmenting human capacity healthcare especially area requiring empathy judgment gpt-4 performance surpassed human performance benchmark emphasizing potential handling complex ethical dilemma involve empathy critical patient management future research consider larger diverse question pool ethical scenario better represent full range soft skill important medical practice study would provide comprehensive understanding llm capability area explore applicability llm real-world clinical setting