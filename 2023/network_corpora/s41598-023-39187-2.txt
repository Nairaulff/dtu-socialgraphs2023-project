introduction main task reactor physic analysis simulate various nuclear process core give key parameter related neutron dynamic nuclear reactor four-factor model six-factor model played important role early physical analysis reactor solution neutron transport equation differential-integral form requires decoupling discretization variable current method angular discretization include spherical harmonic method discrete ordinate method discrete ordinate method individual angular direction instead entire angular space discretize angular variable obtain neutron balance equation specified direction rapid development computing technology method characteristic line moc monte carlo transport calculation method also developed significantly characteristic line method convert neutron transport equation one-dimensional neutron transport equation using series mutually parallel characteristic line covering entire solution region monte carlo method hand involves generating different particle initial position energy emission angle well simulating various process within medium production collision disappearance termination etc. information obtained subjected mathematical statistical analysis utilizing large number random number stochastic model constructed model solved using physical process key solution lie reasonable use large number random process simulate random motion neutron various medium solve contribution motion process certain physical quantity lstm neural network high-speed research neural network deep learning algorithm reinforcement learning largely driving revolution sophisticated algorithm handle complex machine learning task characterized nonlinear relationship interaction feature large number input long short-term memory lstm first proposed neural network specifically proposed solve problem long-term dependence general recurrent neural network cell structure lstm model shown fig calculation formula shown shown figure lstm cell structure diagram full size image time input vector represented weight bias term denoted refers activation function cell structure state value moment represented respectively hyperbolic tangent function used activation function input gate denoted corresponds weight forgetting gate represented corresponding weight similarly output gate denoted corresponding weight cell output value time represented beavrs core introduction beavrs model derived real pressurized water reactor westinghouse basic structure assembly enrichment distribution shown fig figure core arrangement diagram full size image core contains fuel assembly fuel rod assembly arranged lattice assembly contains fuel rod one instrument tube installed center core one instrument tube installed center in-stack measurement guide tube installed around center fuel assembly parameter shown table table fuel assembly parameter full size table table give basic parameter assembly containing burnable absorber rod figure give arrangement combustible absorber assembly table parameter burnable absorber rod full size table figure burnable absorber assembly model burnable absorber rod guide tube instrument tube full size image based beavrs benchmark core description corresponding two-dimensional core fuel burnup calculation model established lattice calculation adopts multi-group two-dimensional transport theory fuel burnup calculation two group cross-sections obtained assembly type fuel type lattice calculation performed using dragon code donjon code used calculate core fuel burnup dragon4.1 donjon4.1 reactor numerical analysis program developed polytechnic university montreal canada among dragon4.1 designed around solution neutron transport equation dragon4.1 lattice code contains several computational module main computational module fine-group micro-section database processing module lib geometric feature description module geo spatial discretization module based collision probability module sybilt excelt nxt discrete coordinate module snt characteristic line module moc resonance processing module includes resonance processing module based equivalence principle resonance processing module includes resonance processing module shi based equivalence principle subgroup resonance module transport equation solving module flu edi evo etc module implemented software package program gan.the module connected together program gan within package data exchanged module well-defined data structure modeling process power chosen constant boron concentration kept constant shown fig fuel rod modeling method chosen lattice calculation assembly form according distribution enrichment absorber including 1.6 enrichment without absorber 2.4 enrichment without absorber 2.4 enrichment absorber fuel rod 2.4 enrichment absorber fuel rod 3.1 enrichment combustible absorber etc 3.1 enrichment assembly absorber fuel rod 3.1 enrichment assembly absorber fuel rod divided different assembly due asymmetry shown fig calculation lattice fuel rod pincell guide tube pincell burnable absorber pincell instrumentation guide tube pincell filled different location according assembly specific geometric material structure shown reference assembly modeling process boundary condition selected reflection transport equation solved collision probability method selection multi-group interface library 69-group cross-section library iaea wimsd4 database selected dragon-readable file format generated njoy program 2-group homogenized few-group interface generated dragon donjon read donjon modeling core modeling selected corresponding assembly filled middle according beavrs core arrangement water reflection layer filled periphery assembly donjon performs core physic calculation reading transport cross-sections various assembies obtained dragon employing coarse mesh finite difference method figure four different assembly absorber fuel rod arrangement method full size image lstm modeling process lstm neural network used predict beavrs core effective multiplication factor eff different hyperparameters taken set prediction model specific process method shown fig figure flow chart lstm-based core effective multiplication eff prediction method full size image data pre-processing effective multiplication factor eff calculated dragon/donjon 0–300 day maximum power sampling frequency one day significant difference range value different feature quantity linear normalization method i.e. maximum normalization method used normalize feature significant quantity achieve better model accuracy formula shown initial feature value eff max feature maximum min feature minimum processed feature value model training study loss function used train model mean squared error mse proportion square difference predicted actual value number sample let sample size anticipated k-effective value actual k-effective value formula mse given deduced lower mse error greater prediction effect model accuracy determined comparing absolute error predicted k-effective value actual value mse training set prediction model formed using processed data hyperparameter setting indicated table trained model used test test set according process outlined based training set time step combined order 1–10 interval depending performance computer used number hidden neuron lstm layer model regularization coefficient 0.001–0.01 interval 0.001 optimizer model regularization coefficient 0.001–0.01 interval 0.001 optimizers selected adam rmsprop adagrad adadelta appropriate number iteration epoch batch size batch callback function callback dropout rate dropout selected table model hyperparameter setting full size table regularization factor conjunction dropout layer minimize model overfitting based occam razor anything two explanation probable true explanation one fewest assumption i.e. straightforward answer given certain training data network design data may explained several weight value i.e. multiple model complex model susceptible overfitting simple one simple model fewer parameter lowering complexity model restricting model weight smaller value weight value distribution becomes regular technique referred weight regularization accomplished adding cost associated bigger weight value network loss function adding regularization factor i.e extra cost proportional square weight coefficient norm weight indicated regularization parameter training sample error without regularization factor loss function dropout refers deep learning training process neural network training unit eliminated network based given probability stochastic gradient descent figure depicts process action prevents model overfitting randomly deleting neuron figure dropout mechanism diagram full size image machine learning several optimization technique used find best model solution contrast rmsprop absence correction factor may result highly biased second-order moment estimate beginning training adam contains bias correction account first-order moment momentum term initialized origin non-central second-order moment estimation analysis result lstm algorithm time step set 1–10 number neural unit regularization coefficient 0.001–0.01 optimizers adam rmsprop adagrad adadelta respectively model first data set build total lstm algorithm model next data set prediction compare error absolute error predicted true value used evaluation index result shown fig figure error map lstm model full size image learned fig problem effective core multiplication factor eff adadelta-based lstm algorithm model best prediction followed rmsprop adam adagrad worst prediction rmsprop adagrad adadelta optimizers average error increase decrease regularization factor increase mean error increase decrease increase adam optimizer adam optimizer increase mean error shown table table mean error variation parameter full size table counting model total model average error pcm model smallest average error counted shown table model smallest average error i.e. time step number cell regularization factor 0.003 optimizer selection adadelta subjected error statistic statistical result shown fig table model error decimal table full size table figure error statistic chart full size image conclusion paper focus exploring feasibility lstm long short-term memory algorithm deep learning effective multiplication factor eff prediction core level modeled beavrs benchmark evaluation validation reactor simulation core first cycle loading eff operating full power 0–300 day used study subject first dataset training validation set last dataset prediction target training alignment result physical parameter assembly obtained using dragon4.1 donjon4.1 code lstm algorithm deep learning applied adjusting number lstm cell regularization parameter optimizer type parameter coefficient algorithm result showed absolute error predicted core effective multiplication factor eff could made within pcm adjusting appropriate parameter validated successful application machine learning transport equation future plan fully leverage advantage big data establish unified model across multiple operating cycle different physical model