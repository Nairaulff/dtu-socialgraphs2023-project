introduction computed tomography powerful imaging technique reveal interior object using x-ray application health care industry life science physic material science well many field synchrotron light source facility x-ray microscopy laboratory time-resolved tomography allows reconstruction dynamically evolving process environment experimental data collected imaging scientist reconstruction often postponed later stage result domain expert little control feedback imaging process experiment may need repeated reconstruction inspected recent advance hardware gpus graphical processing unit cmos complementary metal oxide semiconductor detector technology several synchrotron laboratory developed real-time tomographic pipeline next streamlining data acquisition preprocessing step pipeline achieve reconstruction within millisecond live observation experiment help image scientist domain expert optimize acquisition setting therefore save valuable time storage real-time reconstruction provides valuable insight imaged object spatial distribution x-ray attenuation image processing analysis task often necessary improve evaluate experimental outcome algorithm task perform image enhancement noise artefact removal show whether reconstructed object feature sufficient quality complex case may extract semantic information e.g. object classification counting object instance image segmentation help domain expert evaluation experiment however many image analysis task take much time computed traditional algorithm integration tomographic pipeline therefore yet possible advent deep learning changed new class algorithm called deep neural network dnns particular using convolutional layer cnns show remarkable result wide range image task motion estimation image classification dnns operate timescales required real-time tomography promising next component tomographic pipeline illustrate fig nearby future dnns may able close experimentation loop providing automated feedback experimental control example dnn could automatically identify region interest adapt scanning geometry zoom figure real-time tomographic pipeline measurement acquisition image reconstruction image processing analysis subsequent computational step executed concurrently experiment user feedback dnn automation used steer experiment illustration show dissolving-tablet experiment flex-ray laboratory detailed experimental set-up section full size image practice applying dnns real-time tomographic pipeline straightforward success dnns generally attributed training i.e. optimization algorithmic hyperparameters based data process take place inference i.e. application network unseen data real-time setting make training challenging two main reason first concern acquisition training data image reconstructions—the input neural network—depend object imaged set-up noise level reconstruction algorithm experiment furthermore often repeatable effect user interaction experiment easily anticipated well generating training data sufficient quality quantity previous experiment may therefore difficult relevant training data may simply available second reason lack training time beamline time synchrotron facility scarce expensive making schedule tight train dnn experiment propose overcome challenge taking advantage spatio-temporal continuity sample tomographic pipeline unlike typical data set used train cnns image sample pipeline result continuous process e.g. dynamic experiment mechanic set-up change reconstruction parameter data sample therefore i.i.d sample rather posse high degree spatio-temporal continuity open opportunity train small neural network adapt data experiment simultaneously used inference incoming reconstruction since pattern image likely reappear following second minute say network trained just-in-time inference offer sought flexibility towards changing unpredictable reconstruction fully automated integrated pipeline approach limited network architecture trained quickly applicable learning task require external data however goal perform well current image pipeline small data set required training best knowledge first propose investigate approach cnns article explore new concept aim enable real-time tomography deep learning-based image processing tomographic pipeline purpose replay two experiment recast3d software using data obtained flex-ray laboratory cwi amsterdam experiment entail tablet dissolved fluid inside glass container granular ground see fig experiment feature large spatial structure fast dynamic whereas experiment smaller structure slower dynamic representative study bubble physic important topic material engineering science well dissolution process relevant pharmaceutical research due natural noise artefact experiment allow real-world challenge learning strategy article organized follows first method formalize real-time tomography explain deep learning context self-supervised denoising task called noise2inverse furthermore discus software set-up software contribution subsequent section just-in-time learning introduce three topic identified main challenge stochastic structure real-time data suitable network architecture online learning strategy penultimate section result report finding topic dnns trained experimental data finalize discussion potential remaining challenge just-in-time learning figure illustration recast3d user interface http release v1.1.0 left experiment fast dynamic right experiment slow dynamic experiment reconstruction performed visualized three slice full size image method real-time computed tomography tomographic reconstruction imaging technique probe interior object using penetrating beam x-ray electron real-time x-ray radiographic projection ~\in m_x m_y recorded continuous stream object rotated reconstruction algorithm take last m_\phi\ projection stream usually corresponding full rotation recovers interior object three-dimensional image _t~\in n_x n_y n_z reconstruction algorithm achieves inverting x-ray transform solves _t\ relation t-m_\phi index last projection two major class reconstruction algorithm used practice direct algorithm solve inverse problem analytically formulating solution closed-form equation iterative algorithm hand express inverse optimization objective commonly used direct algorithm filtered-backprojection fbp feldkamp-davis-kress fdk consist single fast filtering operation subsequent application f^t\ adjoint x-ray transform iterative algorithm often better noise-reducing property direct algorithm require multiple application f^t\ increase computational cost time considerably deep-learned image processing analysis task denoising algorithm take reconstructed volume _t\ input produce output _t\ output image also different quantity vector field scalar depending image task denote analysis algorithm mapping _t\ forthcoming section explain recast3d pipeline reconstruction visualization choose image-to-image deep learning component real-time tomographic pipeline process experimentation visualization/automation i.e. different component fig taking place concurrently using software buffer order efficient result multiple reconstruction analysis output generated projection time brevity reuse subscript amongst _t\ _t\ _t\ experimental set-up recast3d high-resolution image reconstruction pose difficulty real-time application compute full volume potential resolution data algorithm take several minute computation time even modern hardware efficient direct solver considered enable visualization analysis much faster frame rate recast3d software real-time reconstruction limit reconstruction user-selected slice volume recast3d graphical user interface shown fig recast3d fbp fdk algorithm thanks linearity computational structure allow reconstructing region interest computation time linearly related number voxels region result called quasi-3d reconstruction permit refresh-rate millisecond graphical user interface user may add remove reposition slice allows fast interrogation object experiment recast3d used real-time alignment explorative imaging visualization experiment quickly evolving dynamic software configuration send output analysis operator _t\ graphical user interface using recast3d package-based communication protocol reconstruction therefore restricted slice i.e. _t\ could also image analysis operator hand must always return slice default recast3d initializes three orthogonal slice fig ease discussion however limit analysis single slice recast3d pipeline illustrated top four process fig discus technical detail section software set-up figure proof-of-concept set-up just-in-time learning recast3d using u-net image denoising noise2inverse loss label 1–4 recast3d pipeline reconstruct slice _t\ stream projection _t\ sends visualization client fig label jitlearn software intercept slice via plug-in concurrent process u-net architecture a_\theta\ trained reconstruction using capacity queue fig figure described detail main text section software set-up full size image experiment take subset projection data two dynamic imaging experiment corresponding five-minute interval laboratory set-up consists polychromatic conebeam source dexela1512ndt cmos detector measurement data acquired rotating sample speed 1.8 second per rotation exposure time millisecond per frame yield projection full rotation field view cropped sample holder resulted 647-by-768 pixel image flat dark field image collected start experiment reconstruction used voxel size 0.25 geometry parameter provided data reconstruction computed full rotation projection fdk algorithm type filtered-backprojection conebeam geometry spatio-temporal reconstruction interval neighbouring reconstruction fixed 0.720 second projection deep learning real-time analysis deep learning recent machine learning technology obtains impressive result various field imaging science e.g. autonomous driving robotics computer vision using deep neural network architecture convolutional layer non-linear activation function dnns enjoy remarkably fast execution due execution gpus therefore especially well-suited tomographic pipeline used perform real-time image enhancement task supersampling artefact removal segmentation denoising fast inference crucial importance synchrotron laboratory environment yield real-time feedback sufficient framerates well create control mechanism steer ongoing experiment analysis operator a_\theta\ fig denote image-to-image dnn general goal perform image task approximating ideal mapping denote image manifold image denoising example aim would approximate perfect denoiser a^\dagger\ would map noisy image noise-reduced counterpart approximation dnns accomplished parametrization mapping i.e. a_\theta\ free parameter weight convolutional filter deep learning concentrate three main aspect representation optimization generalization following describe aspect context real-time tomography introduce notation designing dnn architecture a_\theta\ well-suited task hand area representation design usually concern number layer channel choice operator connection network context tomographic pipeline fig architecture possibility use multiple reconstruction input take advantage spatio-temporal information contained data optimization training optimized using data set input-target pair use denote input—a reconstruction—and denote target supervised learning target ground truth i.e. a^\dagger case supervised denoising example input would noisy image target noise-reduced counterpart input target need correspond data seen inference _i\ _i\ different _t\ _t\ case _i\ _i\ random small regions-of-interest reconstruction image-to-image network designed work input different dimension see speed training process significantly given task-specific loss function measure misfit two image training formulated optimization empirical risk parameter data set architecture a_\theta\ loss aligned a_\theta a_\theta _i\right aligned performance trained network unseen data domain generalization quantify generalization image pair considered random sample latent training distribution probability distribution expected risk describes true unknown performance aligned a_\theta a_\theta aligned whereas empirical risk limited finite sample generalization gap describes distance two risk network said generalize well gap small quantify gap commonly approximated using hold-out data set also sampled also often referred test data set one computes approximation generalization gap setting contain reconstruction need generated different projection set independent sample self-supervised image denoising noise2inverse important analysis task real-time tomography image denoising reconstruction image often severely degraded due noise reconstruction algorithm propagates projection data unavoidable real-time experiment low angular sampling short exposure time necessary observe fast object dynamic unfortunately fast direct reconstruction algorithm including filtered-backprojection capable suppressing noise past year several dnns e.g. dncnn u-net mixed-scale dense network demonstrated excellent denoising quality moreover denoisers already trained small data set small-scale image feature spatio-temporal neighbourhood target often provide sufficient information task example neural network approach named noise2self demonstrates even single image sufficient obtain noiseless output denoising therefore illustrative task just-in-time approach denoising tomographic data different training strategy used practice supervised strategy train noiseless target however strategy feasible tomographic pipeline noiseless ground truth obtained reasonable time case self-supervised learning methodology called noise2noise implemented allows dnns approximate noiseless ground truth using image self-similarity noise2noise requires training data set consisting pair _i\ _i\ image object contain different i.i.d independent identically distributed realization latent noise distribution use noisy target rather ground truth yield optimization problem a_\theta need match _i\ polluted different noise realization input however since unbiased noise doe correlate input target a_\theta\ best reproduce noiseless image feature consistent _i\ _i\ image feature encoded convolutional filter learned due similarity pattern data set noise2noise brought tomographic domain noise2inverse since _i\ _i\ need generated set projection t-m_\phi noise2inverse proposes split projection set odd even time index _i\ _i\ subsequently computed different set result closely similar object statistically independent noise however fewer projection used reconstruction method sacrifice angular resolution software set-up proposed just-in-time learning strategy reconstruction software deep learning software need work closely together fig illustrate proof-of-concept set-up arrow describe recast3d pipeline built top tomopackets library i.e. set software interface passing projection geometry reconstruction user interaction via push/pull publish/subscribe protocol zeromq real-time reconstruction requires two process one compute _t\ fixed projection interval recast3d slicerecon server component another continuously generate training data deep learning requires two similar process first python script component accepts _t\ load memory subsequently computes a_\theta sends external machine running recast3d component accepts batch performs training step update stored parameter optimal set-up four process run concurrently single multiple gpus proof-of-concept software jitlearn release together article see additional information separate training i.e. run component evaluate trained network later time simulate real-time experiment preload projection data ram random access memory synchronize gpu process using projected experiment start time projection data subsequently released gpu process basis virtual framerate set millisecond per frame prolong experimental data minute dark flat field removal well fdk filtering step computed immediately processed gpu soon required reconstruction training store network parameter regular interval allows restoring evaluating network different _t\ afterwards neural network architecture employ experiment noise2inverse u-net widely adopted architecture image processing consisting symmetric downscaling encoder upscaling decoder part complemented skip connection pytorch implementation replicates original u-net proposal downscaling performed strided convolution upscaling transposed convolution u-net implemented three upscaling level level three encoding three decoding convolutional layer concatenating skip connection bridging two part every level double number feature map halving resolution starting feature map first level higher number level increase size network receptive field i.e. region input sequence layer contributes determination feature implementation accommodates reconstruction time time implemented 3-dimensional convolution rather concatenation channel dimension thus treat third dimension similar first two refer architecture without modification standard architecture training employ adam optimizer learning rate 0.0001 batch size train full slice reconstruct small regions-of-interest volume refer patch patch sampled uniformly random slice well spatial neighbourhood fluid container patch sampled outside glass container prevent sampling reconstruction artefact patch-sampling region adjustable experiment currently provide graphical user interface so—by default optimizer sample entire reconstruction volume brevity introduce notation sampling process analysis result training data generation generating sufficient amount reconstruction data pose key challenge training parallel ongoing experiment component fig naive approach would call reconstruction software _i\ _i\ repeat projection preprocessing step well cpu-gpu memory transfer unnecessarily obtaining random sample single high-resolution volume possible either high-resolution reconstruction take second minute complete would result either throttling training process introducing delay network access new reconstruction data problem motivated development new software package released extension gpu-accelerated reconstruction framework called astra toolbox software take nvidia cuda kernel astra package cupy python interface nvidia cuda library software enabled implementation fdk algorithm optimized repeated reconstruction streaming buffer projection data entering buffer projection pre-filtered ram-lak filter stored texture cuda memory structure efficient interpolation modification cuda kernel furthermore enable efficient backprojection arbitrary projection subset benefit efficiency noise2inverse algorithm figure show run-time developed software differently sized reconstruction slice denote reconstruction slab thin reconstruction volume sequence three consecutive reconstruction small training patch take millisecond regardless size computational cost reconstruction algorithm small reconstruction dominated geometry computation timing dnns different input size display expected quadratic trend n_x\ i.e. linear relation total number voxels training small patch reconstruction bottleneck batch 60-by-60 reconstruction example take millisecond iteration optimizer input take millisecond inference hand dnn typically bottleneck inference 1000-by-1000 slab take millisecond reconstruction order magnitude faster figure timing millisecond n_x\ -by- n_x\ slice n_x\ -by- n_x\ -by-3 slab three consecutive slice dnn timing use standard architecture training time includes backpropagation whereas inference time measure forward pas timing calculated using mean repetition warm-up iteration nvidia geforce rtx rev full size image just-in-time learning sketch context just-in-time learning paradigm formulating three interlinked research topic first topic formalizes continuously varying data encountered experiment second topic discus dnn architecture relation reconstruction operator third topic explain buffer-based online training approach developed pipeline data take assumption previous training data ground truth available prior experiment topic real-time imaging data real-time scientific imaging experiment reconstruction often originating continuous physical process data set hence consists consecutive reconstruction real-time setting regard different typical neural network application data set often assumed consist i.i.d image sample distribution instead real-time sample often highly temporally correlated describe type directed random walk manifold see curse blessing sample may appropriately cover sought data distribution short sequence correlated data contain local information help neural network analysis task discussed topic formal modelling real-time data tomographic pipeline would describe single realization continuous stochastic process markov property every input-target pair thus interpreted sample time-dependent _t\ correlated previous sample evolution _t\ reflects evolution physical system imaged system possibly dynamic equilibrium _t\ remains constant time steady-state regime subsequent sample still temporally correlated long collection sample eventually approximate distribution transient regime _t\ change smoothly sample slightly different distribution transient regime occur beginning experiment experiment evolving dynamic instance object exploration zooming pose bigger challenge learning obtaining sufficient sample cover particular _t\ might require multiple realization process i.e. repetition experiment condition however _t\ may also change abruptly time e.g. user change scan geometry alters reconstruction parameter experiment recast3d re-orientation slice effectively change geometry reconstruction image may show differently-oriented feature object call discontinuous jump continuous regime draw _t\ time next timestep t+1\ could possibly display unseen part object therefore radically change latent distribution data sample network trained data set sample drawn _t\ time thus principally unprepared generalize sample jump say data set drawn time t+1\ training distribution result therefore quantify discontinuous jump result iii investigate ability network recover discontinuity topic network architecture order function just-in-time setting dnn architecture need comparatively small need consist limited number parameterized operation layer channel first reason order keep incoming data tomographic pipeline network optimizer must able update network parameter quickly enough possible number operation small second reason inference network presented image high resolution large detector example reconstructed slice potential resolution data large 2000-by-2000 pixel large network typically demand much gpu memory easily attain high framerates resolution hand small network limited expressivity make careful architectural design e.g. number hidden layer residual connection important image task low spatio-temporal complexity segmentation denoising small network yield satisfactory result next consideration architecture additional reconstruction use input process given image slice neighbouring slice additional timesteps—usually given network additional image channels—often provide important contextual information could help improve image task fig show example region interest experiment spatially coherent feature along row experiment spatio-temporally coherent feature along diagonal denoising task redundant pattern contain additional information network experiment may benefit sequence nearby slice whereas network experiment improves spatio-temporal volume generally also point intricate connection reconstruction analysis operator multiple spatial temporal reconstruction may improve network performance also decrease reconstruction speed delay training end-to-end framerate pipeline network therefore ideally designed jointly parameter reconstruction operator spatial extent choice filter image resolution illustrate importance architectural choice result figure different dynamic experiment illustrated spatio-temporal reconstruction vertically-oriented subvolume dimension dissolvent soap-based leading fast-traveling bubble overall large size spatio-temporal pattern appears along time-slice diagonal due slower smaller bubble gel-based dissolvent full size image topic iii online learning online learning incremental learning strategy sample presented sequential chronological order training strategy typically employed machine learning application data processed large volume allows data sample discarded training online learning network weight continuously updated fit recently presented sample result network generalizes well distribution recent sample may diminishing accuracy older samples—a process known catastrophic forgetting behaviour undesired learning static distribution stream data strategy suit real-time tomography since distribution time-dependent goal generalize recently reconstructed image pipeline field online learning today still developing example exist image classification denoising autoencoders just-in-time learning application real-time online learning dynamic imaging data topic explained temporal evolution neural network weight without real-time aspect goal time find optimal weight ^\star _t\ reconstruction _t\ obtained drawing _t\ note since pair generated separately _t\ fig _t\ would ideally designed hoc reconstructing input-target pair expected help generalization towards _t\ example _t\ steady-state regime _t\ could consist large amount reconstruction sampled last projection t-m _t\ corresponding regime time index corresponding data set optimum dynamic real-time experiment lead trajectory ^\star ^\star parameter space principle would like learning process follow trajectory closely possible practice challenge just-in-time learning approximate ^\star _t\ time closely possible also within time constraint experiment similar online strategy achieve continuing training previous suboptimal weight t-1 appending updating data set t-1 new sample form thanks spatio-temporal continuity _t\ difference data set pair small optimization t-1 towards ^\star therefore interpreted minimal example transfer learning t-1 transfer learning significantly faster neural network reuse feature well low-level statistic t-1 basin ^\star basin region parameter space local descend optimization method e.g. commonly-used stochastic gradient descend would converge towards ^\star approach different e.g. continual learning instead goal preserve structure parameter space example constraint weight gradient replaying training data result section train dnns reconstruction generated chronologically however illustrated fig sample generation often slower optimization even tailored reconstruction software form bottleneck would allow single-time access per sample i.e. let way online learning strategy designed optimizer would able continue training new reconstruction generated therefore introduce modification enables resampling reconstruction short duration let training data set time-dependent i.e. _t\ practically implement buffering capacity queue shown fig buffer _t\ time contains last buffer _t|\ draw enabling asynchronous drawing buffer optimizer may select randomly _t\ new reconstruction continuously added figure online learning implemented buffer -sized capacity queue _t\ training-target pair pushed onto queue network optimizer draw pair asynchronously uniform random sampling index full size image size buffer n_\text buffer user-specified parameter optimizer able handle opt sample per second reconstruction generated reco sample per second sample average picked n_\text opt n_\text reco time lifetime _t\ choice buffer thus doe affect often sample picked suitable choice however may straightforward small buffer may contain sufficiently diverse feature contrast buffer large may unnecessarily retain reconstruction become irrelevant current timestep result iii inspect online learning process different buffer strategy result investigate three topic preceding section noise2inverse method learning task data collected using experiment quantify result sample small time window mean-squared error loss doe provide robust performance metric due fluctuating noise level occur data collection set-up similar loss expected risk equation therefore define accuracy empirical accuracy r^\alpha aligned a_\theta a_\theta _2^2 _2^2 ^\alpha a_\theta a_\theta _i\right aligned comparison mean-squared error mse denominator a_\theta equation additionally normalizes noise magnitude result expect empirical accuracy ^\alpha 0.5 see first consider network doe denoise i.e. a_\theta note lead a_\theta ^\alpha consider ideal noise2noise setting discussed method noiseless image denote two independent realization voxelwise-i.i.d noise image zero mean finite variance ^2\ perfect denoising network would predict a_\theta therefore aligned a_\theta _2^2 _2^2 _i^n '^2_i _i^n '_i aligned n_x n_y n_z\ number voxels large i.e. typical image size nominator close ^2\ whereas denominator close 2\sigma ^2\ thus a_\theta 0.5\ using noise2inverse setting however occasionally see ^\alpha 0.5\ possible sparse-angle artefact fast object motion inflicts additional difference even-angle _t\ odd-angle _t\ reconstruction word _t\ _t\ different underlying ground truth dnn network learns predict difference reduces numerator a_\theta _2^2\ result real-time imaging data discussed topic real-time network continuously encounter differently distributed sample due transient discontinuous change data distribution call out-of-distribution sample compared hold-out sample type sample seen network out-of-distribution sample distributed like training set quantify error induces train standard network a_\theta\ data set evaluating sample unseen data set time also train baseline network a_\psi\ accuracy change approximated aligned r^\alpha r^\alpha aligned i.e. difference a_\theta\ trained evaluated baseline a_\psi\ trained evaluated figure display six scenario scenario correspond discontinuous change due user interaction show transient regime experiment generated second projection data experiment error difference dashed line red dotted marker repositioning top half volume bottom half tilting change horizontal vertical slice zooming reduce voxel size 0.125 second row figure choice data set generated different projection projection taken second directly data minute later experiment second data taken experiment fig scenario show evaluation out-of-distribution sample cause significant loss accuracy training a_\theta\ prolonged time doe improve evaluation user interaction error highest case zooming expected convolutional operator scale-invariant therefore take notion size learned image feature note similar change data distribution could also occur acquisition geometry change panel training generalizes well next second however object dynamic experiment changed substantially smaller bubble occur data led decreased accuracy find generalization towards experiment poor likely due change liquid set-up inspect visual effect out-of-distribution evaluation result iii figure empirical accuracy network parameter trained network parameter trained change due interaction experiment data set different projection solid dashed line denote accuracy a_\theta\ a_\psi\ hold-out sample error a_\theta\ out-of-distribution sample i.e. distance potential accuracy i.e. dashed baseline ^\alpha a_\psi achieved accuracy i.e. red marker r^\alpha a_\theta full size image result section noise2inverse experiment indicate small dnns sensitive change pipeline data i.e. learned image feature second data generic enough useful differently placed oriented zoomed slice may suggest extending training data reconstruction similar data augmentation straightforward set-up change discontinuous change physic easily anticipated augmentation moreover small network may expressive enough anticipate large amount data generation additional data require computational resource result network architecture following explore performance different u-net architecture look effect reconstruction dimension experiment table list four variation standard architecture network trained randomly-selected 20-by-20 input patch -networks use feature map first layer follow u-net principle feature map representational capacity network increase i.e. network store larger number filter next class -networks use larger -by- -sized patch training increase receptive field training -networks use nearby spatial reconstructed slice -networks use timeframes middle frame target increase image input training inference use convolution minute training sample continuously generated fixed 12-seconds data set projection data previous reconstruction allowed resampled optimizer avoid throttling representative online training experiment data temporal component yet evaluation hold-out sample taken second interval later experiment figure display evolution empirical accuracy architecture presented table result show denoising network trained satisfactory accuracy minute training approximately fast network although network become significantly slower evaluated high-resolution slice result lower framerate recast3d graphical interface case v30 v150 architecture used larger patch larger patch affect dnn framerate note slower initial convergence valuable experiment eventually resulted better accuracy figure illustrates best architecture category evaluation hold-out sample overall architecture produce comparable output suggesting convergence speed may important criterion selecting network experiment expressive network c32 slice help produce slightly smoother background experiment predicts bubble better produce sharpest bubble interface although latter predicted streak artifact likely due difference occurs even-angle odd-angle reconstruction noise2inverse yet since dynamic experiment slower temporal network may also better using redundancy feature multiple frame table network architecture topic corresponding various parameter choice used compare accuracy training speed fig full size table figure empirical accuracy architecture table experiment using hold-out sample per evaluation point increasing feature slice effective although slow dnn framerate effect patch size frame dataset-dependent full size image figure 90-by-120 region-of-interest network output comparing best-performing dnns class table visually standard c32 v150 output experiment best network fig evaluated input experiment show rising bubble vertically-oriented slice experiment horizontal cross-section middle volume full size image result iii online learning conclude real-time simulation experiment replaying full projection data set just-in-time software using network table 336\ second reconstruction taken random orientation top fluid container t\ge 336\ second sample taken bottom simulates discontinuity pipeline data corresponding repositioning horizontal slice recast3d panel fig scenario reconstruction generated speed batch per second network trained iteration per second figure show empirical accuracy network training estimated sample small window around i.e. accuracy r^\alpha solid black line mark accuracy offline-trained network data discontinuity dashed line total variation based denoising implementation python scikit-image package dotted line describes training strategy without buffer blue red line mark just-in-time strategy buffer reconstruction respectively first observation buffer size perform better network process input sequentially network n_\text buffer =320\ second data performs best suggests including previous reconstruction training process advantageous case slow data generation figure visualizes image pretrained n_\text buffer 320\ network discontinuity note t=4\ minute first row bubble just-in-time network slightly oversmoothed compared pretrained network result satisfactory quality regularization parameter set =0.03\ number iteration n_\text iter =300\ tuning sample top container experiment denoising able outperform data-driven denoisers future work classical combined neural network tomographic pipeline second sample taken bottom volume see tablet fig pretrained just-in-time strategy observe similar accuracy interestingly just-in-time strategy requires short time improve pretrained network second row fig show strategy t=8\ minute clear visual advantage maintains fine-grained structure tablet yellow better able remove noisy background gas around tablet dark blue video combining fig n_\text buffer =320\ network included supplementary information figure accuracy online-trained network experiment different buffer size comparison pretrained baseline total variation based denoising training strategy without buffer second reconstruction sampled bottom fluid container simulate discontinuity pipeline data point graph evaluation network randomly selected sample full size image figure comparison input image denoise denoising result minimization pretrained just-in-time trained network t=4\ t=8\ minute i.e. discontinuity experiment fig difference taken pretrained n_\text buffer =320\ output full size image discussion recent advance deep learning produced powerful image processing analysis algorithm operate fraction time conventional algorithm take just-in-time concept brings algorithm real-time tomographic pipeline synchrotron light source facility x-ray microscopy laboratory large diversity imaging data processed leading principle just-in-time learning continuous regime experiment lead spatio-temporal continuous data pipeline used train experiment ongoing combining small easy-to-train architecture online learning approach allows analysis reconstruction experiment therefore employed without preparation without prior data article demonstrated just-in-time learning using quasi-3d reconstruction paradigm proposed recast3d small u-net architecture noise2inverse loss extending high-resolution fully-3d reconstruction fast dnn inference pose main challenge image partitioning technique divide workload multiple gpus technique yet demonstrated millisecond timescales result small network found sensitive change data distribution even within single experiment generally unavoidable since result illustrates expressive network significantly slower time result show reconstruction large degree spatio-temporal continuity experiment steady-state transient regime enables spatio-temporal dnns allows neural network generalize data tomographic pipeline result several hyperparameter setting number feature map led good visual result however discus hyperparameters picked new set-up task experiment interesting direction future work would find heuristic automate result iii showed straightforward buffer strategy online learning yet also possible design buffer training data set adaptively constraint gradient specialized network architecture possibility retain information longer time span experiment precise description exploration formal concept behind just-in-time learning sketched topic iii important direction future research demonstrated feasibility approach using real-world laboratory data image denoising task using noise2inverse training loss expect representative task low spatio-temporal complexity image super-resolution segmentation artefact removal future research needed see achievable sophisticated image processing tasks—especially diverse image feature need stored network strategy dynamic pruning improved u-net architecture mixed precision weight parallelization improve speed accuracy dnns would widen applicability approach enabling expressive network denoising just-in-time learning already prof viable deep learning paradigm large potential use deep learning-based analysis provide valuable feedback experiment used experimental control real-time scan adaptation optimization technique latter investigate coupling proposed just-in-time learning strategy reinforcement learning approach