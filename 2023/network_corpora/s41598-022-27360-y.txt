introduction new generation sensor complexity remote sensing scene increased significantly due high resolution vhr pose big challenge scene classification recent year researcher developed many solution scene classification deep convolutional neural network cnn demonstrated strong capability field however cnn model require huge labeled datasets effective learning unfortunately availability labeled data still big challenge far datasets small size thousand ten thousand image computer vision area example datasets like imagenet million image labeled herein standard solution use cnn model pre-trained imagenet fine-tuned target dataset known transfer learning approach approach knowledge acquired pre-trained model transferred imagenet domain via fine-tuning model target dataset however transfer learning approach still need large amount target dataset labeled order get accuracy value close otherwise would still fail limited number labeled image known shot scenario due large domain difference source domain imagenet target domain especially multispectral hyperspectral data quite different rgb data hand relatively easy acquire large amount unlabeled image field thus method learn potentially useful knowledge large amount unlabeled image highly beneficial recently novel idea known self-supervised learning ssl appeared computer vision field help achieve effectively applied field computer vision natural language processing object detection ssl method learn discriminative feature representation large amount unlabeled image solving pre-designed task called pretext task transfer target task known downstream task pretext task designed network trained solve learn visual feature easily adapted downstream task ssl paradigm illustrated fig first stage pre-train cnn model pretext classification task available unlabeled image target dataset end first stage cnn model able extract discriminate feature target image second stage knowledge learned pretext task transferred main downstream classification task using transfer learning figure illustration ssl framework consists two phase training phase model pre-trained pretext task pseudo label assigned image phase model fine-tuned downstream task using labeled data full size image order learn representation ssl method define pretext task auxiliary handcrafted classification task example pseudo label pretext task rotation angle image rotated degree respectively pretext task image rotated angle considered belong class thus unlabeled image pseudo label according one four rotation class cnn model pre-trained pretext task classification one four rotation class anticipate learn extract descriptive feature target dataset move second stage cnn model transfer knowledge downstream classification task provide good performance successful pretext task based cross-view prediction framework approach model learns representation predicting different view image one another view generated image using geometric transformation rotation scale flipping cropping however approach shown suffer model collapse problem happens model learns exact representation image case prediction problem becomes trivial recently successful ssl method known cross-view contrastive pretext task proposed achieved good performance self-supervised learning method like simple framework contrastive learning visual representation simclr momentum contrast moco change learning problem prediction one view another view discrimination view generated image well view generated different image augmented view image constitute positive class augmented view image constitute negative class pretext task model trained optimize loss function maximizes similarity view positive class minimizing similarity feature positive negative class cross-view contrastive method often require comparing example many example work well thus quite slow practice due negative sample need considered training cause training image sampled negative pair epoch one recent contrastive ssl method known bootstrap latent byol avoids use negative sample instead byol minimizes distance different transformation view augmentation sample image byol composed online cnn network target cnn network copy main idea byol consists two different image view passed online target cnns cnn extract feature vector corresponding view predictor module used predict target feature vector online feature vector whole architecture optimized minimizing loss target feature vector prediction made online model order avoid model collapse problem contrastive method byol use large batch size training requires high computational resource however hefty computation resource may available time paper propose ssl solution classification remote sensing image shot scenario labeled sample per class method named rs-fewshotssl pre-trained cross-view contrastive learning pretext task try match different augmentation view image downstream task classification image limited number labeled sample rs-fewshotssl architecture similar byol online target network used pretext task online network composed feature encoder projector predictor target network encoder projector copy online network however proposed method efficientnet-b3 cnn feature encoder online target branch also use larger fully connected layer size projection prediction module efficient activation function called swish function address problem limited computation resource available down-sample image pretext task use batch size however downstream task using down-sampled image result highly-degraded accuracy subsampling image reduces higher frequency detail higher frequency detail like local feature low frequency detail like global feature higher resolution image higher frequency detail enables convolutional layer encoder extract localized feature detail encoder work better learning extract local global feature thus propose different architecture downstream task model two branch copy efficientnet-b3 cnn pre-trained pretext task one branch down-sampled image size branch image original size architecture allows reduce need high computational resource benefiting large batch size full image size contribution via paper summarized following point develop method classification scene shot scenario named rs-fewshotssl learn abundant available unlabeled scene using self-supervised learning technique architecture rs-fewshotssl method recent efficientnet cnn backbone model benefitting strong feature extraction capability downstream task built two-branch fusion architecture one branch using large batch sampled image using small batch full-size image architecture allowed reduce memory requirement model enhancing classification accuracy remainder paper organized follows section related work provides overview related work section methodology present approach self-supervised classification section experimental result present experimental result show capability method common scene datasets finally give conclusion future research direction section conclusion related work ssl method scene classification recent ssl method proposed scene classification problem one earliest work yao proposed local manifold-constrained self-paced deep learning ssl method first model trained labeled sample scene second trained model used predict pseudo label unlabeled sample novel idea iteratively select simple trustworthy sample append training set retrain model method constraint ensure consistency pseudo-labels selected sample local manifold labeled sample increase trustworthiness confidence selected sample pseudo-labels tested method nwpu-resisc45 data set another early method yang combine spatial pyramid matching using sparse coding scspm self-supervised learning scspm three-stage algorithm composed dictionary learning sparse representation classification method yang proposed perform dictionary learning stage using ssl unlabeled sample also proposed use much larger auxiliary dataset namely caltech-101 data set perform ssl dictionary learning stage use learned dictionary classify datasets sense also performing knowledge transfer one dataset another method doe deal problem labeled shot work zhao proposed multitask learning framework enhance ability extract feature generalize cnn model framework combine task ssl scene classification dynamic weighting adopting mix-up loss strategy also enables cnn learn discriminative feature without increasing number parameter however work doe deal labeled shot problem try improve typical classification cnn model large amount labeled sample guo proposed method scene classification called self-supervised gated self-attention generative adversarial network gans introduce gated self-attention module introduced gans concentrate key scene area remove unavailing information also introduced pyramidal convolution block residual block discriminator take various level detail image two point meant reinforce feature representation model relation ssl work included unlabeled scene training adding similarity loss component discriminator loss function address shot problem work stojnic study contrastive multiview coding cmc method self-supervised pre-training application ssl analyze influence number domain image used self-supervised pre-training performance downstream task confirm downstream task image classification using self-supervised pre-training image give better result using supervised pre-training image natural scene recently tao proposed method similar called instance discrimination self-supervised learning idssl fact compare three type pretext task image inpainting predict relative position instance discrimination mostly known cross-view contrastive learning scene classification find instance discrimination effective present method using ssl approach shot scenario labeled sample back bone cnn used work resnet50 result study encouraged adopt ssl remote-sensing scene classification relation semi-supervised learning shot learning fsl paradigm ssl learn potentially useful knowledge unlabeled data thus eliminating need costly labeling effort sense ssl similar semi-supervised learning also learns unlabeled data however way method different semi-supervised learning optimize structure unlabeled data specific loss function example use entropy loss function order make unlabeled data uniformly distributed across class general image classification datasets balanced meaning almost number images/scenes class thus ensuring uniform distribution across class preserve structure data help reducing misclassification error contrast ssl defines pretext task model learns image representation unlabeled data assigning handcrafted pseudo label image predicting image view one another discriminating view image versus view image work also assume labeled image per class target dataset similar shot learning fsl fsl important research direction machine learning general however fsl paradigm slightly different ssl paradigm fsl aim recognize class unseen training given labeled sample whereas ssl method assumption model see class fact quite opposite ssl method model pre-trained unlabeled image representing class dataset word model exposed class pre-training albeit without knowing image belongs class methodology describe proposed rs-fewshotssl solution section first introduce efficientnet-b3 cnn model backbone cnn model used feature encoder discus detail proposed rs-fewshotssl method efficientnet convolutional neural network cnn efficientnet one effective cnn model literature proposed tan shown impressive result many image classification task tan used automatic search algorithm optimize base architecture efficientnet called efficientnet-b0 uniformly scaled dimension depth width resolution using compound coefficient set fixed scaling coefficient generate seven larger model using technique able develop family eight cnn model called efficientnet-b0 efficientnet-b7 impressive performance imagenet computer vision dataset compared previous state-of-the-art cnn model family efficientnet model ordered term number parameter efficientnet-b0 smallest model million parameter efficientnet-b7 largest million parameter study implemented pre-trained efficentnet-b3 model consists million parameter proposed self-supervised rs-fewshotssl method previous ssl method based contrastive learning need large batch size work turn computing similarity loss function feature online target network using large batch image indirectly forcing model contrast positive negative sample within batch observed behavior experimented use small batch size pretext training case performance model downstream task severely downgraded fact original paper simclr byol batch size used distributed four graphical processing unit gpu important speed training huge imagenet dataset however due limited computational resource memory constraint batch size given image size maximum batch size possible deal problem using small scale image view size pretext training allows large batch size however classification performance model quite limited case proposed solution rs-fewshotssl motivated dilemma shown fig figure architecture rs-fewshotssl method scene classification architecture used pretext task detail projection prediction layer architecture used downstream task full size image pretext task figure show architecture used pretext task solution composed online target branch employ efficientnet-b3 cnn model feature encoder online branch consists three module encoder projector predictor illustrated fig target branch encoder projector target branch copy online branch minus predictor module except network weight projection prediction module make two branch asymmetric help architecture avoid representation collapse problem online network predicts output target network using prediction module training model aim maximize similarity prediction online network output target network training weight online network updated normally backpropagation algorithm contrast target network updated via exponential moving average online network weight target network provides regression target train online network parameter exponential moving average online parameter precisely given target decay rate in\ training step perform following update +\left 1-\tau given set image image sampled uniformly two distribution image augmentation method start generating two augmented view augmenting image representation branch produce output normalized projection predictor generates normalized prediction online network loss function based cosine similarity defined follows 2-2 cosine =2-2 cosine similarity range indicating high similarity indicating dissimilarity thus way loss function defined make positive inversely proportional similarity follows online branch updated regression loss function regression target given target network detail projection prediction layer shown fig set hidden layer size output layer size addition also used swish activation function instead standard relu swish activation function introduced howard design mobilenet cnn model defined follows swish\left x\right x\right -\beta x\right sigmoid function trainable parameter =1\ known sigmoid-weighted linear unit function finally emphasize architecture pretext task trained small-scale image size pixel allowed use large batch size downstream task downstream task discard target network kept feature encoder online network able produce self-learned representation sample training data learned representation self-consistent invariant different transformation data new classification layer appended encoder whole model fine-tuned downstream task similar usually done fine-tuning pre-trained cnn model indeed pretext step simply produce pre-trained model fine-tuned using typical transfer learning technique figure show proposed architecture downstream task composed two-branch fusion architecture branch consisting copy cnn feature encoder append two output layer called basically fully connected layer softmax activation function provide output classification probability finally fusion layer computes mean probability two output layer cnn feature encoder branch initialized weight pretext stage however fine-tuning downstream task upper model take input image view size lower one take image view original size clever approach provided good compromise resource cost performance view generation via augmentation generate augmentation view used following geometric transformation random crop resize random flip random rotation 90° however unlike previous work used different transformation parameter including color jitter strength 0.4 probability 0.8 minimum cropping size original size random gray scale probability 0.2 finally vertical flip horizontal flip random rotation probability 0.5 use gaussian blur already down-sampled image adding gaussian blur significantly reduce quality image potentially cause severe degradation figure exhibit example batch image random view generated online target cnn encoders figure sample augmentation view generated training original image batch random view online cnn random view target cnn full size image rs-fewshotssl algorithm following algorithm show main step proposed method particular datasets split training unlabeled test set follow method splitting data literature see example order able compare work experimental result section present experimental work study first describe datasets used explain set executed experiment finally present result compare state-of-the-art method dataset description evaluate scene classification model remote sensing used three datasets called ucmerced aid nwpu-resisc45 table provides summary different property datasets table detail datasets scene used work full size table ucmerced dataset published one earliest datasets 2,100 image distributed class size dataset make unsuitable deep learning model need large amount data however amount data augmentation used proposed method compensate shortcoming addition beneficial model analysis fine-tuning low processing time requirement two datasets aid nwpu-resisc45 nwpu short large diverse aid dataset acquired ftom different remote imaging sensor different country around world collection 10,000 annotated aerial image distributed land-use scene class used image-classification purpose nwpu-resisc45 dataset created northwestern polytechnical university nwpu includes 31,500 image separated scene class image class experiment setup training protocol implemented proposed deep ssl learning solution using pytorch deep learning library written python executed various experiment lambda server intel® xeon silver cpu running clock frequency 2.20 ghz housing cpu ram two nvidia titan rtx gpus memory pretext task scene resized able set batch size optimize model fig epoch using lars optimizer cosine decay learning rate schedule without restarts set base learning rate 0.2 scaled linearly also used warm-up period epoch learning rate constant 0.001 addition use global weight decay parameter 1.5 excluding bias batch normalization parameter lars adaptation weight decay main downstream task fine-tune model fig total epoch batch size using adam optimizer learning rate 0.0001 reduced epoch 0.00001 performance evaluation present result using overall accuracy ratio number correctly classified sample total number tested sample standard way evaluate scene classification method claimed suitable defined following formula oa= i=1 test number correct classification class test set number class test total number test sample present full algorithm rs-fewshotssl method result comparison state-of-the-art ssl method section evaluate performance rs-fewshotssl method three datasets dataset tested two shot scenario aid nwpu-resisc45 datasets used five labeled sample per class however ucmerced small dataset sample per class thus using sample equivalent data used labeled data consistent datasets labeled data dataset thus ucmerced use labeled sample per class instead use three five sample per class shown table table classification accuracy value obtained using ucmerced dataset batch size varied full size table table rs-fewshotssl result ucmerced dataset comparison state-of-the-art method full size table first experiment shown table test proposed method ucmerced dataset illustrate importance using large batch size self-supervised learning experiment fix image size allows experiment batch size pretext downstream task mentioned downstream task target network discarded online network fine-tuned labeled sample fixed parameter method described experimental setup section varied batch size starting reported downstream task seen table downstream task degrades decrease batch size confirming importance using large batch size ssl method based cross-view contrastive learning next present table final result rs-fewshotssl method ucmerced aid nwpu-resisc45 datasets respectively show effectiveness rs-fewshotssl compared many state-of-the art ssl method including simclr moco byol idssl obtained result method moco simclr byol result marta gans idssl method reported author note usually aim run experiment least five time report mean standard deviation performance method however due time constraint especially since method take hour complete opted execute experiment method moco simclr byol method explains report standard deviation method however proposed method execute experiment time using different random labeled set every time report mean standard deviation result table rs-fewshotssl result aid dataset comparison state-of-the-art method full size table table rs-fewshotssl result nwpu-resisc45 comparison state-of-the-art method full size table first observe obtained result better one yielded fine-tuning efficientnet-b3 model pre-trained imagenet dataset denoted pre-trained imagenet first row table clearly show rs-fewshotssl able learn large amount unlabeled data enhance classification accuracy second observation using five labeled sample per class result improved outperformed ssl method discussed literature except nwpu-resisc45 scenario idssl method tao achieved 80.62 0.03 method obtained 71.18 0.88 nevertheless upon inspection found tao obtained result using unlabeled sample available totaled 25,200 whereas used 10,000 unlabeled sample selected randomly 25,200 reduce running time experiment due time limitation access computational platform tao also obtained result using 10,000 unlabeled sample achieved overall accuracy value 68.52 lower result 71.18 0.88 observation time result proposed rs-fewshotssl method improved simply fine-tuning efficientnet–b3 model pre-trained imagenet dataset exception found aid dataset labeled sample per class case fine-tuning imagenet pre-trained model achieved 88.40 0.73 proposed rs-fewshotssl method achieved 87.13 0.67 also observed improvement significant larger aid nwpu datasets compared ucmerced due large size unlabeled set aid nwpu recall size unlabeled set used 10,000 ucmerced aid nwpu respectively since deep learning model benefit greatly much larger datasets improvement provided proposed rs-fewshotssl imagenet pre-trained model significant aid nwpu datasets illustrate effectiveness self-supervised learning proposed method used distributed stochastic neighbor embedding t-sne technique visualize distribution feature vector extracted backbone cnn model t-sne non-linear technique used data exploration visualizing high-dimensional data show fig tsne plot distribution feature vector extracted unlabeled set sample pretext training phase generated plot every 50th epoch figure show plot epoch one see unlabeled data start class mixed together training model slowly learns extract feature cluster together based image class course still confusion class remember sample unlabeled model learning cluster nevertheless figure tsne plot unlabeled set aid dataset throughout training process number epoch plot saved every 50th epoch full size image finally fig show tsne plot testing set nwpu dataset case sample per class plot fig show distribution testing set feature produced fine-tuning pre-trained imagenet model plot fig show distribution testing set feature produced proposed rs-fewshotssl model distribution plot see proposed rs-fewshotssl method able learn feature enable classify image better thus proposed method able effectively utilize unlabeled set enhance quality discriminability learned feature turn enhances final classification accuracy figure tsne plot test set nwpu dataset plot supervised training training set fine-tuning pre-trained efficientnet-b3 model plot supervised training training set downstream task fine-tuning rs-fewshotssl model pre-trained unlabeled set pretext task full size image conclusion study developed new ssl paradigm scene classification remote sensing image shot scenario called rs-fewshotssl based enhanced cross-view contrastive ssl approach similar byol combined efficientnet-b3 cnn model study found cross-view contrastive ssl method use negative sample byol actually indirectly contrastive loss positive negative sample use batch normalization layer large batch unlabeled data pretext training phase thus order method work need use large batch size byol used proposed method consists novel deep learning architecture trained using high-resolution low-resolution image thereby latter allow larger batch size significantly boost performance proposed pipeline task classification tested rs-fewshotssl three public datasets namely usmerced aid nwpu-resisc45 result showed rs-fewshotssl outperforms recent ssl method supervised classification downstream task shot scenario utilizing labeled sample per class future work focus overcoming issue large batch size solution problem include using distributed computation another completely different ssl method another direction expanding model learn unlabeled data multiple datasets challenging problem due data distribution shift datasets addition fact datasets share image class domain adaptation technique considered utilized