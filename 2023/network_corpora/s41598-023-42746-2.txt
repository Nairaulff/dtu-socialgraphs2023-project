introduction recent breakthrough machine learning including stunning success alphazero alphago demonstrated tremendous potential transforming scientific computing way analogy identify three crucial piece needed ml-enabled scale-bridging advance algorithm cooperative deep neural network nns emulate fine-scale simulation within prescribed error bound incorporating known physical constraint domain expertise active learning method generate large targeted datasets intelligently massive computational resource including ml-optimized processor integrating advance algorithm hardware within scale-bridging framework would transform scientific computing application learning represent subscale physic often weakest link current modeling capability designing effective method multi-scale simulation longstanding challenge without clear separation scale difficulty becomes tremendous using emulate fine-scale model learn effective fine/coarse scale-bridging protocol exciting prospect allows advance state art beyond sequential training inference facilitate scale-bridging novel technique special case semi-supervised learning algorithm interactively use fine-scale model obtain desired output new data point making ideal concurrent scale-bridging optimally procedure dynamically ass uncertainty model query new fine-scale simulation necessary use new data incrementally improve model order achieve goal three grand challenge need addressed best emulate coarse-scale dynamic anticipate speculatively execute relevant fine-scale simulation nns heuristic used estimate trajectory coarse-scale state best dynamically update nns new training data generated careful sub-sampling existing data addition newly generated data used retrain nns update uncertainty tensor core well suited step lower precision computation acceptable trade-off massive gain performance since prediction inherently noisy estimate uncertainty one possible solution cooperative learning approach using pair nns one prediction second uncertainty computation however one traditional drawback nns unstable outside calibration range thus alternatively may posited quantified uncertainty required since ultimate purpose decide new fine scale simulation result valuable automatically determining whether used outside calibration range new data generated extends range necessary paper select two exemplar application exercise different part al-enabled scale-bridging approach inertial confinement fusion icf interface dynamic demonstrate address aspect icf application since nanotransport shale dependent coarse-scale problem plagued uncertainty since subsurface opaque demonstrate incorporating uncertainty calculation addressing challenge overall research goal transform nns scientific computing unlocking enormous potential learn complex hierarchical physic big data training time scale linearly dataset size result present integrated multi-disciplinary framework fig coarse-scale model lattice boltzmann method lbm kinetic multi-ion vlasov–bhatnagar–gross–krook multi-bgk model two application respectively divided subdomains partitioned across node coarse-scale model insufficient advance simulation active learner either generates approximation existing result performs new fine-scale molecular dynamic computation depending estimate uncertainty quantifier new result added first local database eventually consistent distributed database trade-off duplicated computing communication computational realization algorithm requires several aspect next-generation architecture/algorithm design including reliance asynchronous task-based computing paradigm enabling programming execution model technology use tensor core training inference aspect required fully exploit massive concurrency heterogeneity future exascale computing platform algorithm computational framework generally applicable developed demonstrated two specific application molecular-scale physic impact result continuum-scale model illustrated fig described detail figure scale-bridging framework coarse- fine-scale model database active learning task latter utilize gpus accelerator infrequent expensive training upscaled parameter uncertainty frequent inference full size image icf target consist small quantity deuterium-tritium fuel surrounded heavier shell compress fuel enough enable thermonuclear fusion however practice significant degradation compression due hydrodynamic instability interfacial mixing well heat loss central hot spot overlap flow molecular scale result purely hydrodynamic simulation missing important effect non-local transport nevertheless due resolution constraint state art icf problem mesh coarse enough fit current supercomputer non-physical numerical diffusion relatively hoc model approximate molecular effect recent attempt overcome crude approximation either address flow scale accurate continuum solver higher resolution account high temperature molecular effect using electron quantum description simulation highlighted importance flow instability plasma transport hot spot dynamic however accurate early-time simulation needed identify exact structure flow perturbation example preliminary result marble experiment described show complex flow evolution pre-heat self-generated shock temporarily halting mixing please see murphy detail marble experiment national ignition facility similarly late-time mixing orbital free density functional theory of-dft calculation show common transport model significant error may need locally updated non-local transport may also require different continuum-level description amenable off-line property evaluation accuracy demonstrated efficient computational tool probe microscopic property high-energy density experiment used validate various microscopic model implemented icf hydrodynamics code equilibration phenomenon structure radiative process however currently performed time length scale relevant icf experiment becomes expensive higher temperature instance state-of-the-art massively parallel simulation micron capsule-shell icf interface took week simulate 0.2–0.5 total implosion time underscoring need new multi-scale approach second application majority pore space rock formation e.g. shale relevant subsurface energy water resource exists pore continuum model often break quantitative prediction transport nanoporous medium critical optimize hydrocarbon extraction low permeability shale formation hydraulic fracturing shown methane production vastly underestimated knudsen diffusion one several molecular subnanoscale effect accounted coarse-scale nanoscale navier–stokes model knudsen diffusion result 10–100 increase bulk property permeability intuitively many interaction fluid confining pore wall small pore size 10–100 could result emergent phenomenon affecting methane extraction effect microscopic random fluctuation coarse-grained model using hybrid algorithm studied problem including linear diffusion inviscid burger navier–stokes ginzburg–landau equation direct atomistic-continuum scale-bridging model challenging implement primarily due disparate length timescales resulting inaccurate information transfer across scale poor computational performance although study hybrid atomistic-continuum model including coupling lbm dense fluid focus simple system e.g. single-phase laminar flow past carbon nanotube scale-bridging simulation nanoporous flow relevant subsurface application essentially non-existent limited study off-line use molecular simulation generate model lbm i.e. sequential scale-bridging ignoring overlap scale various component scale-bridging framework fig implemented generic learning user enablement glue code enables bridging various scale glue code handle communication interaction various component workflow—the computing platform coarse-scale code fine-scale code surrogate model also active learning algorithm based input output coarse fine-scale model relevant application glue code embeds application programming interface apis model talk enable passing information hood glue code work high-performance computing job scheduler e.g. slurm work modern exascale computing architecture reader referred pachalieva detail glue code figure machine learning based scale-bridging framework inertial confinement fusion shale nanoconfinement application full size image inertial confinement fusion inertial confinement fusion experiment fundamentally multi-scale nature modeling experiment quantum molecular dynamic similar model collect data dynamic structure process occurring microscopic scale however icf experiment result evolve macroscopic scale space time much larger simulated microscopic model accurate understanding connection expensive microscale physic experimental observables needed large scale property crucially affected microscale information equation state ionic electronic transport coefficient additional closure information provided microscale model communicated macroscale via e.g. table formula etc accurate closure information large number multi-scale technique developed bridge scale perform well relatively simple system sheer number fine-scale calculation needed realistic problem quickly become impractical without additional guidance one challenging feature measure icf experiment atomic mixing material dense plasma particular issue relevant marble-like icf experiment icf target filled engineered deuterated foam foam pore made specified size filled gas containing tritium resulting fusion yield initially separated reactant experiment give measure amount mixing occurred model atomic mixing use multi-ion vlasov–bhatnagar–gross–krook kinetic model key closure collision frequency specie standard model frequency rely weak coupling assumption significantly simplify analysis may valid pre-heat regime icf capsule implosion instead use compute multi-species diffusion rate via green–kubo relation used define collision rate kinetic model demonstrated coupling using kinetic code multi-bgk code lammps study mixing argon–deuterium interface reduce cost gathering information used active learning train neural network data obtained randomly sampling small subset parameter space constructed dataset random set latin hypercube experiment design containing point five-dimensional input parameter space order capture expected condition marble focus material density range 10^ 10^ plasma temperature range model performance test data shown fig top row point shown colored using quality flag s_i\ allows selecting point model confidence low requesting new calculation reader referred diaw detail simulation parameter applied method plasma interfacial mixing problem relevant warm dense matter showing considerable computational gain compared full kinetic-md approach approach enables probing coulomb coupling physic across broad range temperature density inaccessible current theoretical model surrogate model give result within acceptable measure accuracy compared all-md simulation much expense material artificially heated outside range data used initialize surrogate model learner framework successfully update see fig bottom row figure top row show performance surrogate model mutual diffusion point test dataset generated randomly sampling point 5-d input parameter space left right deuterium–deuterium deuterium–argon argon–argon mutual diffusion point selected verification circled red bottom row location call interfacial mixing simulation white blue surrogate model black database duplicate simulation left used uniform temperature profile case right artificially heated push state system away initial training data used surrogate model full size image proposed alternative method evaluate diffusion coefficient required close kinetic model applies modern data science tool predict thermodynamic factor multi-species phase space thermodynamic factor measure ideal non-ideal mixing used specie self-diffusion coefficient generate inter-species diffusion term needed kinetic model inter-species term relatively expensive compute self-diffusion term especially case trace amount one specie mixture improvement allows significant decrease amount time needed call return relevant data kinetic model speed multi-scale simulation fine-scale simulation expensive relative coarse-scale simulation essential choose data point prudently possible maximize improvement surrogate model also improves overall multi-scale performance reduces number request surrogate answer end sampling workflow provided score surrogate newly evaluated data additional optimization loop attempt find surrogate validity converged optimum diaw assessed performance different sampling strategy learning asymptotically valid surrogate benchmark function compared traditional sparse sampling find sample least-populated point draw optimizer-directed approach initial draw used starting point optimizer run termination used metric based error surrogate predicted value versus truth averaged entire response surface methodology produced surrogate average high quality across whole parameter range regardless sampling strategy used found given one doe upfront work tune optimizer termination condition given problem optimizer-directed approach efficient traditional sampling approach importantly also noted metric used guarantee quality surrogate neighborhood turning point found optimizer-directed approach much better minimizing model error neighborhood turning point see fig even metric doe call explicitly conversely traditional sampling strategy blind response surface generally demonstrated much larger misfit near turning point thus using metric judge quality surrogate misfit turning point produce high-quality surrogate optimizer-directed approach even greater efficiency fig compare performance optimizer-directed approach versus traditional random sampling strategy 8-dimensional rosenbrock function figure candidate surrogate 8-dimensional rosenbrock function learned thin-plate rbf estimator using sparsitysampler mystic validity based average model error test score per iteration optimizer-directed sampling blue pure systematic random sampling orange model evaluation sampled random sampling strategy optimizer-directed sampling using ensemble nelder mead solver default configuration surrogate produced either sampling approach appear identical truth log-scaled view surrogate random sampling near global minimum log-scaled view surrogate optimizer-directed sampling near global minimum identical truth pure systematic random sampling converges faster optimizer default configuration used optimizer-driven sampling generally provides accurate surrogate near critical point full size image critical point response surface key point find guarantee long-term validity surrogate new data collected generally interesting physic occurs demonstrate applied methodology two test problem showed could efficiently learn surrogate equation state calculation dense nuclear matter yielding excellent agreement surrogate model across parameter range specifically region includes physically-relevant feature phase transition also showed methodology produce highly-accurate surrogate radial distribution function expensive molecular dynamic simulation neutral charged system several dimension across extensive range thermodynamic condition nanoconfinement shale shale reservoir redefined energy landscape world currently shale reservoir account total gas production entire oil production however characteristic significantly differ conventional reservoir among characteristic nanoconfinement fluid/pore wall interaction dominate fluid behavior shale approximately pore scale surface phenomenon adsorption slip control equilibrium transport property fluid however current continuum simulator account effect given challenge nanoscale experiment augmented limited experimental data molecular simulation reveal confinement effect simulation become necessary research technique conduct computational experiment giving ever-increasing ability predict matter physical chemical property probe experimentally inaccessible area confinement however computational cost molecular simulation prohibitive studying natural system therefore scale-bridging technique become necessary upscale molecular simulation insight continuum pore-scale simulation start simulation derive physic confined fluid adsorption slip length diffusivity adopt deep-learning technique develop scale-bridging workflow calibrate pore-scale simulation account confinement effect crucial innovation relies designing training neural network model enable active learning uncertainty quantification discussed method section figure present one innovation modeling adsorption behavior nanopores overcome computational burden fine- coarse-scale simulation using efficient emulator emulator used afterward train upscaler map fine-scale simulation coarse-scale simulation although upscaler could trained directly fine- coarse-scale simulation trained emulator enabled speculatively execute new fine-scale calculation based coarse-scale trajectory forecast worth noting molecular dynamic consider molecular discrete nature matter unlike continuum-scale simulation consequently adapted indirect coupling scheme match adsorption phenomenon case applied emulator/upscaler approach simple slit pore ranging 2.4 simulation used quantify adsorption characteristic inform pore-scale simulation reader referred lubber detail development emulator upscaler demonstration fig note upscaler workflow shown fig one portion scale-bridging framework fig specifically lbm upscaler called active learning strategy fig part upscaled parameter arrow on-demand fine scale model box coarse scale model box implemented uncertainty-based active learning strategy complex pore structure figure show representative result implementation strategy strategy quantified uncertainty neural network identifying area mismatch among randomly-initialized similarly-trained neural network allows dynamically update augment fine-scale database highly-uncertain data point meaning area higher uncertainty strategy call fine-scale sampling successfully reached accuracy conventional approach using dataset uncertainty-based training shown fig right work demonstrates workflow build fine-scale emulator using active learning approach utilizing uncertainty-guided data collection strategy seven order magnitude computational speed-up seen figure left show sample porous shale domain used study fig center show uncertainty pore representative epoch active learning strategy reader referred santos detail implementation simulation setup figure top show detailed workflow include nanoconfinement effect pore-scale simulation train deep neural network emulator using molecular dynamic lattice boltzmann method lbm simulation pore profile range condition emulator later used train upscaler update lbm parameter include confinement effect instance adsorption case bottom left show shale sample region used uncertainty-guided active learning strategy bottom right performance uncertainty-guided active learning strategy approach reach error target faster conventional approach bottom center show high low uncertainty pore fine-scale emulator prediction active learning strategy uncertainty neural network guide training process full size image discussion critical aspect bridging scale usually form constitutive equation specifies relationship macro-scale observables macro-scale response result underlying micro-scale interaction otherwise specified macro-scale equation often termed closure empirical constitutive relation also often termed surrogate model near-equilibrium scenario well described macroscopic theory simple constitutive relationship satisfactory example use constant diffusion coefficient fluid viscosity however specific model need developed capture physic extreme condition icf nanoporous medium example bgk model suitable icf require closure information specify typical collision rate particle specie simple model capture scaling behavior perturbative context e.g. weakly coupled plasma completely sufficient cover context underlying physic change strong-coupling regime capture behavior microscopic physic complex system researcher often built hand surrogate model capture behavior data describing underlying microscopic process using empirical functional form adjustable parameter process doe naturally treat many variable simultaneously laborious researcher find appropriate functional form additionally one may know ahead time phase space might encountered macroscopic system resulting iterative need expand surrogate model cover data rise machine learning automation natural fit constructing surrogate model complex condition historically machine learning developed pattern-recognition-based endeavor make progress artificial intelligence definition mean machine learning workflow geared toward automation model developed variety task many possible even simple human accomplish difficult directly program solution result ample space possible input exponentially large space possible function input result machine learning method capable searching high-dimensional space function useful empirical model evaluated quickly computer model trained call extract information cheap often executed million time negligible computational cost scaled treat massive datasets result model assume apriori functional form upscaling function fine-scale input coarse-scale lbm kinetic theory parameter scale-bridging result numerous small simulation treated dataset machine learning like many fine-scale simulation scale-bridging many machine learning method parallelized reduce training time developed al-based framework efficiently accurately capture fine-scale physic used bridge fine coarse scale reducing overall computation time order magnitude depending problem required accuracy also provided broadly useful advance scientific quantifying uncertainty model forecasting coarse-scale trajectory speculatively execute new fine-scale calculation dynamically updating emulation model new data obtained demonstrated approach hydrocarbon extraction tightly coupled lbm simulation methane adsorption transport nanopores addition extended approach critical nanoscale physicochemical process training calculation parameter required lbm accounting effect nanoscale confinement phase behavior hand developed consistent framework icf bridge continuum scale simulation remains computationally feasible also dramatically increasing accuracy calculation work demonstrated potential active learning technique develop efficient accurate scale-bridging workflow various application field method present detail approach workflow neural network scale-bridging machine learning approach scale-bridging traditionally addressed using gaussian process regression gpr gpr construct regression model based kernel function kernel function specifies correlation response function data point kernel large two point assumed nearly response sense kernel thought type distance similarity function gpr emulator proven remarkably successful number context optimal design simulating radiative transfer modeling fluid flow however crucial limitation gpr cost building model associated matrix inverse problem scale like n^3 data point thus doe scale effectively dataset grows size another drawback gpr kernelized model finding suitable kernel number feature grows larger crucial ansatz often suffers curse dimensionality hard build accurate gprs high-dimensional space addition previous effort apply kriging-based adaptive sampling technique multi-scale material application found neither global gpr model patching together several local gpr model worked well alleviate limitation gpr approach utilized machine learning particular deep neural network dnn instead main advantage using neural network comparison traditional scale-bridging method summarized training performed multivariable parameter space simultaneously allows handling large datasets simultaneously incorporates many simulation property approach doe depend dimensionality system currently framework solves pore however generalized two- three-dimensional pore system unlike traditional scale-bridging model upscaled function need assume functional form parameter priori assumption function generated neural network framework extended enforce physical constraint flow condition equation state modification phase transition characteristic conservation low mass momentum current dnn model also take advantage fast automatic differentiation algorithm computing gradient available within dnn library using automatic differentiation emulator upscaler network trained faster laborious deriving coding numerical finite-difference gradient indirect scale-bridging fine-scale input need explicitly matched fine-scale input instead additional upscaler trained based emulator defined fine-scale coarse-scale input data since emulator defined basis respective input space collected data upscaler solves problem implicitly network incorporates property many simulation simultaneously operates multivariate parameter space instead optimizing upscale parameter point-by-point basis indirect upscaler workflow detailed lubber based fine-scale molecular dynamic simulation pore structure spanning range temperature density pore width corresponding coarse-scale simulation lattice boltzmann method lbm calculation input effective temperature density pore width adsorption parameter emulator learned datasets lbm emulator one extra input—the adsorption parameter trained construct large dataset emulated profile proceed train upscaler network take input temperature density pore width output lbm temperature adsorption parameter passed lbm emulator density pore width parameter fixed train upscaler loss function based primarily excess density net density fluid pore present purely due adsorption effect supplement regularizing term aim make sure emulated profile lbm close forward active learning essential ensure surrogate model capture behavior simulation entire space potential simulation collecting data mentioned earlier expensive often apriori knowledge needed calculation reduce cost applying active learning includes following step perform calculation parallel ahead time build surrogate model input parameter run chosen randomly sampling parameter space querying surrogate model request model predicts uncertainty surrogate model model report prediction carry large uncertainty new simulation spawned collect ground-truth data surrogate model periodically retrained account available data ensures surrogate model flexible adapt new macro-scale condition fly case using expected increase accuracy surrogate model comparison approximation reduces computational time spent simulation collecting necessary data automatically build ensemble query-by-committee uncertainty training network random subsection dataset random data withheld calibration ensemble produce different model since initialization network weight data split random calculate network performance model using calibration data model delivers good result coefficient determination r^2 0.7\ otherwise rejected procedure executed n_\text ensemble =5\ network successfully trained active learning asymptotic validity recently used active learning generate surrogate fine-scale material response roehm used kriging construct surrogate stress field communicate result fine-scale code solves macro-scale conservation law elastodynamics noack used similar kriging-based approach construct surrogate autonomous x-ray scattering experiment none approach attempt ensure surrogate valid future data thus provide guarantee validity surrogate instead whenever learned surrogate evaluated uncertainty metric also evaluated determine new fine-scale simulation launched example noack genetic algorithm find maximum variance measured data point draw new sample distribution localized around solved maximum passive approach validity assume expensive model always available may performance significantly hampered need frequently request new expensive model evaluation throughout life surrogate present new online learning methodology efficiently learn surrogate asymptotically valid respect future data use asymptotically valid indicate formal proof validity versus future data strong evidence support claim least approximate validity respect future data light condition specifically conjecture minimal data set necessary produce highly accurate surrogate composed model evaluation critical point model response surface conjecture come condition selected surrogate class enough flexibility reproduce model accurately hence beneficial use radial basis function rbf multilayer perceptron mlp similar estimator universal function approximation capability training surrogate model response surface methodology three key component sampling strategy generate new training test data learning strategy generate candidate surrogate training data validation metric evaluate candidate surrogate test data implementation leverage mystic open-source optimization learning uncertainty quantification toolkit mystic decade use optimization complex model including using uncertainty metric optimally improve model accuracy robustness increase statistical robustness surrogate recent development included configurable optimizer-directed sampling use combination online learning train surrogate accuracy respect future data general procedure producing valid surrogate start initial surrogate built training database existing model evaluation surrogate tested validity condition i.e. quality metric expected model error given tolerance surrogate deemed valid testing execution stop otherwise surrogate updated retraining database stored model evaluation surrogate validated fine-tuning surrogate hyperparameters quality metric iterative retraining improves surrogate saved otherwise sample new model evaluation generate new data process repeat testing produce valid surrogate procedure producing valid surrogate extended asymptotic validity adding validity convergence condition called surrogate deemed test valid thus instead stopping execution surrogate test valid merely complete iteration toward validity convergence converged trigger new iteration sampling new data continue iterate surrogate validity converged iterative procedure likely generate surrogate valid future data testing convergence validity unseen data several iteration database model evaluation sparsely populated expect new data likely trigger surrogate update note adding new data database doe ensure surrogate become valid however conjecture sample critical point model response surface ability learn valid surrogate improves reduce number sampling iteration required increase number sampler used iteration focused choice sampling strategy impact efficiency producing asymptotically valid surrogate validity defined evolution surrogate test score compared optimizer-directed sampling versus traditional random sampling efficiency learning response surface fundamentally approach utilize identical first draw optimizer-driven sampling draw first draw member starting point optimizer instead repeating probability sampling used first draw approach follows snowball sampling point first draw member corresponding optimizer directing sampling toward critical point response surface termination condition optimizer met probability sampling used first draw generate new starting point new optimizer continues termination sampling learning component workflow fundamentally independent able run asynchronously linked database stored model evaluation data point generated sampler populated database learning algorithm always data contained database new training requested concern minimizing number model evaluation could sampler run continuously feeding model evaluation database however include sampling part iterative workflow described minimize number model evaluation