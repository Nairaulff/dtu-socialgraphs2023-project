Introduction The Particle Swarm Optimization (PSO) algorithm, introduced by Kennedy in 1995 1 , has garnered significant attention from researchers since its inception. It has found successful applications in various practical engineering problems, such as image segmentation 2 , 3 , sound classification 4 , power planning 5 , 6 , path planning 7 , 8 , 9 , water pressure control 10 , voltage regulation 11 , sensor networks 12 , among others. Additionally, numerous enhancements have been developed to improve the PSO algorithm. These enhancements typically fall into three main categories: modifying the algorithm’s topology 13 , enhancing the particle swarm learning strategy 14 , and combining PSO with other algorithms. Changing the topology to design update strategies tailored to particles with distinct characteristics can optimize the utilization of information within the particle swarm 15 . Liang introduced the APSO-C algorithm 16 , which incorporates two key strategies. The first strategy involves partitioning the particle swarm using the k-means method, resulting in subgroups with varying capabilities. The second strategy aims to balance the local and global search aspects of the algorithm. On the other hand, Xu proposed the QLPSO algorithm 17 , which integrates reinforcement learning into the particle swarm algorithm. In this approach, each particle autonomously selects the best topology by referring to a reinforcement learning table that evolves progressively during the iterative process. Comparatively, this experiment demonstrates a faster convergence rate when compared to particle swarm algorithms based on alternative topologies. Enhancing the learning strategy of the particle swarm algorithm proves to be an effective means of boosting its performance. This improvement can involve adjustments to various learning parameters 18 , such as inertia weights 19 , among others. Tian introduced the MPSO algorithm 20 , which employs a unique approach. It initializes the particle swarm using a logical map and then selects inertia weights using both linear and nonlinear strategies. Furthermore, an auxiliary update mechanism is implemented for global optimal particles, contributing to the algorithm’s robustness. Karim, on the other hand, proposed MPSOEG 21 , an algorithm that optimizes the learning framework by eliminating inertia weights and velocity parameters. Experimental results highlight the algorithm’s efficiency in solving single-objective optimization problems. Wang introduced a novel particle adaptive learning strategy for tackling large-scale optimization problems 22 . One effective approach to diversify particle swarm information is by combining particle swarm algorithms with other algorithms. Zhu, for instance, integrated the fireworks algorithm with PSO 23 . Dadvar, on the other hand, combined Differential Evolution (DE), a stochastic optimization algorithm, with PSO 24 . This fusion of DE and PSO leverages Nash bargaining theory, demonstrating its superiority over other hybrid models in various applications. In a similar vein, Wang introduced DFS-CPSO 25 , a hybrid algorithm that combines the depth-first search algorithm with the particle swarm algorithm. By integrating the DFS strategy, this approach enhances the diversity of particles and exhibits superior performance, particularly in solving high-dimensional multi-modal problems. In 2003, Kennedy introduced the bare-bone particle swarm algorithm (BBPSO) 26 , aiming to simplify the PSO by removing intricate parameters. BBPSO employs the Gaussian algorithm during its iterative process, making it more comprehensible. It has been successfully applied to tackle complex problems, such as the traveling salesman problem. Nonetheless, BBPSO is susceptible to getting trapped in local optima 27 . Consequently, numerous researchers have made extensive efforts to enhance the algorithm’s performance in addressing this issue. In 2014, Campos introduced SMA-BBPSO 28 , an algorithm that employs a matrix following the T distribution to update particle positions. This approach enhances the balance of particles during the iteration process. In 2018, Guo presented DRBBPSO 29 , which incorporates a dynamic reconstruction strategy to bolster the algorithm’s performance by retaining elite particles. This feature helps prevent the algorithm from becoming ensnared in local optima when addressing multi-modal problems. In 2021, Guo proposed CBBPSO 30 , which not only keeps a record of global worst particles but also enhances its capability to solve high-dimensional problems. Subsequently, in 2022, Tian expanded BBPSO by incorporating a transition operator and an orbit merging operator 31 . Then, in 2023, Xiao introduced TMBBPSO 32 , which integrates two memory mechanisms into BBPSO, tailored for solving nonlinear problems. In 2016, Yong introduced the Dolphin Swarm Optimization Algorithm (DSOA) 33 , which simulates the social and hunting behaviors of barracudas within the search area. Vafashoar introduced two essential mechanisms into BBPSO in their work 34 . Firstly, they employed cellular learning automata (CLA) for parallel computation of mathematical simulation models, facilitating particle flight and path refinement. Secondly, they reoriented particle directions based on the maximum likelihood principle. The combination of these two mechanisms significantly enhances the algorithm’s capability to solve complex optimization problems. Guo’s FHBBPSO 35 introduces both a fission and fusion strategy. Initially, the particle swarm is divided into groups using the fission strategy, with each group independently seeking its optimal solution. Subsequently, the fusion strategy is employed to identify the optimal group, followed by another round of fission strategy. This cyclic process continues until the end of the iteration. The fusion strategy draws inspiration from the competitive processes observed in chimpanzee groups, and the combination of these two strategies demonstrates strong performance in solving single-objective optimization problems. Zamani proposed a Quantum-based avian navigation optimizer algorithm 36 in 2021, a Starling murmuration optimizer 37 in 2022 Nadimi-Shahraki 38 proposed an enhanced Moth-Flame optimization method in 2023. Variants of PSO algorithms find widespread applications in the field of sensors. Kim introduced a novel PSO approach for multi-sensor data fusion 39 . Senthil proposed a PSO-based method to enhance the lifespan of wireless sensor networks 40 . Wang introduced a novel resampling PSO to improve sensor network performance 41 . Moreover, PSO can optimize cooperative working strategies, energy usage strategies, and sensor co-working strategies. There are also a lot of researcher inspired from natural groups. Mirjalili citeMirjalili2014 proposed the gray wolf optimizer (GWO) in 2014. Heidari2019 42 proposed the Harris hawks optimization (HHO) in 2021. Emary proposed the Abdollahzadeh proposed the african vultures optimization algorithm and artificial gorilla troops optimizer in 2021. Xue 43 proposed theDung beetle optimizer (DBO) in 2023 As technology advances, applied research, such as sensor deployment and sensor data transmission, becomes increasingly high-dimensional and complex. To address this challenge, this paper delves into the characteristics of high-dimensional space and introduces a new nature-inspired metaheuristic algorithm: the Pair Barracuda Swarm Optimization Algorithm (PBSO). Barracudas are highly social marine mammals that typically form large groups known as pods. The arrangement of sensors can draw inspiration from the distribution of these pods. The size of barracuda pods varies depending on the species and their environment. Common barracuda pods generally consist of a few dozen to a few hundred individuals, while king barracuda pods can number over a thousand individuals. These pods exhibit a strict social structure, typically led by a male, with females and juveniles comprising the rest. The leader of the barracuda pod guides the group’s movements, food search, and other activities. Communication within barracuda groups involves various methods, including sounds, body language, and physical contact. Barracudas emit high-frequency calls that can travel significant distances, aiding in underwater communication and navigation. Barracudas frequently cooperate in activities such as fishing, protecting their young, and defending against predators. They also form supersets to hunt large fish and cetaceans collectively. Barracudas display high intelligence and learning capabilities, enabling them to use tools for obtaining food, such as fishing with hooks, and collaborate with humans in tasks like rescue operations and marine research. The main contributions of this paper are as follows: A novel evolution strategy is proposed in this paper to balance the global and local search ability of the algorithm. A Gaussian distribution is used in future position selection of particle units. Deep memory mechanism is introduced to enhance the global optimum escaping ability of the barracuda swarm. A barracuda pairs evolution model is designed to increase the optimization precision. The rest of this paper is organized as follows: Section " Materials and methods " introduces details of the proposed method; Section " Results " introduces experiments and discussion; Section " Conclusions " presents the conclusion of this work. Materials and methods Barracudas swarms in nature Survival resources within a barracuda population encompass necessities like food, water, and resting places. It is essential to distribute these resources effectively to cater to the needs of the entire population. In a barracuda swarm, the leader plays a pivotal role. The leader’s responsibilities include guiding the direction of the search and allocating resources. In some barracuda groups, the leader determines the group’s movement direction and coordinates activities such as food acquisition, thereby facilitating the collective access to survival resources. Typically, the leader barracuda enjoys priority access to food and resting places, while other group members adhere to group-defined allocation rules. In certain instances, barracudas engage in competitive resource allocation. This communication and resource allocation behavior in barracudas offers novel insights for sensor deployment strategies. For example, when searching for food, swifter or more skilled hunters among the barracudas may receive larger portions. In such cases, weaker members of the group may need to rely on cooperation and assistance to secure survival resources. Nonetheless, nature also illustrates instances of cooperative allocation and equitable distribution. In their exploration of barracuda population structures, researchers have observed that barracudas typically reside in pods. These life patterns can vary depending on the region, species, and season, but they often exhibit common patterns. barracuda groups: Most barracudas live in groups, usually consisting of dozens to hundreds of barracudas. Within these groups, barracudas often collaborate to feed, move and breed. barracuda Pairs: During the breeding season, some barracudas form pairs, consisting of a male and a female barracuda. Solitary barracudas: Some barracudas may also live alone, usually because they have been expelled or separated from the group. These solitary barracudas may find a new group or continue to live alone. In short, barracudas usually live in groups, which can consist of a few to hundreds of barracudas. barracudas often have close social bonds with each other and exhibit a variety of life patterns, including groups, families, pairs and solitude. Barracudas swarm optimization algorithm Inspired by the social structure and team behavior of barracudas, a novel barracudas swarm optimization algorithm (PBSO) is proposed in this work. The minimum evolutionary unit in PBSO is the barracuda pair, which contains separate DNA but shared memory. Roles and behaviors In PBSO, four different roles builds a stable relationship to explore the global best point. Details of different roles are listed below: barracuda pair: two barracudas in a pair, the evolutionary process involves the barracuda exchanging information with the leader barracuda and acquiring new candidate positions. Then each barracuda pair is given two new candidate positions, and the barracuda pair selects the optimal one from the two historical optimal positions and the two candidate positions. In PBSO, the barracuda pair is the standard evolutionary unit. During the evolutionary process, each barracuda participates in the computation. Best barracuda pair: The best two barracudas formed a best barracuda pair. Solitary barracuda: One of the best barracudas in history, always following the leader barracuda. Leader barracuda: The best barracuda pair and the solitary barracuda. During the evolutionary process, the leader barracuda is an aggregation of three barracuda individuals. In each iteration, every barracuda will try to move toward to the barracuda leader. The candidate position of a barracuda is calculated by Eq. ( 1 ). $$\begin{aligned} \begin{array}{l} \alpha = (individuals + leaders)/2\\ \beta = |individuals - leaders|\\ d\_candi = Gausi(\alpha ,\ \beta ) \end{array} \end{aligned}$$ (1) where individuals stands the barracudae pair in the swarm, leader are the best barracuda in the swarm, \(Gausi(\alpha , \beta )\) is the Gaussian distribution with a mean \(\alpha\) and a standard deviation \(\beta\) . Deep memory mechanism of the barracuda swarm optimization To enhance the algorithm’s performance, we employ a deep memory mechanism that mimics the pairing behavior of barracudas. This mechanism involves two types of individuals: Ordinary individuals: They possess the current position and a single layer of depth memory. Leader individuals: They have the current position and a more extensive memory with two layers of depth. This approach faithfully replicates the social structure of a barracuda school. You can find the detailed specifics of this strategy in Eq. ( 2 ). $$\begin{aligned} \begin{array}{l} individuals=(memory_1, memory_2)\\ leaders=(leader_memory_1, leader_memory_2, leader_memory_3)\\ d\_candi = Gausi(\alpha ,\ \beta ) \end{array} \end{aligned}$$ (2) where individuals stands for the normal barracudaes, the leaders stands for the leader of the barracuda swarm. Building upon this hierarchical structure, the algorithm simultaneously generates six candidate positions when calculating individual barracudae positions using Eq. ( 1 ). Subsequently, the algorithm identifies the top two positions from this pool for each barracudae individual. Once all barracuda individuals complete their updates, the algorithm can then determine the best two positions across the entire evolution. These two positions are combined with the best individuals from the previous round, resulting in five standout positions. Finally, the algorithm selects the top three positions to designate as the barracudae leader positions for this round. The process, pseudo-code and flowchart of barracudas swarm optimization algorithm The PBSO includes three major process: barracuda pairs evolution, barracuda swarm leader selection and the barracuda swarm reallocation. The flowchart of PBSO is shown in Fig. 1 . Details of all processes are summarized as follows: barracuda pairs evolution: Two barracudas entwined in pairs during the evolutionary process. They exchange information with the barracuda leader separately and rank themselves according to their fitness after updating their position. barracuda swarm leader selection: A support barracuda keeps following the leader barracuda pair. The support barracuda will engage into the evolutionary process, information exchanging, and position selection. barracuda swarm reallocation: In each iteration, each barracuda pair will generate six candidate positions with the barracuda swarm leader using Eq. ( 1 ). Then the top two positions will be selected as the new position of the barracuda pair. After all barracuda pairs get new positions, the barracuda swarm leader will update their positions with the new swam-best barracuda pair. barracuda leader: In each iteration, each barracuda pair will generate six candidate positions with the barracuda swarm leader using Eq. ( 1 ). Then the top two positions will be selected as the new position of the barracuda pair. After all barracuda pairs get new positions, the barracuda swarm leader will update their positions with the new swam-best barracuda pair. Figure 1 The flow chart of PBSO. Full size image Results Experimental methods To verify the optimization ability of proposed PBSO, the CEC2017 benchmark functions are used in validation test. The CEC2017 Benchmark Functions, also known as the IEEE Congress on Evolutionary Computation (CEC) 2017 Benchmark Functions, are a set of numerical optimization problems used to evaluate and benchmark the performance of optimization algorithms, particularly evolutionary algorithms. These benchmark functions were introduced as part of the CEC 2017 competition, which aimed to advance the field of optimization by providing a standardized set of challenging test problems. CEC2017 benchmark functions contain 4 types test functions: Unimodal Functions: fcuntion 1-2; Simple Multimodal Functions: fcuntion 3-9; Hybrid Functions: fcuntion 10-19; Composition Functions: fcuntion 20-29. In order to validate the ability of PBSO to search in high-dimensional spaces, we used the highest dimensions of CEC2017. For the control group, we chose 2 classes of well-known algorithms. The first category is 5 state-of-the-art natural-inspired methods and the second category is 5 famous particle swarm-based algorithms. Comparison experiments with state-of-the-art natural-inspired methods In this part, 5 state-of-the-art natural-inspired methods, including AVOA,DBO, GTO, GWO, and HHO, are tested with the CEC2017 benchmark functions. Experimental results are shown in Tables 1 , 2 , 3 , 4 and 5 . The mean, standard deviation, best and worst results of the 37 runs are recorded. Also, the Fridman test is implemented. The average rank and experimental parameters are shown in Table 5 . Table 1 Simulation results of AVOA, DBO, GTO, GWO, HHO and PBSO, \(f_1\) to \(f_{5}\) . Full size table Table 2 Simulation results of AVOA, DBO, GTO, GWO, HHO and PBSO, \(f_6\) to \(f_{12}\) . Full size table Table 3 Simulation results of AVOA, DBO, GTO, GWO, HHO and PBSO, \(f_{13}\) to \(f_{19}\) . Full size table Table 4 Simulation results of AVOA, DBO, GTO, GWO, HHO and PBSO, \(f_{20}\) to \(f_{25}\) . Full size table Table 5 Simulation results of AVOA, DBO, GTO, GWO, HHO and PBSO, \(f_{26}\) to \(f_{29}\) . Full size table In a total of 29 test functions, PBSO have 9 firsts, 13 seconds, 5 thirds, 1 fourth and 1 fifth. The average rank is 2.0345. PBSO performs the best among all tested algorithms and is able to consistently provide high-precision solutions to high-dimensional optimization problems. Compared withe other state-of-the-are natural-inspired methods, PBSO does not require pre-training of parameters and does not have complex control functions. The structure of the population and the adaptive evolutionary strategy provide an excellent local optimal escape ability for PBSO. Furthermore, the local organization of the PBSO algorithm enables the population to explore information more efficiently in high-dimensional spaces. At the same time, the deep memory mechanism equips the barracuda swarm with a stronger ability to escape local optima. To demonstrate the convergence capability of test algorithms in more detail, convergence diagram (CD) are shown in Figs. 2 , 3 , 4 , 5 , 6 , 7 and 8 . Figure 2 The CD curve of \(f_{1-4}\) for BBPSO, DLSBBPSO, PBBPSO, TBBPSO, ETBBPSO and PBSO. Full size image Figure 3 The CD curve of \(f_{5-8}\) for BBPSO, DLSBBPSO, PBBPSO, TBBPSO, ETBBPSO and PBSO. Full size image Figure 4 The CD curve of \(f_{9-12}\) for BBPSO, DLSBBPSO, PBBPSO, TBBPSO, ETBBPSO and PBSO. Full size image Figure 5 The CD curve of \(f_{13-16}\) for BBPSO, DLSBBPSO, PBBPSO, TBBPSO, ETBBPSO and PBSO. Full size image Figure 6 The CD curve of \(f_{17-20}\) for BBPSO, DLSBBPSO, PBBPSO, TBBPSO, ETBBPSO and PBSO. Full size image Figure 7 The CD curve of \(f_{21-24}\) for BBPSO, DLSBBPSO, PBBPSO, TBBPSO, ETBBPSO and PBSO. Full size image Figure 8 The CD curve of \(f_{25-29}\) for BBPSO, DLSBBPSO, PBBPSO, TBBPSO, ETBBPSO and PBSO. Full size image Comparison experiments with PSO-based methods In this part, the standard BBPSO, DLSBBPSO, PBBPSO, TBBPSO, and ETBBPSO are used in control group. The mean, standard deviation, best and worst results of the 37 runs are recorded in Tables 6 , 7 , 8 , 9 , and 10 . In a total of 29 test functions, PBSO gets 23 firsts, 2 seconds, 1 thirds, 2 fourths, and 1 sixths, the average rank is 1.52. Also, the Fridman test is implemented. The average rank and experimental parameters are shown in Table 10 . Table 6 Simulation results of BBPSO, DLSBBPSO, ETBBPSO, PBBPSO, TBBPSO and PBSO, \(f_1\) to \(f_{6}\) . Full size table Table 7 Simulation results of BBPSO, DLSBBPSO, ETBBPSO, PBBPSO, TBBPSO and PBSO, \(f_7\) to \(f_{13}\) . Full size table Table 8 Simulation results of BBPSO, DLSBBPSO, ETBBPSO, PBBPSO, TBBPSO and PBSO, \(f_{14}\) to \(f_{20}\) . Full size table Table 9 Simulation results of BBPSO, DLSBBPSO, ETBBPSO, PBBPSO, TBBPSO and PBSO, \(f_{21}\) to \(f_{27}\) . Full size table Table 10 Simulation results of BBPSO, DLSBBPSO, ETBBPSO, PBBPSO, TBBPSO and PBSO, \(f_{28}\) to \(f_{29}\) . Full size table Discussion In both sets of experiments, PBSO consistently outperformed other methods. When compared to nature-inspired algorithms, PBSO achieved impressive results with 9 first-place rankings, 13 second-place rankings, 5 third-place rankings, 1 fourth-place ranking, and 1 fifth-place ranking. On average, it ranked 2.03, securing the top position among all algorithms. However, PBSO’s performance was less satisfactory when applied to single-modal test functions. This can be attributed to the fact that PBSO was not originally designed with a specialized evolutionary strategy for single-modal functions, which presents an important avenue for future research. In contrast to PSO-based algorithms, PBSO excelled with 23 first-place rankings, 2 second-place rankings, 1 third-place ranking, 1 fourth-place ranking, and 1 sixth-place ranking, averaging an impressive 1.52 across all rankings and taking the first position among all algorithms. The experimental results suggest that, compared to traditional particle swarm algorithms, PBSO offers several advantages, including higher optimization accuracy, a simpler structure, and greater ease of understanding. More specifically, the pairwise barracuda structure significantly enhances interconnections between barracuda individuals, while the deep memory mechanism increases their chances of escaping local optima in high-dimensional search spaces. Furthermore, the leadership barracuda, equipped with a three-layer memory setting and a focus on balancing search resources, enhances the overall search accuracy of the entire barracuda group. In summary, the experiments clearly demonstrate that PBSO is capable of providing highly precise solutions for high-dimensional single-objective optimization problems. Conclusions In this study, we introduce a novel metaheuristic approach inspired by nature, known as the Pair Barracuda Swarm Optimization algorithm (PBSO). PBSO is designed to emulate the social structure and collective behavior observed in barracuda swarms. The Pair Barracuda structure enhances the ability of individual barracudas to escape local optima. To enhance the search accuracy in high-dimensional spaces, we have devised an innovative iterative strategy. Notably, both the new structure and the iterative strategy have linear complexity, resulting in a time complexity of O(n) for PBSO. PBSO is compared to its predecessor, PBSO, and is found to be simpler, more user-friendly, and more robust in functional simulations. The experimental results consistently support PBSO’s superior performance. To further evaluate PBSO’s capabilities, we conducted high-dimensional simulations using the CEC2017 benchmark functions with a test dimension of 100. These experimental results firmly establish PBSO as the leading algorithm across all tested scenarios, providing dependable solutions for high-dimensional optimization challenges. However, it’s worth noting that PBSO tends to converge towards local optima when dealing with combinatorial optimization problems. This issue is attributed to the limited information transfer from the barracuda leader and the shallow memory of the barracuda pair. Consequently, future research should focus on improving the speed of information transfer from the barracuda leader to the common barracuda and enhancing the memory depth of barracudas. Additionally, exploring the application of PBSO in real-world scenarios, such as wireless sensor networks, holds promise for future investigations.