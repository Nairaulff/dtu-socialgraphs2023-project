Introduction Optimization problems have a large number of applications in many fields, such as engineering design, optimization of structural parameters, financial investments, etc. 1 , 2 . To improve the handling of these problems, a series of global optimization algorithms have been developed to traditional mathematical theories and solution methods 3 , 4 , which are divided into deterministic optimization algorithms and stochastic optimization algorithms 5 . As stochastic optimization algorithms, metaheuristic optimization algorithms have high solution accuracy and efficiency, mainly including genetic algorithm (GA) inspired by biological evolution 6 , simulated annealing algorithm (SA) 7 and gravity search algorithm (GSA) inspired by physical principles 8 , particle swarm algorithm (PSO) inspired by animal population behavior 9 and artificial bee colony algorithm (ABC) 10 . As optimization problems become more and more complex, many metaheuristic optimization algorithms have been presented to solve large-scale global optimization (LSGO) problems, such as the wild goose algorithm (WGA) 11 , the african condor optimization algorithm (AVOA) 12 , the australian wild dog optimization algorithm (DOA) 13 . Conscious neighborhood-based crow search algorithm (CCSA) 14 , starling murmuration optimizer (SMO) 15 , diversity-maintained multi-trial differential evolution algorithm (DMDE) 16 and enhanced moth-flame optimization algorithm using an effective stagnation finding and replacing strategy (MFP-SFR) 17 , etc. Compared with the other metaheuristic algorithms, the ABC has advantages such as few control parameters, easy implementation, and outstanding exploration capability, it performs both global and local optimal solution searches during each iteration, therefore the probability of being able to find the optimal solution is greatly increased. The ABC is a novel swarm intelligence algorithm by simulating the foraging behavior of bees, which is widely used in the fields of PID parameter optimization, image processing, numerical optimization, structural design, etc. Bingul et al. 18 compared the PSO and the ABC to find the best performance parameters of the PID controller, and the results of the robustness analysis show that the PID controller parameters adjusted by the ABC have stronger robustness under the internal and external perturbations. Öztürk et al. 19 analyzed the improved ABC for medical image processing proposed during 2010–2020. Hussain et al. 20 proposed the improved ABC for copolymerization of high-dimensional data, and showed that the combination of a new similarity measure and an optimized local search method, significant progress was achieved in searching for optimal clusters. Sagayam et al. 21 proposed a hybrid one-dimensional HMM model with ABC to optimize its parameters and observed state sequences to improve performance, and the results showed a very low recognition error rate. Li et al. 22 proposed an ABC algorithm-based structural design optimization method for fiber-reinforced plastic (FRP) vessels, and the results showed that the weight of a 32.98 m FRP fishing vessel could be reduced by 8.31%. However, the traditional ABC contains some disadvantages, such as slow convergence, easy stagnation, etc. Therefore, many researchers delivered a lot of improvement measures to enhance the convergence speed and exploitation capability of ABC. Zhang et al. 23 proposed an improved ABC algorithm with a unitary inheritance (OPI) mechanism (OPIABC), to address the fact that the solution in the ABC varies in only one dimension. Wang et al. 24 presented a selection method based on the radius of the neighborhood, which improves the search phase of the detection bee and enhances the exploitation of the ABC. Shi et al. 25 introduced the concept of queen bee to propose a new neighborhood search mechanism and improved the dimensional selection strategy to realize the conversion between one-dimensional search and full-dimensional search. In real bee colonies, onlooker bees and employed bees have different exploitation mechanisms, and onlooker bees choose the best one nectar source for exploitation. Karaboga et al. 26 designed a new search equation for onlooker bees, and the proposed quick artificial bee colony algorithm (qABC) more accurately simulates the behavior of onlooker bees and improves the local search capability of the ABC. To enhance the global convergence speed, Gao et al. 27 introduced a new search mechanism which include logistic chaos mechanism and backward learning to improve algorithm, the mechanism can control the probability of introducing two search equations. Xiao et al. 28 proposed a new adaptive neighborhood search gaussian perturbation algorithm (NGABC), which first used an adaptive method to adjust the neighborhood, then applied the global optimal solution to guide the search, and finally designed a new Gaussian perturbation. To solve the shortage of ABC algorithm with strong exploration capability and weak development capability, Zhu et al. 29 proposed the Gbest-guided artificial bee colony algorithm (GABC), which adds the influence of global optimal solution to the neighborhood search equation and improves the exploitation capability of the algorithm. Zheng et al. 30 used cat chaos mapping to increase the diversity of the solutions in the initial stage, applied differential evolution to improve the search strategy, and designed adaptive scaling factors to achieve dynamic search. To improve the development efficiency and convergence speed, Chouaib et al. 31 proposed a multiple population ABC based on global and local optima (MPGABC), which divides the population into multiple subpopulations and introduces global and local optimal solutions in the search equation of the solution. Brajevi´ et al. 32 added shuffle variation operators to the hiring bee and onlooker bee phases, making the algorithm get a good balance between global search ability and local exploitation ability to solve integer programming and minimax problems. Zhao et al. 33 proposed a novel method (QABC) with a search equation based on the idea of quasi-Avanti transformation, which enhanced the exploitation capability of the algorithm, and then introduced a collaborative search matrix to update the position of the nectar source to ensure the randomness and balance of the search. Even though the improved ABC algorithms can produce satisfying solutions in solving optimization problems, with regard to effectiveness and efficiency (such as slow convergence speed, easy premature maturation), there is still space for improvement further of ABC performance. In this study, a novel chaotic and neighborhood search-based artificial bee colony algorithm (CNSABC) is proposed. The proposed CNSABC includes three novel mechanisms, which are chaotic mapping with mutual exclusion mechanism, neighborhood search mechanism with compression factor and sustained bees. The chaotic mapping with mutual exclusion mechanism is introduced to have better ergodicity in the solution space and enhance global exploration; the neighborhood search mechanism with compression factor is presented to and enhance the convergence efficiency and local exploitation capability. A new type of bee named sustained bees is proposed to improve the ability to explore optimal solution, further to avoid the appearance of premature maturity in some degree. These three strategies work together to improve the performance of global exploration and local exploitation, resulting in a faster convergence speed and decent quality of solutions for the ABC. To verify the performance of CNSABC and confirm the effectiveness of the proposed mechanisms, three sets of numerical experiments are conducted on selected 26 benchmark functions. The first set includes CNSABC and the ABC algorithm with a single strategy for improvement, the second set includes CNSABC and five commonly used metaheuristic optimization algorithms (ABC, PSO, GWO 34 , WOA 35 , and BOA 36 ), the last set includes CNSABC and five improved ABC algorithms in other literatures (qABC, SBABC, MPGABC, GABC, and NGABC). In addition, the Tension/compression spring design problem and the Speed reducer design problem are used to test the ability of the CNSABC for solving real engineering problems. The results verify the dominant of CNSABC with regard to the convergence speed and optimal solution search ability, which can also indicate the three proposed mechanisms play a guiding role on enhancing ABC algorithm. The rest sections of this study are arranged as follows. Section " Improved ABC algorithm " introduces the principle and pseudo-code of the ABC algorithm and the strategy of the CNSABC in detail. Section " Experimental results and analyses " shows three sets of experimental results comparing the CNSABC with other algorithms and analyzes the effect of the improvements. Section " CNSABC for solving engineering optimization problems " uses two engineering example problems to verify the practicality of CNSABC for solving practical problems. Section " Conclusions " summarizes this research and illustrates some future research directions. Improved ABC algorithm Traditional ABC algorithm ABC algorithm is inspired by the foraging behavior of honeybees. The foraging behavior of bees is shown in Fig. 1 . In a colony, there are three main types of bees: employed bees (E), onlooker bees (O), and scout bees (S). Each bee is closely related to the location of the nectar source. Employed bees harvest nectar near the initial nectar source (A, B) and share nectar information (EF1) within the hive (Dance area A, B), onlooker bees select some better nectar sources to exploit (EF2), and when the nectar source has no nectar, onlooker bees transform into scout bees to find new nectar sources. ABC algorithm is an iterative process and the steps of the algorithm are as follows. Figure 1 Honey foraging behavior. Full size image Initialization The nectar source in the ABC is a multidimensional vector. ABC starts the search from a set of randomly distributed initial nectar sources, which are generated by using Eq. ( 1 ). $$x_{ij} = x_{j}^{\min } + rand \times \left( {x_{j}^{\max } - x_{j}^{\min } } \right)$$ (1) in which, i and j take values in the interval [1, SN ] and [1, D ], \(x_{j}^{\min }\) and \(x_{j}^{\max }\) are the lower and upper limits of the j- th dimension, SN is the total number of nectar sources, D is the number of dimensions, and rand is a uniform random number in the value interval [0, 1]. Employed bees phase Each employed bee is associated with a nectar source ( \(x_{ij}\) ), and performs a neighborhood search in the vicinity of the associated nectar source, which in turn produces a new nectar source ( \(v_{ij}\) ), the location is generated by using Eq. ( 2 ). $$v_{ij} = x_{ij} + \phi_{ij} \times \left( {x_{ij} - x_{kj} } \right)$$ (2) in which, i is the serial number of the current honey source, k is the serial number of other honey sources, k ∈ [1, SN], k ≠ i , \(\phi_{ij}\) is a random number within the value interval of [− 1, 1]. In this phase, the employed bee compares the original nectar source with the new one and then chooses the better one to develop and returns to the hive to share the information of the better nectar source with other bees. Onlooker bees phase Onlooker bees evaluate all known honey sources and select a certain probability of nectar source for exploitation, the probability ( \(p_{i}\) ) of nectar source selection is calculated by using Eq. ( 3 ). $$p_{i} = \frac{{fit_{i} }}{{\sum\nolimits_{j} {fit_{j} } }}$$ (3) in which, \(fit_{i}\) is the fitness value of the i- th nectar source. The nectar source with better fitness value is more likely to be selected and then follow the bee to exploit this source and then update the nectar location by Eq. ( 2 ). Like the employed bee, the better nectar source will be selected for retention based on the greed criterion. Scout bee phase Employed bees and onlookers may keep taking nectar from an un-renewed nectar source, therefore an upper limit is set on the number of times a nectar source can be exploited. When the upper limit of exploitation is reached and the nectar source is not renewed, this source is abandoned, and the bees that were following it are transformed into scout bees to regenerate a new source using Eq. ( 1 ). The proposed CNSABC Bernoulli chaotic mapping with mutual exclusion mechanism The initial solution of the traditional ABC is generated by using Eq. ( 1 ), where rand is a uniformly distributed random number belongs to the pseudo-random number 37 , 38 , 39 , 40 , 41 . However, when solving high-dimensional problems, the initial population generated in this way is not uniform enough. Therefore, it is not guaranteed to get a better population in the global search. In addition, the use of rand in the search process reduces the local search ability in the employed bees phase and onlooker bees phase. To overcome this shortcoming, a chaotic mapping with mutual exclusion mechanism is introduced to generate the initial population. Chaos mapping methods mainly include Logistic chaotic mapping and Bernoulli chaotic mapping 42 , 43 , 44 . Among them, the Logistic chaotic mapping is the most widely used. However, the logistic chaotic mapping has a high probability of taking values in the interval [0, 0.1] and [0.9, 1], which is not uniformly traversed in the global optimization search process and may lead to reduce the efficiency of the algorithm. Bernoulli chaos mapping is uniformly distributed between [0, 1]. Compared with Logistic chaos mapping, Bernoulli chaos mapping has better traversal uniformity and randomness. The chaos value distribution of Logistic chaos mapping and Bernoulli chaos mapping at 10 5 iterations is shown in Fig. 2 . Figure 2 The chaos number distribution. Full size image The Bernoulli chaotic mapping is e expressed by Eq. ( 4 ). $$z_{k + 1}^{B} = \left\{ {\begin{array}{*{20}l} {z_{k}^{B} /(1 - \beta ),} \hfill & {z_{k}^{B} \in \left( {0,1 - \beta } \right]} \hfill \\ {(z_{k}^{B} - 1 + \beta )/\beta ,} \hfill & {z_{k}^{B} \in \left( {1 - \beta ,1} \right]} \hfill \\ \end{array} } \right.$$ (4) in which, the range of \(\beta\) is (0,1). In the range of \(\beta\) , the system is in the chaotic state. By introducing the Bernoulli chaotic mapping with mutual exclusion mechanism into the initial population generation equation, Eq. ( 1 ) becomes into Eqs. ( 5 ) and ( 6 ). The initial individual \(x_{ij}\) is considered to be the one with the best fitness, as shown in Eq. ( 7 ). The mutual exclusion mechanism makes the search direction into two opposite directions, which can improve the exploration capability. $$x_{i1j} = x_{j}^{\min } + z^{B} (x_{j}^{\max } - x_{j}^{\min } )$$ (5) $$x_{i2j} = x_{j}^{{{\text{m}} ax}} - z^{B} (x_{j}^{\max } - x_{j}^{\min } )$$ (6) $$x_{ij} = \min (x_{i1j} ,x_{i2j} )$$ (7) The pseudo-code of Bernoulli chaotic mapping is shown in Fig. 3 . Figure 3 The pseudo-code of Bernoulli chaotic mapping. Full size image The standard deviation of the initial population individuals generated by Bernoulli chaos mapping with mutual exclusion mechanism, and the initial population individuals generated by rand are counted in the value range of [− 100, 100] to generate 30-dimensional, 50-dimensional and 100-dimensional population individuals, respectively. The larger the standard deviation of the generated initial population individuals, the better the initial population diversity. The standard deviation results are shown in Table 1 . Table 1 Standard deviation results. Full size table Neighborhood search mechanism with compression factor In nature, employed bees and onlooker bees play different roles in a bee colony. The main purpose of employed bees is to explore more nectar sources, and onlooker bees is to exploit known nectar sources. However, in traditional ABC algorithm, the same neighborhood search way is used for both employed and onlooker bees to simulate nectar collection behavior. This way leads to limit exploration and insufficient exploitation of the bee colony. Therefore, to balance the exploration and exploitation capabilities of the algorithm, a neighborhood search mechanism with compression factor is proposed by improving search mechanism for the employed bees and onlooker bees. In the new neighborhood search mechanism, the employed bee focuses on expanding the search range and enhancing the global exploration ability, as expressed in Eq. ( 8 ). And the onlooker bee focuses on improving the exploitation ability and doing local exploration to obtain better solutions, as shown in Eq. ( 9 ). $$v_{ij} = x_{ij} + \phi_{ij} \left( {x_{ij} - x_{kj} } \right) + \psi_{ij}^{1} (x_{best} - x_{ij} )$$ (8) $$v_{ij} = cp \times x_{ij} + C \times \phi_{ij} \left( {x_{ij} - x_{kj} } \right) + \psi_{ij}^{2} (x_{best} - x_{ij} )$$ (9) $$cp = \frac{2}{{\left| {2 - (C \times mean(\phi_{i,:} ) + \psi_{ij} ) - \sqrt {(C \times mean(\phi_{i,:} ) + \psi_{ij}^{2} )^{2} - 4(C \times mean(\phi_{i,:} ) + \psi_{ij}^{2} )} } \right|}}$$ (10) $$C = \frac{1}{{\pi (1{ + }iter^{2} )}}$$ (11) $$\phi_{ij} = - 1 + 2z^{B}$$ (12) $$\psi_{ij}^{1} = 1.5z^{B}$$ (13) in which, \(cp\) is the adaptive compression factor, it can be calculated by Eq. ( 10 ), the theoretical value range is [0, 1]; C is the Cauchy distribution, calculated by Eq. ( 11 ); \(\phi_{ij}\) is a uniform random number with values ranging from [− 1, 1], calculated by Eq. ( 12 ); \(\psi_{ij}^{1}\) is a uniform random number with values ranging from [0, 1.5], which can be calculated by Eq. ( 13 ) ; iter is the number of generations of the current iteration. The weight \(\psi_{ij}^{2}\) influences the local exploration of the onlooker bees, and then has an impact on the exploitation capacity of the onlooker bees. To test the effect of the parameter \(\max \psi_{ij}^{2}\) on the search ability of the onlooker bees, tests are performed with \(\max \psi_{ij}^{2}\) = 1.5, 4, 6 and 8 respectively. The test functions are F1, F6, F15 and F25. The colony size is 20, the number of iterations is 1000. Due to their random peculiarity, intelligent heuristic algorithms may generate better or worse solutions than those they anteriorly produced in exploring new solutions. Thus, it is a good select to compare the results by the statistical approach. Then, when testing each function, all algorithms run independently 20 times, the results are shown in Table 2 . Table 2 Test results. Full size table As shown in Table 2 , it can be seen that \(\max \psi_{ij}^{2}\) = 6 in the search formula of the onlooker bee is the most appropriate. Therefore, \(\psi_{ij}^{2}\) is a uniform random number with values ranging from [0, 6], which can be calculated by Eq. ( 14 ). $$\psi_{ij}^{2} = 6z^{B}$$ (14) To ensure the randomness of exploration, the generation of \(\phi_{ij}\) and \(\psi_{ij}^{1}\) , \(\psi_{ij}^{2}\) all introduce Bernoulli chaos mapping. The introduction of cp and C will further improve the capability of local search. Sustained bees In traditional ABC algorithm, the colony has three kinds of bees. Due to the mechanism of scout bees, each bee has an upper limit of exploitation, which will lead to some bees may develop to a more optimal solution, but give up exploitation because the upper limit of exploitation has been reached. Therefore, a new bee species is proposed that will continuously exploit the current optimal honey source without the upper limit of exploitation, called sustained bees. Sustained bees are influenced by the global optimal solution to develop new solutions based on the current optimal solution. The update formula of the sustained bee is shown in Eq. ( 15 ) $$v_{ij} = x_{iter} + \frac{2}{1 + iter} \cdot rand(x_{best} - x_{iter} ) + \frac{1}{1 + iter} \cdot rand(x_{ij} - x_{kj} )$$ (15) in which, \(x_{iter}\) is the current optimal solution. A proposed variant of ABC In this study, the traditional ABC algorithm is combined with three improvements, including the Bernoulli chaotic mapping with mutual exclusion mechanism, neighborhood search mechanism with compression factor and sustained bees. Then, a novel chaos and neighborhood search-based ABC algorithm (CNSABC) is formed. Figure 4 shows the pseudo-code of CNSABC. Figure 4 Pseudo-code of CNSABC. Full size image Time complexity is an important tool to determine the computational complexity of an algorithm. Generally, the time complexity of algorithm is determined by the population size, variable dimensionality and fitness function. In the proposed CNSABC, the population size is SN and the dimensionality is D . Assuming that the parameter initialization time is t 0 and the initialization solution time is t 1 , this, the time complexity of the initialization phase is shown in Eq. ( 16 ). $$T_{1} { = }O\left( {t_{0} + SN \cdot D \cdot t_{1} } \right)$$ (16) In the iterative process, the number of iterations is K , and f(D) is the time to calculate the fitness value of the optimal individual. The time to select the better individual in the hire in colony is t 2 , the time to replace the last iteration individual in the employed bee phase is t 3 , the time to replace the last iteration individual in the onlooker bee phase is t 4 , the time to replace the last iteration individual in the scout bee phase is t 5 , the time to replace the last iteration individual in the sustained bee phase is t 6 , and the time to calculate the weight is t 7 , so the time complexity of the employed bee phase is Eq. ( 17 ) is shown. $$T_{2} = O\left( {t_{4} + SN \cdot D \cdot (t_{3} + f(D))} \right)$$ (17) $$T_{3} = O\left( {t_{2} + t_{4} + SN \cdot D \cdot (t_{4} + t_{5} + f(D))} \right)$$ (18) $$T_{4} = O\left( {D \cdot t_{6} } \right)$$ (19) In conclusion, the time complexity of the CNSABC can be calculated by Eq. ( 20 ) $$T = T_{1} + K \cdot (T_{2} + T_{3} + T_{4} )$$ (20) Experimental results and analyses In this part, to verify the performance of the presented CNSABC, comprehensive experiments are conducted and analyzed based on the experimental results obtained from 26 benchmark test functions. Firstly, the 26 benchmark test functions are presented and the strategies in Sect. " Improved ABC algorithm " are tested individually to confirm the effectiveness of their improvements. Then CNSABC is compared with five standard algorithms (ABC, PSO, GWO, WOA, and BOA) and finally with five advanced improved ABC (qABC, SBABC, MPGABC, GABC, and NGABC). In addition, all algorithms are coded by Matlab 2020a and run on Intel ® Core i5-2400, CPU@3.0 GHz, 2 GB RAM and Windows 10 computer. Benchmark functions In this experiment, the 26 test functions proposed by Zhong 45 , Luo 46 , Gao 47 , Zhu and Kwong 29 , Karaboga and Akay 48 are used to test the effectiveness of the presented CNSABC. These test functions include unimodal separable (US) functions (F1, F2, and F3), unimodal non-separable (UN) functions (F4, F5, F6, F7, F8, F9, F10, F11, F12 and F26), multimodal separable (MS) functions (F13, F14, F15, F16, and F17), and multimodal non-separable (MN) functions (F18, F19, F20, F21, F22, F23, F24, and F25). The test functions are shown in Table 3 , respectively. Specifically, the unimodal functions can be applied to test the exploitation capability of the algorithm and the multimodal functions can be applied to test the exploration capability. Table 3 Benchmark functions. Full size table Influence of improvement points To determine the effectiveness of the three improvement strategies, each improvement strategy is combined with the ABC algorithm separately to form three variants of ABC. In Table 4 , "√" indicates that the improvement strategy is used in combination with ABC, and " ● " indicates that the variant of ABC does not use the improvement strategy. In fairness, the same initial parameters are used for all algorithms throughout the testing process, which is a population size of 40, maximum iterations K = 1000, and each benchmark function tested 40 times. The experimental results containing the mean, standard deviation, maximum and minimum values are recorded in Table 5 . The last row of Table 5 indicates the advantages and disadvantages of this algorithm compared with the ABC, and the symbols " + ", "−" and " = " indicate algorithms that are better than, worse than and equal to other comparisons, respectively. Table 4 ABC with three improvement strategies. Full size table Table 5 Experimental results of different algorithms. Full size table By comparing the mean and standard deviation in Table 5 , it can be analyzed that sustained bees and Bernoulli chaotic mapping with mutual exclusion mechanism have limited improvement on ABC and performs poorly in the tests of the 4 benchmark functions F7, F11, F16 and F25; the neighborhood search mechanism with compression factor has stronger improvement on ABC; and the CNSABC shows stronger exploration and exploitation when the three improved strategies work simultaneously. In the benchmark functions F4, F5, F13, F14, F15, F20, F21 and F24 are extremely close to the theoretical optimal values, due to the fact that the values exceed the display digits of Matlab, thus, the decimal digits are not displayed. The performance of each algorithm in the tests of benchmark functions F5, F19 and F23 is not extremely different. In addition, each comparison algorithm is compared with the CNSABC for 5% nonparametric statistics Wilcoxon test and Friedman test, the p -values of Wilcoxon test for ABC, CABC, NABC and SABC are 1.5856E−07, 8.3922E−09, 1.3414E−10 and 1.6690E−08 respectively, which are less than 5%, indicating that there are remarkable distinctions between algorithms. Friedman test is shown in Table 6 , Mean-rank represents the average ranking of each algorithm, and smaller values represent better algorithm performance. In conclusion, the three proposed mechanisms, including the Bernoulli chaotic mapping with mutual exclusion mechanism, a neighborhood search mechanism with compression factor and sustained bees, have superiority to improve the performance of ACO. Table 6 Mean rank of Friedman test between CNSABC and comparison algorithm. Full size table Comparison with other advanced original algorithms To verify the advantage of the CNSABC, five commonly used metaheuristic optimization algorithms including PSO, ABC, GWO, WOA and BOA are used for comparison. The parameters of different algorithms are set to be same: population size N = 20, maximum iterations K = 1000, and each benchmark function tested 40 times. The other parameters are set based on the recommended values in the original manuscript of the literature, and the mean, standard deviation, minimum and maximum values of the results of the 40 experiments are counted in Table 7 . In Table 7 , " + ", "−" and " = " indicate the number of benchmark functions in which the CNSABC is better, worse and equal to other original algorithms or the number of benchmark functions in which other original algorithms are better, worse and equal to the CNSABC among the 26 benchmark functions, respectively. Table 7 Comparison results of the CNSABC and other algorithms. Full size table Specifically, for the results of the mean, CNSABC gets the best results in 20 out of 26 benchmark functions (F1, F2, F3, F4, F5, F8, F9, F10, F12, F13, F14, F15, F19, F20, F21, F22, F23, F24, F25 and F26); For the standard deviation, CNSABC obtains the best results in 18 functions (F1, F2, F3, F4, F5, F9, F10, F12, F13, F14, F15, F19, F20, F21, F22, F24, F25 and F26). This can indicate that the CNSABC has a strong exploitation capability and stability. Meanwhile, CNSABC obtains the optimal result for the minimum value among 22 functions (F1, F2, F3, F4, F5, F8, F9, F10, F11, F12, F13, F14, F15, F16, F18, F19, F20, F21, F22, F23, F24 and F25). The result indicates that the CNSABC is the best performance among the compared algorithms. Figure 6 shows representative convergence curves for only a subset of the 26 benchmark functions (F1, F3, F6, F10, F13, F15, F19, and F25) 49 . To allow for a more intuitive comparison, the y -axis of the convergence curves is the logarithm of the fitness, besides F19 function. From the results in Table 7 , it can be seen that the CNSABC demonstrates excellent performance of other functions in the tests of F2, F3, F4, F9, F12, F19, F25 and F26, however, the worst performance in the tests of F16 and F17. In Fig. 5 , the convergence curves show different characteristics for different search strategies, one is a slow convergence from the beginning of the iteration, and the other is a cliff-like decline in the convergence process. The first kind of curve mainly reflected in the single-peaked benchmark functions, which are F1–F14. And the second kind of curve mainly reflected in the multimodal benchmark functions, which are F15–F25. By analyzing the different types of benchmark functions, the CNSABC has excellent performance in both convergence accuracy and convergence process. Most convergence curves are the first kind of convergence curves, reflecting that the CNSABC achieves a well balance between exploitation capability and exploration capability. Each comparison algorithm is compared with CNSABC for 5% nonparametric statistics Wilcoxon test and Friedman test. p -values of Wilcoxon test for PSO, GWO, WOA BOA and ABC are 7.2866E−04, 5.4838E−11, 3.4412E−11, 5.3564E−15, 2 and 1.2364E−07, respectively, which are less than 5%, indicating that there are remarkable distinctions between algorithms. Friedman test can be seen in Table 8 , Mean-rank represents the average rank of each algorithm, the smaller the value is, the wonderful performance of the algorithm is. Figure 5 A subset of the 26 benchmark functions. Full size image Table 8 Mean rank of Friedman test between the CNSABC and comparison algorithm. Full size table Comparison with other improved ABC algorithms In this part, the performance of the CNSABC is compared with other improved ABC algorithms in 26 benchmark function tests, including qABC, SBABC, MPGABC, GABC and NGABC. To be fair, the different algorithms all contain parameters with the same settings: population size N = 20, maximum iterations K = 1000, and 40 tests for each benchmark function. The experimental results are recorded in Table 9 , containing the mean, standard deviation, minimum and maximum values of the results obtained from the 40 sets of experiments. in the last row of Table 9 , " + ", "−" and " = " indicate the amount of benchmark functions in which the CNSABC is better, worse and equal to other algorithms or the number of benchmark functions in which other algorithms are better. In Table 9 , in terms of the mean, the CNSABC obtains the best results for 21 of the 26 benchmark functions (F2, F3, F4, F5, F6, F8, F9, F10, F11, F12, F13, F14, F15, F18, F19, F20, F21, F22, F23, F24 and F25). In terms of the minimum value, the CNSABC obtains the best results for 22 functions (F2, F3, F4, F5, F6, F8, F9, F10, F11, F12, F13, F14, F15, F16, F18, F19, F20, F21, F22, F23, F24 and F25). The results indicate that CNSABC has wonderful performance in terms of the convergence accuracy. Figure 7 shows representative convergence curves for only a subset of the 26 benchmark functions (F2, F3, F6, F10, F13, F15, F19, and F25). Table 9 Comparison results of CNSABC with other improved ABC algorithms. Full size table In Table 9 , the CNSABC is stronger than other improved ABC algorithms, whether it is a single-peak benchmark function or a multimodal benchmark function, which also reflects the exploration and development capability of the CNSABC to achieve a well balance. In Fig. 6 , the CNSABC has outstanding superiority compared with other improved ABC algorithms. In most of the convergence curves show a slow decreasing trend from the beginning of the iterations, but have been better in fitness than other algorithms, which indicates that the CNSABC has a strong exploration and exploitation capability. This is attributed to the introduction of three mechanisms. Each comparison algorithm is compared with CNSABC for 5% nonparametric statistics Wilcoxon test and Friedman test. p -values of Wilcoxon test for qABC, SBABC, MPGABC GABC and NGABC are 1.0921E−09, 6.2631E−10, 7.0176E−10, 1.8958E−05 and 1.2600E−02 respectively, Moreover, the better results of CNSABC also can be demonstrated by Friedman test in Table 10 . Figure 6 A subset of the 26 benchmark functions. Full size image Table 10 Mean rank of Friedman test between CNSABC and comparison algorithms. Full size table CNSABC for solving engineering optimization problems Tension/compression spring design optimization problem The main objective of this engineering problem is to minimize the mass of the tension/compression spring. The optimization constraints of this problem are described as follows: (1) Shear stress. (2) Surge frequency. (3) Minimum deflection. The schematic diagram of spring is exhibit in Fig. 7 Figure 7 Schematic view of tension–compression spring design. Full size image . This problem has three variables: wire diameter ( d ), mean coil diameter ( D ), and the number of active coils (P). The mathematical model is described as follows: Consider: $${\vec{\text{x}}} = [x_{1} \, x_{2} \, x_{3} ] = [d \, D \, P]$$ Minimize: $$f(\vec{x}) = (x_{3} + 2)x_{2} x_{1}^{2}$$ Subject to: $$\begin{gathered} g_{1} \left( {\vec{x}} \right) = 1 - \frac{{x_{2}^{3} x_{3} }}{{71785x_{1}^{4} }} \le 0 \hfill \\ g_{2} \left( {\vec{x}} \right) = \frac{{4x_{2}^{2} - x_{1} x_{2} }}{{12566\left( {x_{2} x_{1}^{3} - x_{1}^{4} } \right)}} + \frac{1}{{5108x_{1}^{2} }} \le 0 \hfill \\ g_{3} \left( {\vec{x}} \right) = 1 - \frac{{140.45x_{1} }}{{x_{2}^{2} x_{3} }} \le 0 \hfill \\ g_{4} \left( {\vec{x}} \right) = \frac{{x_{1} + x_{2} }}{1.5} - 1 \le 0 \hfill \\ \end{gathered}$$ $$0.05 \le x_{1} \le 2.0, \, 0.25 \le x_{1} \le 1.3, \, 2.0 \le x_{1} \le 15.0$$ In fairness, CNSABC uses the same penalty function as the other algorithms, the results are shown in Table 11 . Table 12 shows the mean, standard deviation, minimum and maximum values of the 10 experiments of the CNSABC. Figure 8 shows the adaptation convergence curve of CNSABC computing the tension/compression spring design. The best solution is obtained by CNSABC at design variables \(\vec{x} = [x_{1} \, x_{2} \, x_{3} ]\) with \(f(\vec{x}) = {0}{\text{.012192037027776}}\) . In solving tension/compression spring design problem, results show that the optimal weights compared with EPO, SHO, GWO, MVO, SCA, EPO, DE, ES, GA, RO, improved HS, HSCA, CB-ABC and I-ABC greedy, CNSABC increased by 3.6735%, 3.8028%, 3.8356%, 4.87755%, 4.0727%, 3.6728%, 3.7739%, 3.8559%, 4.0630%, 3.8392%, 3.7770%, 3.3734%, 3.3734%, and 3.3734%, respectively. CNSABC has superiority performance than the other algorithms. Table 11 Comparison results for tension/compression spring design. Full size table Table 12 Statistical results obtained from CNSABC for tension/compression spring design. Full size table Figure 8 Convergence analysis of CNSABC for tension/compression spring design. Full size image Speed reducer design optimization problem The main objective of this engineering problem is to minimize the mass of the reducer as much as possible. There are 7 design variables in this model, which are the face width ( b) , the tooth die ( m ), the number of pinion teeth ( p ), the length of the first and second shaft between the bearings ( l 1 ), ( l 2 ), the diameter of the first shaft ( d 1 ) and the diameter of the second shaft ( d 2 ). The design variables of the reducer are reflected in Fig. 9 . The mathematical model can be described as: Figure 9 structural parameters. Full size image Consider: $$\vec{x} = [x_{1} \, x_{2} \, x_{3} \, x_{4} \, x_{5} \, x_{6} \, x_{7} \left] = \right[b \, m \, p \, l_{1} \, l_{2} \, d_{1} \, d_{2} ]$$ Minimize: $$\begin{aligned} f\left( {\vec{x}} \right) = & 0.7854x_{1} x_{2}^{2} \left( {3.3333x_{3}^{2} + 14.9334x_{3} - 43.0934} \right) - 1.508x_{1} \left( {x_{6}^{2} + x_{7}^{2} } \right) + 7.4777\left( {x_{6}^{3} + x_{7}^{3} } \right) \\ & + 0.7854\left( {x_{4} x_{6}^{2} + x_{5} x_{7}^{2} } \right) \\ \end{aligned}$$ Subject to: $$\begin{gathered} g_{1} \left( {\vec{x}} \right) = \frac{27}{{x_{1} x_{2}^{2} x_{3} }} - 1 \le 0 \hfill \\ g_{2} \left( {\vec{x}} \right) = \frac{397.5}{{x_{1} x_{2}^{2} x_{3}^{2} }} - 1 \le 0 \hfill \\ g_{3} \left( {\vec{x}} \right) = \frac{{1.93x_{4}^{3} }}{{x_{2} x_{6}^{4} x_{3} }} - 1 \le 0 \hfill \\ g_{4} \left( {\vec{x}} \right) = \frac{{1.93x_{5}^{3} }}{{x_{2} x_{7}^{4} x_{3} }} - 1 \le 0 \hfill \\ g_{5} \left( {\vec{x}} \right) = \frac{{\left[ {\left( {745\left( {x_{4} /x_{2} x_{3} } \right)} \right)^{2} + 16.9 \times 10^{6} } \right]^{1/2} }}{{110x_{6}^{3} }} - 1 \le 0 \hfill \\ \end{gathered}$$ $$\begin{gathered} g_{6} \left( {\vec{x}} \right) = \frac{{\left[ {\left( {745\left( {x_{5} /x_{2} x_{3} } \right)} \right)^{2} + 157.5 \times 10^{6} } \right]^{1/2} }}{{85x_{7}^{3} }} - 1 \le 0 \hfill \\ g_{7} \left( {\vec{x}} \right) = \frac{{x_{2} x_{3} }}{40} - 1 \le 0 \hfill \\ g_{8} \left( {\vec{x}} \right) = \frac{{5x_{2} }}{{x_{1} }} - 1 \le 0 \hfill \\ g_{9} \left( {\vec{x}} \right) = \frac{{x_{1} }}{{12x_{2} }} - 1 \le 0 \hfill \\ g_{10} \left( {\vec{x}} \right) = \frac{{1.5x_{6} + 1.9}}{{x_{4} }} - 1 \le 0 \hfill \\ g_{11} \left( {\vec{x}} \right) = \frac{{1.1x_{7} + 1.9}}{{x_{5} }} - 1 \le 0 \hfill \\ \end{gathered}$$ in which: $$\begin{gathered} 2.6 \le x_{1} \le 3.6,0.7 \le x_{2} \le 0.8,17 \le x_{3} \le 28,7.3 \le x_{4} \le 8.3 \\ 7.3 \le x_{5} \le 8.3,2.9 \le x_{6} \le 3.9,5.0 \le x_{7} \le 5.5 \\ \end{gathered}$$ In fairness, CNSABC uses the same penalty function as the other algorithms, the results of CNSABC and the other algorithms are displayed in Table 13 50 , 51 . Table 14 shows the mean, standard deviation, minimum and maximum values of the 10 experiments of the CNSABC. Figure 10 shows the adaptation convergence curve of CNSABC computing the tension/compression spring design problem. The best solution is obtained by CNSABC at design variables \(\vec{x} = [x_{1} \, x_{2} \, x_{3} \, x_{4} \, x_{5} \, x_{6} \, x_{7} ]\) with objective function \(f(\vec{x}) = 2994.534574\) . The optimal value obtained in speed reducer design problem, CNSABC improved over SHO, GWO, POS, MVO, SCA, GSA, GA, AO, AOA, HS FA, HSCA, CB-ABC and I-ABC greedy by 0.13605%, 0.2271%, 0.3757%, 0.2816%, 1.1909%, 1.8566%, 2.3828%, 0.4409%, 0.1149%, 1.1400%, 0.5205%, 1.3358e-09, 1.3358e-09 and 2.2041e-08. CNSABC has wonderful performance than the other algorithms. Table 13 Comparison results for speed reducer design. Full size table Table 14 Statistical results obtained from CNSABC for speed reducer design. Full size table Figure 10 Convergence analysis of CNSABC for speed reducer design. Full size image Conclusions In this study, a chaotic and neighborhood search-based ABC algorithm (CNSABC) is presented to solve the shortcomings of traditional ABC for solving optimization problems. Firstly, Bernoulli chaos mapping with mutual exclusion mechanism is proposed to increase the diversity of populations and strengthen the global exploration capability. Secondly, neighborhood search mechanism with compression factor and sustained bees are presented to improve the local exploration and exploitation capability, and further to avoid the appearance of premature maturity. Subsequently, three groups of simulation experiments based on 26 benchmark functions are conducted to compare the CNSABC with eight existing variants of ABC and five commonly used metaheuristic optimization algorithms. The experimental results composed of “Mean”, “Std.”, “Max”, “Min” of the 26 benchmark functions verify the dominant of CNSABC in the optimal solution search ability. In detail, the overall performance of CNSABC is generally superior to PSO, ABC, GWO, WOA and BOA on 21, 26, 23 and 26 out of 26 functions. For the five variants of ABC, the CNSABC outperforms qABC, SBABC, MPGABC, GABC and NGABC with 26 benchmark functions of 25, 25, 25, 22 and 24, respectively. Finally, the CNSABC is applied to two engineering examples, the experimental results show that CNSABC can effectively solve practical application problems. Although the proposed CNSABC achieves excellent results in terms of exploitation capability and local exploration capability, the research of the CNSABC is still in the initial stage, and many problems need further study, such as a deficiency of low computational efficiency. Future work will focus on further enhancing the algorithm efficiency and exploring more improvement directions. For example, search strategies can be borrowed and combined with other algorithms. The application is extended to more practical applications, such as PID parameter optimization, and improved parameter search combined with neural network.